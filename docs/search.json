[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BSTA 513/613: Categorical Data Analysis",
    "section": "",
    "text": "BSTA 513/613: Categorical Data Analysis\n\nSpring 2025\n \nWelcome to BSTA 513/613! In this course, we will continue to learn about regression analysis, but not with categorical outcomes. We will build some theoretical understanding in order to interpret and apply logistic regression models appropriately. We will learn how to build a logistic regression model, interpret the model and coefficients, and diagnose potential issues with our model.  \n\n\n\n\n\n \nLink to Student Files OneDrive\nLink to Echo360 page\n\n\nInstructor\n Dr. Nicky Wakim\n Vanport 622A\n wakim@ohsu.edu\n\n\nOffice Hours\nTBD\n\n\nCourse details\n Mondays, Wednesdays\n March 30 - June 13\n 1:00 PM - 2:50 PM\n In-person, RPV Room B/C\n\n\nContacting me\nE-mail or Slack is the best way to get in contact with me. I will try to respond to all course-related e-mails within 24 hours Monday-Friday.\n\n\n\n\n\n\n\n\n View the source on GitHub"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-1",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-1",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1\nMake sure to remember your answer!! We’ll use this on Wednesday!"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#review-of-test-of-association-12",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#review-of-test-of-association-12",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Review of Test of Association (1/2)",
    "text": "Review of Test of Association (1/2)\n\nLast week: learned some tests of association for contingency tables\n\n \n\nFor studies with two independent samples\n\nGeneral association\n\nChi-squared test\nFisher’s Exact test\n\nTest of trends\n\nCochran-Armitage test\nMantel-Haenszel test"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#review-of-test-of-association-22",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#review-of-test-of-association-22",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Review of Test of Association (2/2)",
    "text": "Review of Test of Association (2/2)"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#test-of-association-does-not-measure-association",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#test-of-association-does-not-measure-association",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Test of association does not measure association",
    "text": "Test of association does not measure association\n\nTest of association does not provide an effective measure of association. The p-value alone is not enough\n\n\\(\\text{p-value} &lt; 0.05\\) suggests there is a statistically significant association between the group and outcome\n\\(\\text{p-value} = 0.00001\\) vs. \\(\\text{p-value} = 0.01\\) does not mean the magnistude of association is different\n\n\n \n\nBut it does not tell how different the risks are between the two groups\n\n \n\nWe want to find out one or more measurements for quantifying the risks across the groups."
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#measures-of-association",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#measures-of-association",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Measures of Association",
    "text": "Measures of Association\n\nWhen we have a 2x2 contingency table and independent samples, we have an option of three measures of association:\n \n\nRisk difference (RD)\n\n \n\nRelative risk (RR)\n\n \n\nOdds ratio (OR)\n\n\n \nEach measures association by comparing the proportion of successes/failures from each categorical group of our explanatory variable."
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#before-we-discuss-each-further",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#before-we-discuss-each-further",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Before we discuss each further…",
    "text": "Before we discuss each further…\n \nLet’s define the cells within a 2x2 contingency table:\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nThen we can define risk: the proportion of “successes”\n\nWith \\(\\text{Risk}_1 = \\dfrac{n_{11}}{n_1}\\)"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#risk-difference-rd",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#risk-difference-rd",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Risk Difference (RD)",
    "text": "Risk Difference (RD)\n\nRisk difference computes the absolute difference in risk for the two groups (from the explanatory variable)\nPoint estimate: \\[\\widehat{RD} = \\widehat{p}_1 - \\widehat{p}_1 = \\dfrac{n_{11}}{n_1} - \\dfrac{n_{21}}{n_2}\\]\n\nWith range of point estimate from \\([-1, 1]\\)\n\nApproximate standard error:\n\n\\[ SE_{\\widehat{RD}} = \\sqrt{\\frac{\\hat{p}_1\\cdot(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2\\cdot(1-\\hat{p}_2)}{n_2}}\\]\n\n95% Wald confidence interval for \\(\\widehat{RD}\\):\n\n\\[\\widehat{RD} \\pm 1.96 \\cdot SE_{\\widehat{RD}}\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#recall-the-strong-heart-study",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#recall-the-strong-heart-study",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Recall the Strong Heart Study",
    "text": "Recall the Strong Heart Study\n\n\nThe Strong Heart Study is an ongoing study of American Indians residing in 13 tribal communities in three geographic areas (AZ, OK, and SD/ND). We will look at data from this study examining the incidence of diabetes at a follow-up visit and impaired glucose tolerance (ITG) at baseline (4 years apart).\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Risk Difference",
    "text": "SHS Example: Risk Difference\n\n\n\n\nRisk difference\n\n\nCompute the point estimate and 95% confidence interval for the diabetes risk difference between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\nNeeded steps:\n\nCompute the risk difference\nCompute 95% confidence interval\nInterpret the estimate"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-14",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-14",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Risk Difference (1/4)",
    "text": "SHS Example: Risk Difference (1/4)\n\n\n\n\nRisk difference\n\n\nCompute the point estimate and 95% confidence interval for the diabetes risk difference between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nCompute the risk difference \\[\\widehat{RD}={\\hat{p}}_1 - {\\hat{p}}_2=\\frac{n_{11}}{n_1}-\\frac{n_{21}}{n_2}=\\ \\frac{198}{532}\\ - \\frac{128}{1132}=0.3722−0.1131=0.2591\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-24",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-24",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Risk Difference (2/4)",
    "text": "SHS Example: Risk Difference (2/4)\n\n\n\n\nRisk difference\n\n\nCompute the point estimate and 95% confidence interval for the diabetes risk difference between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nCompute 95% confidence interval\n\n\\[\\begin{aligned} &\\widehat{RD}\\pm z_{\\left(1-\\frac{\\alpha}{2}\\right)}^\\ast \\times SE_{\\widehat{RD}} \\\\\n= &\\widehat{RD}\\pm z_{\\left(1-\\frac{\\alpha}{2}\\right)}^\\ast \\times \\sqrt{\\frac{{\\hat{p}}_1\\ (1-{\\hat{p}}_1)}{n_1}+\\frac{{\\hat{p}}_2(1-{\\hat{p}}_2)}{n_2}\\ }\\\\\n=  & 0.2591 \\pm 1.96\\times \\sqrt{\\frac{0.3722(1-0.3722)}{532}+\\frac{0.1131(1-0.1131)}{1132}\\ }\\\\\n=  & (0.2141,\\ 0.3041 )\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-34",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-34",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Risk Difference (3/4)",
    "text": "SHS Example: Risk Difference (3/4)\n\n\n\n\nRisk difference\n\n\nCompute the point estimate and 95% confidence interval for the diabetes risk difference between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n1/2. Compute risk difference and 95% confidence interval\n\nfmsb::riskdifference(198, 128, 532, 1132)\n\n                 Cases People at risk         Risk\nExposed    198.0000000    532.0000000    0.3721805\nUnexposed  128.0000000   1132.0000000    0.1130742\nTotal      326.0000000   1664.0000000    0.1959135\n\n\n\n    Risk difference and its significance probability (H0: The difference\n    equals to zero)\n\ndata:  198 128 532 1132\np-value &lt; 2.2e-16\n95 percent confidence interval:\n 0.2140779 0.3041346\nsample estimates:\n[1] 0.2591062"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-44",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-44",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Risk Difference (4/4)",
    "text": "SHS Example: Risk Difference (4/4)\n\n\n\n\nRisk difference\n\n\nCompute the point estimate and 95% confidence interval for the diabetes risk difference between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nInterpret the estimate\n\nThe diabetes diagnosis risk difference between impaired and normal glucose tolerance is 0.2591 (95% CI: 0.2141, 0.3041). Since the 95% confidence interval contains 0, we do not have sufficient evidence that the risk of diabetes diagnosis between impaired and normal glucose tolerance is different."
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#when-is-the-risk-difference-misleading",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#when-is-the-risk-difference-misleading",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "When is the risk difference misleading?",
    "text": "When is the risk difference misleading?\n\nThe same risk differences can have very different clinical meanings depending on the risk for each group\n\n \n \n\nExample: for two treatments A and B, we know the risk difference (RD) is 0.009. Is it a meaningful difference?\n\nIf the risk is 0.01 for Trt A and 0.001 for Trt B?\nIf the risk is 0.41 for Trt A and 0.401 for Trt B?\n\n\n \n\nUsing the RD alone to summarize the difference in risks for comparing the two groups can be misleading\n\nThe ratio of risk can provide an informative descriptive measure of the “relative risk”"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#relative-risk-rr",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#relative-risk-rr",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Relative Risk (RR)",
    "text": "Relative Risk (RR)\n\nRelative risk computes the ratio of each group’s proportions of “success”\n\nAlso called risk ratio    \n\nPoint estimate: \\[\\widehat{RR}=\\dfrac{\\hat{p}_1}{\\hat{p}_2} = \\dfrac{n_{11}/n_1}{n_{21}/n_2}\\]\n\nRange: \\([0, \\infty]\\)"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-2",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-2",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#log-transformation-of-rr",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#log-transformation-of-rr",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Log-transformation of RR",
    "text": "Log-transformation of RR\n\nSampling distribution of the relative risk is highly skewed unless sample sizes are quite large\n\nLog transformation results in approximately normal distribution\nThus, compute confidence interval using normally distributed, log-transformed RR\nThen we convert back to the RR\n\nWe take the log (natural log) of RR: \\(\\ln(\\widehat{RR})\\) or \\(log(\\widehat{RR})\\)\n\nWhenever I say “log” I mean natural log (very common in statistics)\n\n\n\n\n\nThen we need to find approximate standard error for \\(\\ln(\\widehat{RR})\\) \\[SE_{\\ln(\\widehat{RR})}=\\sqrt{\\frac{1}{n_{11}}\\ -\\frac{1}{n_1}+\\frac{1}{n_{21}}-\\frac{1}{n_2}}\\]\n95% confidence interval for \\(\\ln(\\widehat{RR})\\): \\[\\ln(\\widehat{RR}) \\pm 1.96 \\times SE_{\\ln(\\widehat{RR})}\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#how-do-we-get-back-to-the-rr-scale",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#how-do-we-get-back-to-the-rr-scale",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "How do we get back to the RR scale?",
    "text": "How do we get back to the RR scale?\n\nWe computed confidence interval using normally distributed, log-transformed RR (\\(\\ln(\\widehat{RR})\\)):\n\n\\[ \\bigg(\\ln(\\widehat{RR}) - 1.96 \\times SE_{\\ln(\\widehat{RR})}, \\ \\ln(\\widehat{RR}) + 1.96 \\times SE_{\\ln(\\widehat{RR})}\\bigg)\\]\n\nNow we need to exponentiate the CI to get back to interpretable values\n\nTake exponential of lower and upper bounds\n\n95% confidence interval for RR: two ways to display equation\n\n\\[ \\bigg(e^{\\ln(\\widehat{RR}) - 1.96 \\times SE_{\\ln(\\widehat{RR})}}, \\ e^{\\ln(\\widehat{RR}) + 1.96 \\times SE_{\\ln(\\widehat{RR})}}\\bigg)\\] \\[ \\bigg(\\exp\\big(\\ln(\\widehat{RR}) - 1.96 \\times SE_{\\ln(\\widehat{RR})}\\big), \\ \\exp\\big(\\ln(\\widehat{RR}) + 1.96 \\times SE_{\\ln(\\widehat{RR})}\\big)\\bigg)\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#relative-risk-rr-1",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#relative-risk-rr-1",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Relative Risk (RR)",
    "text": "Relative Risk (RR)\n\nCan you compute the estimated RRs for the previous example?\n\nIf the risk for Trt A is 0.01 and Trt B is 0.001? \\(\\widehat{RR}= 10\\)\nIf the risk for Trt A is 0.41 and Trt B is 0.401? \\(\\widehat{RR}= 1.02\\)\n\nWhen \\(\\widehat{RR}= 1\\) …\n\nRisk is the same for the two groups\nIn other words, the group and the outcome are independent\n\nWhen computing \\(\\widehat{RR}\\) it is important to identify which variable is the response variable and which is explanatory variable\n\nWe may say “risk for Trt A” but this translates to the risk (or probability) of outcome success for those receiving Trt A"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-16",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-16",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Relative Risk (1/6)",
    "text": "SHS Example: Relative Risk (1/6)\n\n\n\n\nRelative risk\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Relative risk between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\nNeeded steps:\n\nCompute the relative risk\nFind confidence interval of log RR\nConvert back to RR\nInterpret the estimate"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-26",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-26",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Relative Risk (2/6)",
    "text": "SHS Example: Relative Risk (2/6)\n\n\n\n\nRelative risk\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Relative risk between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nCompute the relative risk \\[\\widehat{RR}=\\dfrac{{\\hat{p}}_1}{{\\hat{p}}_2}=\\dfrac{n_{11}/{n_1}}{n_{21}/{n_2}}=\\ \\frac{ 198/532}{128/1132}=\\dfrac{0.3722}{0.1131}=3.2915\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-36",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-36",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Relative Risk (3/6)",
    "text": "SHS Example: Relative Risk (3/6)\n\n\n\n\nRelative risk\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Relative risk between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nFind confidence interval of log RR\n\n\\[\\begin{aligned} & \\ln(\\widehat{RR}) \\pm 1.96 \\times SE_{\\ln(\\widehat{RR})} \\\\\n= &\\ln(\\widehat{RR}) \\pm z_{\\left(1-\\frac{\\alpha}{2}\\right)}^\\ast \\times \\sqrt{\\frac{1}{n_{11}}\\ -\\frac{1}{n_1}+\\frac{1}{n_{21}}-\\frac{1}{n_2}}\\\\\n=  & 1.1913 \\pm 1.96\\times \\sqrt{\\frac{1}{198}\\ -\\frac{1}{532}+\\frac{1}{128}-\\frac{1}{1132}}\\\\\n=  & (0.9944,\\ 1.3883 )\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-46",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-46",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Relative Risk (4/6)",
    "text": "SHS Example: Relative Risk (4/6)\n\n\n\n\nRelative risk\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Relative risk between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nConvert back to RR\n\n\\[\\begin{aligned} & (\\exp(0.9944),\\ \\exp(1.3883 )) \\\\\n= & (2.703,\\ 4.0081 )\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-56",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-56",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Relative Risk (5/6)",
    "text": "SHS Example: Relative Risk (5/6)\n\n\n\n\nRelative risk\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Relative risk between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n1/2/3. Compute risk ratio and 95% confidence interval\n\nlibrary(epitools)\nSHS_ct = table(SHS$glucimp, SHS$case)\nriskratio(x = SHS_ct, rev = \"rows\")$measure\n\n          risk ratio with 95% C.I.\n           estimate    lower    upper\n  Normal   1.000000       NA       NA\n  Impaired 3.291471 2.702998 4.008061"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#pause-other-option-in-pubh-package",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#pause-other-option-in-pubh-package",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Pause: other option in pubh package",
    "text": "Pause: other option in pubh package\n\nSHS = SHS %&gt;% mutate(glucimp = as.factor(glucimp) %&gt;% relevel(ref = \"Normal\"))\ncontingency(case ~ glucimp, data = SHS)\n\n          Outcome\nPredictor     1    0\n  Impaired  198  334\n  Normal    128 1004\n\n             Outcome +    Outcome -      Total                 Inc risk *\nExposed +          198          334        532     37.22 (33.10 to 41.48)\nExposed -          128         1004       1132      11.31 (9.52 to 13.30)\nTotal              326         1338       1664     19.59 (17.71 to 21.58)\n\nPoint estimates and 95% CIs:\n-------------------------------------------------------------------\nInc risk ratio                                 3.29 (2.70, 4.01)\nInc odds ratio                                 4.65 (3.61, 6.00)\nAttrib risk in the exposed *                   25.91 (21.41, 30.41)\nAttrib fraction in the exposed (%)            69.62 (63.00, 75.05)\nAttrib risk in the population *                8.28 (5.63, 10.94)\nAttrib fraction in the population (%)         42.28 (34.71, 48.98)\n-------------------------------------------------------------------\nUncorrected chi2 test that OR = 1: chi2(1) = 154.239 Pr&gt;chi2 = &lt;0.001\nFisher exact test that OR = 1: Pr&gt;chi2 = &lt;0.001\n Wald confidence limits\n CI: confidence interval\n * Outcomes per 100 population units \n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  dat\nX-squared = 152.6, df = 1, p-value &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-66",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-66",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Relative Risk (6/6)",
    "text": "SHS Example: Relative Risk (6/6)\n\n\n\n\nRelative risk\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Relative risk between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nInterpret the estimate\n\nThe estimated risk of diabetes is 3.29 times greater for American Indians who had impaired glucose tolerance at baseline compared to those who had normal glucose tolerance (95% CI: 2.70, 4.01).\n \nAdditional interpretation of 95% CI (not needed): We are 95% confident that the (population) relative risk is between 2.70 and 4.01.\n \nSince the 95% confidence interval does not include 1, there is sufficient evidence that the risk of diabetes differs significantly between impaired and normal glucose tolerance at baseline."
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#odds-building-up-to-odds-ratio",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#odds-building-up-to-odds-ratio",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Odds (building up to Odds Ratio)",
    "text": "Odds (building up to Odds Ratio)\n\nFor a probability of success \\(p\\) (or sometimes referred to as \\(\\pi\\)), the odds of success is: \\[\\text{odds}=\\frac{p}{1-p}=\\frac{\\pi}{1-\\pi}\\]\n\nExample: if \\(\\pi=0.75\\), then odds of success \\(= \\dfrac{0.75}{0.25}=3\\)\n\nIf odds &gt; 1, it implies a success is more likely than a failure\n\nExample: for \\(odds = 3\\), we expect to observe three times as many successes as failures\n\nIf odds is known, the probability of success can be computed \\[\\pi = \\dfrac{\\text{odds}}{\\text{odds}+1}\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#odds-ratio-or",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#odds-ratio-or",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Odds Ratio (OR)",
    "text": "Odds Ratio (OR)\n\nOdds ratio is the ratio of two odds:\\[\\widehat{OR}=\\frac{odds_1}{odds_2}=\\frac{{\\hat{p}}_1/(1-{\\hat{p}}_1)}{{\\hat{p}}_2/(1-{\\hat{p}}_2)}\\]\n\nRange: \\([0, \\infty]\\)\nInterpretation: The odds of success for “group 1” is “\\(\\widehat{OR}\\)” times the odds of success for “group 2”\n\n\n \n\nWhat do values of odds ratios mean?\n\n\n\n\n\n\n\nOdds Ratio\nClinical Meaning\n\n\n\n\n\\(\\widehat{OR} &lt; 1\\)\nOdds of success is smaller in group 1 than in group 2\n\n\n\\(\\widehat{OR} = 1\\)\nExplanatory and response variables are independent\n\n\n\\(\\widehat{OR} &gt; 1\\)\nOdds of success is greater in group 1 than in group 2"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-3",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-3",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#odds-ratio-or-1",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#odds-ratio-or-1",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Odds Ratio (OR)",
    "text": "Odds Ratio (OR)\n\nValues of OR farther from 1.0 in a given direction represent stronger association\n\nAn OR = 4 is farther from independence than an OR = 2\nAn OR = 0.25 is farther from independence than an OR = 0.5\nFor OR = 4 and OR = 0.25, they are equally away from independence (because ¼ = 0.25)\n\n\n \n\nWe take the inverse of the OR for success of group 1 compared to group 2 to get…\n\nOR for failure of group 1 compared to group 2\nOR for success of group 2 compared to group 1"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#log-transformation-of-or",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#log-transformation-of-or",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Log-transformation of OR",
    "text": "Log-transformation of OR\n\nLike RR, sampling distribution of the odds ratio is highly skewed\n\nLog transformation results in approximately normal distribution\nThus, compute confidence interval using normally distributed, log-transformed OR\n\n\n\n\n\nApproximate standard error for \\(\\ln (\\widehat{OR})\\): \\[SE_{\\ln(\\widehat{OR})}=\\sqrt{\\frac{1}{n_{11}}\\ +\\frac{1}{n_{12}}+\\frac{1}{n_{21}}+\\frac{1}{n_{22}}}\\]\n95% confidence interval for \\(\\ln(\\widehat{OR})\\): \\[\\ln(\\widehat{OR}) \\pm 1.96 \\times SE_{\\ln(\\widehat{OR})}\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#how-do-we-get-back-to-the-or-scale",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#how-do-we-get-back-to-the-or-scale",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "How do we get back to the OR scale?",
    "text": "How do we get back to the OR scale?\n\nWe computed confidence interval using normally distributed, log-transformed RR (\\(\\ln(\\widehat{OR})\\)):\n\n\\[ \\bigg(\\ln(\\widehat{OR}) - 1.96 \\times SE_{\\ln(\\widehat{OR})}, \\ \\ln(\\widehat{OR}) + 1.96 \\times SE_{\\ln(\\widehat{OR})}\\bigg)\\]\n\nNow we need to exponentiate the CI to get back to interpretable values\n\nTake exponential of lower and upper bounds\n\n95% confidence interval for RR: two ways to display equation\n\n\\[ \\bigg(e^{\\ln(\\widehat{OR}) - 1.96 \\times SE_{\\ln(\\widehat{OR})}}, \\ e^{\\ln(\\widehat{OR}) + 1.96 \\times SE_{\\ln(\\widehat{OR})}}\\bigg)\\] \\[ \\bigg(\\exp\\big(\\ln(\\widehat{OR}) - 1.96 \\times SE_{\\ln(\\widehat{OR})}\\big), \\ \\exp\\big(\\ln(\\widehat{OR}) + 1.96 \\times SE_{\\ln(\\widehat{OR})}\\big)\\bigg)\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-16",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-16",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Odds Ratio (1/6)",
    "text": "SHS Example: Odds Ratio (1/6)\n\n\n\n\nOdds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes odds ratio between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\nNeeded steps:\n\nCompute the odds ratio\nFind confidence interval of log OR\nConvert back to OR\nInterpret the estimate"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-26",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-26",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Odds Ratio (2/6)",
    "text": "SHS Example: Odds Ratio (2/6)\n\n\n\n\nOdds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Odds ratio between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nCompute the odds ratio\n\n\\(\\widehat{p}_1 = 198/532 = 0.3722\\), \\(\\widehat{p}_2 = 128/1132 = 0.1131\\) \\[\\widehat{OR}=\\frac{\\widehat{p_1}/(1-\\widehat{p_1})}{\\widehat{p_2}/(1-\\widehat{p_2})}= \\dfrac{0.3722/(1-0.3722)}{0.1131/(1-0.1131)}= 4.6499\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-36",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-36",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Odds Ratio (3/6)",
    "text": "SHS Example: Odds Ratio (3/6)\n\n\n\n\nOdds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Odds ratio between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nFind confidence interval of log OR\n\n\\[\\begin{aligned} & \\ln(\\widehat{OR}) \\pm 1.96 \\times SE_{\\ln(\\widehat{OR})} \\\\\n= &\\ln(\\widehat{OR}) \\pm z_{\\left(1-\\frac{\\alpha}{2}\\right)}^\\ast \\times \\sqrt{\\frac{1}{n_{11}}\\ +\\frac{1}{n_{12}}+\\frac{1}{n_{21}}+\\frac{1}{n_{22}}}\\\\\n=  & 1.5368 \\pm 1.96\\times \\sqrt{\\frac{1}{198}\\ +\\frac{1}{334}+\\frac{1}{128}+\\frac{1}{1004}}\\\\\n=  & (1.2824,\\ 1.7913 )\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-46",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-46",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Odds Ratio (4/6)",
    "text": "SHS Example: Odds Ratio (4/6)\n\n\n\n\nOdds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Odds ratio between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nConvert back to OR\n\n\\[\\begin{aligned} & (\\exp(1.2824),\\ \\exp(1.7913 )) \\\\\n= & (3.6053,\\ 5.9971 )\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-56",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-56",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Odds Ratio (5/6)",
    "text": "SHS Example: Odds Ratio (5/6)\n\n\n\n\nOdds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Odds ratio between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n1/2/3. Compute OR and 95% confidence interval\n\nlibrary(epitools)\nSHS_ct = table(SHS$glucimp, SHS$case)\n# no `rev` needed below bc we set the reference level in slide 32\noddsratio(x = SHS_ct, method = \"wald\")$measure \n\n          odds ratio with 95% C.I.\n           estimate    lower    upper\n  Normal   1.000000       NA       NA\n  Impaired 4.649888 3.605289 5.997148"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#pause-other-option-in-pubh-package-1",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#pause-other-option-in-pubh-package-1",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Pause: other option in pubh package",
    "text": "Pause: other option in pubh package\n\ncontingency(case ~ glucimp, data = SHS, digits = 3)\n\n          Outcome\nPredictor     1    0\n  Impaired  198  334\n  Normal    128 1004\n\n             Outcome +    Outcome -      Total                 Inc risk *\nExposed +          198          334        532  37.218 (33.097 to 41.482)\nExposed -          128         1004       1132   11.307 (9.521 to 13.298)\nTotal              326         1338       1664  19.591 (17.709 to 21.581)\n\nPoint estimates and 95% CIs:\n-------------------------------------------------------------------\nInc risk ratio                                 3.291 (2.703, 4.008)\nInc odds ratio                                 4.650 (3.605, 5.997)\nAttrib risk in the exposed *                   25.911 (21.408, 30.413)\nAttrib fraction in the exposed (%)            69.618 (63.004, 75.050)\nAttrib risk in the population *                8.284 (5.631, 10.937)\nAttrib fraction in the population (%)         42.284 (34.713, 48.976)\n-------------------------------------------------------------------\nUncorrected chi2 test that OR = 1: chi2(1) = 154.239 Pr&gt;chi2 = &lt;0.001\nFisher exact test that OR = 1: Pr&gt;chi2 = &lt;0.001\n Wald confidence limits\n CI: confidence interval\n * Outcomes per 100 population units \n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  dat\nX-squared = 152.6, df = 1, p-value &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-66",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-66",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Odds Ratio (6/6)",
    "text": "SHS Example: Odds Ratio (6/6)\n\n\n\n\nOdds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Odds ratio between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nInterpret the estimate\n\nThe estimated odds of diabetes for American Indians with impaired glucose tolerance at baseline is 4.65 times the odds for American Indians with normal glucose tolerance at baseline.\n \nAdditional interpretation of 95% CI (not needed): We are 95% confident that the odds ratio is between 3.61 and 6.00.\n \nSince the 95% confidence interval does not include 1, there is sufficient evidence that the odds of diabetes differs significantly between impaired and normal glucose tolerance at baseline."
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#inversing-an-odds-ratio",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#inversing-an-odds-ratio",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Inversing an Odds Ratio",
    "text": "Inversing an Odds Ratio\n\nSome clinicians may prefer interpretations of OR &gt; 1 instead of an OR &lt; 1\nThe transformation can easily be done by inverse\n\nRemember we discussed that OR = 4 is an equivalent a strong association as OR = 0.25 (1/4)\n\nOR comparing group 1 to group 2 = inverse of OR comparing group 2 to group 1\n\n\\[ OR_{1v2}=\\frac{{\\hat{p}}_1/(1-{\\hat{p}}_1)}{{\\hat{p}}_2/(1-{\\hat{p}}_2)}=\\frac{1}{\\frac{{\\hat{p}}_2/(1-{\\hat{p}}_2)}{{\\hat{p}}_1/(1-{\\hat{p}}_1)}}=\\frac{1}{OR_{2v1}}\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-4",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-4",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-inversing-odds-ratio",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-inversing-odds-ratio",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Inversing Odds Ratio",
    "text": "SHS Example: Inversing Odds Ratio\n\n\n\n\nInversing odds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes odds ratio between normal and impaired glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\nNeeded steps:\n\nInverse point estimate and confidence interval\n\n\\[\\widehat{OR}=\\frac{1}{4.6499}=0.2151\\] The 95% Confidence interval is then\n\\[ \\left(\\frac{1}{5.9971}, \\frac{1}{3.6053}\\right)\\ =\\ (0.1667, 0.2774)\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-inversing-odds-ratio-1",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-inversing-odds-ratio-1",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Inversing Odds Ratio",
    "text": "SHS Example: Inversing Odds Ratio\n\n\n\n\nInversing odds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes odds ratio between normal and impaired glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\nNeeded steps:\n\nInverse point estimate and confidence interval\n\n\nlibrary(epitools)\noddsratio(x = SHS_ct, method = \"wald\", rev = \"rows\")$measure \n\n          odds ratio with 95% C.I.\n           estimate     lower     upper\n  Impaired 1.000000        NA        NA\n  Normal   0.215059 0.1667459 0.2773702"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-inversing-odds-ratio-2",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-inversing-odds-ratio-2",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Inversing Odds Ratio",
    "text": "SHS Example: Inversing Odds Ratio\n\n\n\n\nInversing odds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes odds ratio between normal and impaired glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\nNeeded steps:\n\nInterpret the estimate\n\nThe estimated odds of diabetes for American Indians with normal glucose tolerance at baseline is 0.22 times the odds for American Indians with impaired glucose tolerance at baseline.\n \nAdditional interpretation of 95% CI (not needed): We are 95% confident that the odds ratio is between 0.17 and 0.28.\n \nSince the 95% confidence interval does not include 1, there is sufficient evidence that the odds of diabetes differs significantly between impaired and normal glucose tolerance at baseline."
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#pubh-vs.-epitools",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#pubh-vs.-epitools",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "pubh vs. epitools",
    "text": "pubh vs. epitools\n\nIn pubh with contingency()\n\nGet all the info at once\nReally nice to double check how the code is interpreting your input\n\nIn epitools with riskratio() or oddsratio()\n\nMuch easier to grab the numbers!\nIn Quarto you can take R code and directly put it in your text\n\ng = oddsratio(x = SHS_ct, method = \"wald\", rev = \"rows\")\ng$measure[2,1]\n\n[1] 0.215059\n\n\n\nI can write {r eval=\"false\" echo=\"true\"} round(g$measure[2,1], 3) to print the number 0.215"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Week\nDate\nLes-son\nTopic\nKey Info\nSlides HTML\nSlides PDF\nSlides Notes\nExit tix\nEcho 360\nMuddy Points\n\n\n\n\n1\n3/31\n1\nWelcome\n\n\n\n\n\n\n\n\n\n\n\n2\nIntroduction to Categorical Analysis\n\n\n\n\n\n\n\n\n\n\n4/2\n3\nMeasurement of Association for Contingency Tables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2\n4/7\n4\nMeasurements of Association and Agreement\n\n\n\n\n\n\n\n\n\n\n4/9\n5\nSimple Logistic Regression\n\n\n\n\n\n\n\n\n\n\n4/11\n\nLab 1 due @ 11 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3\n4/14\n6\nTests for GLMs using Likelihood function\n\n\n\n\n\n\n\n\n\n\n4/16\n7\nPredictions and Visualizations in Simple Logistic Regression\n\n\n\n\n\n\n\n\n\n\n\n\nRequired in-person attendance: Lab 1 Discussion\n\n\n\n\n\n\n\n\n\n\n4/18\n\nHW 1 due @ 11 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4\n4/21\n8\nInterpretations and Visualizations of Odds Ratios\n\n\n\n\n\n\n\n\n\n\n4/23\n10\nMultiple Logistic Regression\n\n\n\n\n\n\n\n\n\n\n4/23\n\nQuiz 1 opens at 3pm\n\n\n\n\n\n\n\n\n\n\n4/25\n\nLab 2 due @ 11 pm\n\n\n\n\n\n\n\n\n\n\n4/27\n\nQuiz 1 closes at 11pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5\n4/28\n11\nInteractions\n\n\n\n\n\n\n\n\n\n\n4/30\n\nRequired in-person attendance: Lab 2 Discussion\n\n\n\n\n\n\n\n\n\n\n5/2\n\nHW 2 due @ 11 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6\n5/5\n12\nAssessing Model Fit\n\n\n\n\n\n\n\n\n\n\n5/7\n13\nNumerical Problems\n\n\n\n\n\n\n\n\n\n\n5/9\n\nLab 3 due @ 11 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7\n5/12\n14\nModel Diagnostics\n\n\n\n\n\n\n\n\n\n\n\n\nRequired in-person attendance: Lab 3 Discussion\n\n\n\n\n\n\n\n\n\n\n5/14\n15\nModel Building\n\n\n\n\n\n\n\n\n\n\n5/14\n\nQuiz 2 opens at 3pm\n\n\n\n\n\n\n\n\n\n\n5/16\n\nHW 3 due @ 11 pm\n\n\n\n\n\n\n\n\n\n\n5/18\n\nQuiz 2 closes at 11pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8\n5/19\n16\nPoisson Regression, potentially virtual\n\n\n\n\n\n\n\n\n\n\n5/21\n17\nOther types of categorical regressions!, , potentially virtual\n\n\n\n\n\n\n\n\n\n\n5/23\n\nLab 4 due 11pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9\n5/26\n\nNo class, Memorial Day\n\n\n\n\n\n\n\n\n\n\n5/28\n18\nOther types of categorical regressions: More Analysis\n\n\n\n\n\n\n\n\n\n\n\n\nRequired in-person attendance: Lab 4 Discussion\n\n\n\n\n\n\n\n\n\n\n5/28\n\nQuiz 3 opens at 3pm\n\n\n\n\n\n\n\n\n\n\n5/30\n\nHW 4 due 11 pm\n\n\n\n\n\n\n\n\n\n\n6/1\n\nQuiz 3 closes at 11pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10\n6/2\n9\nMissing Data\n\n \n\n\n\n\n\n\n\n\n6/4\n\nCatch-up day?\n\n\n\n\n\n\n\n\n\n\n6/6\n\nHW 5 due 11 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n11\n6/9\n\nRequired in-person attendance: Project Day!!\n\n\n\n\n\n\n\n\n\n\n6/9\n\nProject due 11pm\n\n\n\n\n\n\n\n\n\n\n6/15\n\nAll late coursework due 11pm"
  },
  {
    "objectID": "lessons.html",
    "href": "lessons.html",
    "title": "Lessons",
    "section": "",
    "text": "Lesson\n1\nTopic\nFile Organization within R\nSlides | :=======================================================================================================================================================================================================================:+  |\n\n\n\n\n\n\n\n2\nIntroduction to Categorical Analysis\n |\n\n\n3\nMeasurement of Association for Contingency Tables\n |\n\n\n4\nMeasurements of Association and Agreement\n |\n\n\n5\nSimple Logistic Regression\n |\n\n\n6\nTests for GLMs using Likelihood function\n |\n\n\n7\nPredictions and Visualizations in Simple Logistic Regression\n |\n\n\n8\nInterpretations and Visualizations of Odds Ratios\n |\n\n\n9\nMissing Data\nActivity:\n  | |\n\n\n10\nMultiple Logistic Regression\n |\n\n\n11\nInteractions\n |\n\n\n12\nAssessing Model Fit\n |\n\n\n13\nNumerical Problems\n |\n\n\n14\nModel Diagnostics\n |\n\n\n15\nModel Building\n |\n\n\n16\nPoisson Regression\n |\n\n\n17\nOther types of categorical regressions!\n |"
  },
  {
    "objectID": "syllabus.html#description",
    "href": "syllabus.html#description",
    "title": "BSTA 513/613 Syllabus",
    "section": "Description",
    "text": "Description\nWelcome to BSTA 513/613! In this course, we will continue to learn about regression analysis, but not with categorical outcomes. We will build some theoretical understanding in order to interpret and apply logistic regression models appropriately. We will learn how to build a logistic regression model, interpret the model and coefficients, and diagnose potential issues with our model.\n\nCourse Learning Objectives\nAt the end of this course, students should be able to…\n\nApply and interpret a variety of hypothesis-testing procedures for two-way and three-way contingency tables\nCompute and interpret measures of association for binary and ordinal data.\nCalculate and correctly interpret odds ratios using logistic regression, make comparison across groups and examine relationship between binary outcome and predictor variables.\nApply appropriate model-building strategies for logistic regression. Effectively use statistical computing packages for contingency table and logistic regression procedures.\nPerform Poisson regression analysis using count data and interpret model estimates, make comparison across groups and examine relationship between outcome and predictor variables.\nCoherently summarize methods and results of data analyses, and discuss in context of original health-related research questions to audiences with varied statistical background."
  },
  {
    "objectID": "syllabus.html#instructors",
    "href": "syllabus.html#instructors",
    "title": "BSTA 513/613 Syllabus",
    "section": "Instructors",
    "text": "Instructors\nHere is the instructor page. This also has office hours!"
  },
  {
    "objectID": "syllabus.html#meeting-times",
    "href": "syllabus.html#meeting-times",
    "title": "BSTA 513/613 Syllabus",
    "section": "Meeting Times",
    "text": "Meeting Times\nMondays          1:00 PM – 2:50 PM PST in RLSB 3A001\nWednesdays    1:00 PM – 2:50 PM PST in RLSB 3A001"
  },
  {
    "objectID": "syllabus.html#materials",
    "href": "syllabus.html#materials",
    "title": "BSTA 513/613 Syllabus",
    "section": "Materials",
    "text": "Materials\n\nTextbooks\n\nApplied Logistic Regression by David Hosmer, Stanley Lemeshow, and Rodney Sturdivant\nAn Introduction to Categorical Data Analysis by Alan Agresti\nIntroduction to Regression Methods for Public Health Using R by Ramzi W. Nahhas\n\nSpecifically Chapter 6\n\n\n\nSupplemental Readings (Optional)\n\nAn Introduction to R\n\n\n\n\nOnline Resources\n\nSakai\nWhile most course materials will be delivered online through this website, assignments will be turned in through Sakai, OHSU’s course management system. I will include a link on this website to the Sakai assignment page. \n\n\nWebex\nWebex software will be used for virtual office hours. To give everyone the best possible experience with Webex, I recommend the following best practices:\n\nPlease stay muted until you want to participate\nDuring office hours, please send a message in chat with your question or with a statement like “I have a question.” This makes sure I or the TA can address everyone’s questions in order. \nI encourage you to attend office hours with your video on. This helps me recognize you, and keep mental notes on what techniques/concepts I emphasize to facilitate your specific understanding. \n\n\n\nPoll Everywhere\nWe will use the Poll Everywhere tool as an interactive feature of the course. Poll Everywhere is a web-based application that allows students to participate by responding via text messages or by visiting a web page on an internet-enabled device (smartphone, tablet, laptop). Instructions will be displayed on-screen. The poll that is embedded within the presentation will update in real time. While there is no cost to use this software, standard text messaging rates will apply if you use your phone. Please make sure that you have a Poll Everywhere account before our first class. You are not required to use your OHSU/PSU email to make an account. \nDuring lectures I will pose questions to the class. These questions are designed to provide real-time feedback to both students and the instructor on how well students are grasping the material. This is meant to be an interactive, learning activity with NO contribution to your grade. Your identity will never be connected to your answers, so I encourage you to answer honestly.\n\n\nPennState STAT 504 Website\nPennState has a class offered to online MS students that has some overlap with our class. They have all their course notes posted on this page. This is a great source if you would like to see class notes with different phrasing.\nNot all of our topics are covered in their notes, but the most important ones are. If you are having trouble finding our course’s concepts on their page, please make ask me at Office Hours, after class, or in a private meeting. I do not explicitly state corresponding sections under our schedule because I believe it is important for you to develop skills involving resources and learning key words that can help you find answers. \n\n\nR: Statistical Computing Software\nStudents will use statistical software to complete homework assignments. Students are required to use R/RStudio for this course. R can be freely downloaded. Helpful documentation on installing R is available. I encourage you to install R prior to attending our first lecture. Please email me if you need help installing R or RStudio.\nYou will need to download the following three things:\n\nR https://www.r-project.org/\nRstudio https://posit.co/download/rstudio-desktop/\nQuarto https://quarto.org/docs/get-started/\n\n\nAdditional R Resources\nYour learning and practicing of R will hopefully not be limited to this course. One of the best aspects of programming in R is that many resources are freely available online. Here are just a few additional resources you may explore beyond this class to continue your training in R.\n\n\nUseful online R resources\n\nR for the rest of us\nStatistical tools for high-throughput data analysis. ggplot2 essentials\nR-bloggers\nStack Overflow for troubleshooting\nR Graphical Manual\nQuick-R. Accessing the power of R\nR for SAS, STATA, and SPSS Users\nggplot2\nLearn R 4 free\nJoin a local R user groups\nLearning Machines\n\n\n\n\nOnline R courses to complement or refresh material from class\n\nR for the rest of us\nCoursera: R programming\nedX: R basics\nData Carpentry: For Biologists\nData Carpentry: For Ecologists\nPsychiatric R\nR coder"
  },
  {
    "objectID": "syllabus.html#assessment",
    "href": "syllabus.html#assessment",
    "title": "BSTA 513/613 Syllabus",
    "section": "Assessment",
    "text": "Assessment\nThe course is structured around the following four components:\n\n\n\n\n\n\n\n\n\nComponent\nModality\nFrequency\nDescription\n\n\nLecture\nIn person\nTwice, Weekly\nCourse content is provided through in-person lectures. Lectures will consist of didactic lessons, interactive examples, and PollEverywhere questions. Sessions will be recorded through Explain Everything and posted to Sakai. Attending or viewing the lecture within 7 days of the original lecture date is mandatory. Class attendance will be taken through an Exit Ticket. If viewing the lecture asynchronously, you must take the Exit Ticket to verify your attendance.\n\n\nHomework\nOnline\nWeekly\nThe course includes 7 homework assignments. They are an opportunity for you to engage with important concepts, practice coding, and apply calculating skills. Homework assignments should be submitted online, and will be graded by me. Students are encouraged to work in groups for homework assignments, but each person should do their own summary and hand in their work. Homework assignments will be due on Thursday at 11 PM.\n\n\nQuizzes\nOnline\nEvery 3 weeks\nThe purpose of the quizzes is to assess how well you have achieved the learning objectives through questions covering important concepts, conducting statistical processes, and interpreting output. We will have our quizzes in-class, and it will be open book. Students must work on the quizzes independently.\n\n\nProject (Labs and Poster)\nOnline\n4 labs, 1 final poster\nThe project will be a combination of submitted labs that will span the quarter and one final report submitted at the end of the quarter. This is meant to translate the tools learned in the course to the work one may do in the workforce. This will help instill the procedure for shaping research goals, model selection, analyzing data, and interpreting meaningful results. Labs will guide you through the needed analysis and background for the project. The final poster will summarize your work over the labs by giving context and results to your research question. Students will work independently on each lab.\n\n\n\n\nTypes of assessments\nThis class will use a combination of formative and summative assessments to build and test our knowledge. Below I define each of these types of assessments:\n\nFormative assessment: Activity or work meant to help students learn and practice. Feedback on these assessments are meant to help the instructor and student identify gaps in knowledge and highlight accomplishments.\nSummative assessment: Work meant to test how well students have achieved learning objectives. Grading of these assessments are meant to gauge how well a student grasps the learning objectives and will be able to use their knowledge outside of the classroom."
  },
  {
    "objectID": "syllabus.html#assessment-breakdown",
    "href": "syllabus.html#assessment-breakdown",
    "title": "BSTA 513/613 Syllabus",
    "section": "Assessment Breakdown",
    "text": "Assessment Breakdown\n\nGrading & Requirements\nLetter grades will be assigned roughly according to the following scheme: A (&gt;=93%), A- (90-92%), B+ (88-89%), B(83-87%), B- (82-80%), C+(78-79%), C(73-77%), C- (70-72%), D (60 – 69%), F(&lt;60%).\nGrades will be based on homework assignments, midterm exam, class “attendance”, and final exam, as follows:\n\n\n\n\n\n\n\n\n\n\nCourse activity\nType of Assessment\nDue Date\nPercentage of final grade (BSTA 513)\nPercentage of final grade (BSTA 613)\n\n\nHomework\nFormative\nEvery 1-2 weeks\n30%\n25%\n\n\nQuizzes\nSummative\n4/27, 5/18, 6/1\n25%\n25%\n\n\nProject Labs\nFormative/Summative\nEvery 2-3 weeks\n25%\n25%\n\n\nLab Discussion + Poster Day Participation\nN/A\nEvery 2-3 weeks\n5%\n5%\n\n\nProject Poster\nSummative\n6/9\n10%\n10%\n\n\nExit tickets (Attendance)\nN/A\nTwice Weekly\n5%\n5%\n\n\n613 Readings\nFormative\nApprox. every other week\n0%\n5%\n\n\n\n\n\nHomework grading\nNo student has the same amount of time available to dedicate to homework. This class may not be a priority to you, you may be taking several other courses, or you may need to dedicate time to other activities. Homeworks are formative assessments, meaning its purpose is to help you learn and practice. To reduce the pressure on you to have perfect or complete homework, I have a very simple grading policy: Your homework will be given a check mark if you turn in 75% of the questions parts completed (whether the 75% is correct or wrong). I highly encourage you to stay up-to-date with the homeworks and put in as much effort as you can. This will be the most helpful work in this class!\nIf you turn in the homework on time, the TAs will give you feedback (on one or more complete problems). There is no penalty for turning in the homework late, but you will not get feedback on your work. Please make sure to check the solutions or go to office hours to assess your work.\n\n\nViewing Grades in Sakai\nPoints you receive for graded activities will be posted to the Sakai Gradebook. Click on the Gradebook link on the left navigation to view your points."
  },
  {
    "objectID": "syllabus.html#course-instructor-evaluations",
    "href": "syllabus.html#course-instructor-evaluations",
    "title": "BSTA 513/613 Syllabus",
    "section": "Course & Instructor Evaluations",
    "text": "Course & Instructor Evaluations\n\nOngoing Course Feedback\nThroughout the duration of the course, you are also welcome to informally and anonymously submit your feedback through this Microsoft Form or Class Exit Tickets. This form will be available on Sakai. Students can submit feedback at any time and this form will be reviewed regularly by me. Your responses will be anonymous unless you elect to leave your email address. If I have done anything to make you feel uncomfortable, please give me feedback so I can change my behavior. Ultimately, this class is for you, and my individual social identity/behavior should not inhibit your learning. Thank you for your help making BSTA 513/613 a more successful class! Examples of ongoing feedback are:\n\nNicky talks a little fast during lecture time. May you speak slower?\nDuring Office Hours, Dr. Wakim made a face when I asked a question. This face made me feel self-conscious about my question.\nDr. W asked me a question about my experience that made me feel like a monolith. Please do not assume I can speak on behalf of my social identity groups.\nThe in-class examples do not make me more interested in the material.\n\n\n\nFinal Course Feedback\nAt the conclusion of the course, you will be asked to complete a formal online review of the course and the instructor. Your feedback on this University evaluation is critical to improving future student learning in this course as well as providing metrics relevant to the instructor’s career advancement (or lack of). Since our class is on the smaller side, everyone’s participation is needed for feedback to be released."
  },
  {
    "objectID": "syllabus.html#schedule",
    "href": "syllabus.html#schedule",
    "title": "BSTA 513/613 Syllabus",
    "section": "Schedule",
    "text": "Schedule\nPlease refer to the Schedule page. I will make changes to this schedule if we need more or less time on a concept. You do not need to read the corresponding chapters in the textbook for each class."
  },
  {
    "objectID": "syllabus.html#how-to-succeed-in-this-course",
    "href": "syllabus.html#how-to-succeed-in-this-course",
    "title": "BSTA 513/613 Syllabus",
    "section": "How to succeed in this course",
    "text": "How to succeed in this course\nEvery professor has different expectations when assigning certain work or providing certain resources. I want to walk through each class resource and assignment so that you know what you can do to succeed in this class. For resources, I want you to optimize the opportunities to learn. For assignments, I want you to know the strategies that students can use to learn the most and prepare for future exams.\n\nResources\n\n\n\n\n\n\n\n\nResource\nWhat is it?\nHow do I use it?\n\n\nOffice Hours\nBlocks of time a professor or TA dedicates for questions. The teaching staff will be located in a specific room. Several students may enter the space at a time and will ask specific or broad questions. If many students attend office hours, a queue will be created so that students can be served equally.\nThe main use of office hours is to ask questions about an assignment or lecture notes. You are welcome to sit and do homework in office hours. OH are also an informal way of meeting fellow students to collaborate with.\n\n\nLectures and lecture recordings\nTime shared between the professor and students where the professor conveys important class material. Material discussed in lectures include concepts, calculations, code, and examples. Lectures are a mix of presentation of information, working through examples together, interactive activities, and in-class polls.\nStudents should attend lectures in person if possible. You should attempt to understand new material presented by following the presentation slides, taking notes on additional details that may conveyed verbally, and working through examples with the professor. Students are encouraged to ask questions when you don’t understand the material at any point in the lecture.\n\n\nTextbooks\nWritten and published material that explains concepts, steps through calculations, provides examples, and provides practice problems. The listed textbooks is the basis for this course. While I am to cover all topics in class, the textbook provides alternative explanations and additional examples.\nWhile coming to class having read the accompanying textbook chapters helps understanding during class, I do not expect students to have read it. I see the textbook as a good resource if you are struggling with a specific topic after class, in need of an example while working on homework, or want additional practice when studying for the exam.\n\n\nWebsite\nThe course website is designed by me so that you have access to all the course materials in a more organized and flexible way. All resources delivered from me to you will be available on the website. Any assignments turned in will be through Sakai.\nYou can navigate through different course resources and information using the left-side tabs or top navigation bar. Course materials, like lecture notes, homework, data examples, and recordings, can be found under each week’s page under the schedule tab. You can also find the individual resources under the “Course Materials” tab on the left. Links to turn in assignments through Sakai will be given on the website. Please explore the tabs and get a sense of the organization.\n\n\nSakai\nSakai is a learning management system for higher ed. This is the university sanctioned LMS where we will submit assignments.\nYou will turn in assignments through Sakai under the “Submissions” tab. Generally, there will be a link to each assignment on the course website. You can also view your grades under “Gradebook” and links to Webex under “Webex.”\n\n\n\n\n\nAssignments\n\n\n\n\n\n\n\n\n\nAssignment\nType of assessment\nBefore you submit/take it\nAfter it is graded\n\n\nHomework\nFormative\n\nWork out each problem on your own as much as you can\nTalk through problems with a peer\nGo to Office Hours for help\nWrite down work that shows your thought process\nSearch your issue on Stack Exchange/Stack Overflow\n\n\nReview the solutions\nReview your mistakes\nFor solutions that involve writing sentences, check with me or a TA if your answer fits the solution\nGo to Office Hours to ask about your solutions\n\n\n\nQuizzes\nSummative\n\nIdentify and achieve learning objectives in each lecture\nUnderstand why certain statistics tools are used for certain cases\nPractice testing yourself and others on concepts\nCome to Office Hours for help with specific problems or concepts\n\n\nReview the solutions\nReview your mistakes\nFor solutions that involve writing sentences, check with me or a TA if your answer fits the solution\nGo to Office Hours to ask about your solutions\nDo not ask for a regrade unless you have viewed the solutions\n\n\n\nProject Labs and Poster\nFormative and Summtive\n\nStart the lab as early as possible\nWork on R coding and check with classmates on work\nCome to Office Hours for help with specific R work\nFor the report, compile your work from the labs, and decide what is important in the analysis.\n\n\nThis will be graded at the end of the semester, so you will not have a chance to interact with my feedback as much\nIf you have questions about your grade, you may email me\nKeep the project paper for future reference\nYou can add this project to your resume!\n\n\n\nClass Exit Tickets\nN/A\n\nBring appropriate electronic device to participate in polls\nComplete the survey during the last 5 minutes of class or after class within 7 days\n\n\nReview muddiest and clearest points from the week\n\n\n\n\nIf you would like any other course resources explained in this format, please request it through the Ongoing Course Feedback."
  },
  {
    "objectID": "syllabus.html#course-policies-and-resources",
    "href": "syllabus.html#course-policies-and-resources",
    "title": "BSTA 513/613 Syllabus",
    "section": "Course Policies and Resources",
    "text": "Course Policies and Resources\n\nLate Work Policy\nI encourage you to make your best effort to submit all assignments on time, but I understand circumstances arise that are beyond our control. Please see this Swansea University’s page on extenuating circumstances for some examples. Not all circumstances are covered here, so please reach out if you have questions. \n\nThe class will end on June 9, 2024. All coursework is expected to be completed by June 15, 2024 at 11pm. If you have extenuating circumstances, and need additional time to complete class assignments, please contact me. Together, we will come up with a plan for completion and to sort out registrar logistics.\nIf you have extenuating circumstances that may jeopardize your ability to do work for several weeks, please contact me. We will come up with a plan to keep you on track in the course and prevent any delay in your education.\nFor homework, there is a due date posted, but you may turn in the assignment any time before the class ends. I will give you the check regardless of when you submit the assignment. However, if you would like feedback on the homework, you must turn it in on time OR email me asking for feedback for your late homework.\nFor non-homework assignments, including labs, I ask you to email me directly. You can explain your circumstances and may ask me for an extension, but I won’t necessarily grant one.\nFor labs, you will have ONE no-questions-asked, 3-day extension. Please use this wisely! You just need to send me a quick email saying “I am using my no-questions-asked extension for Lab __.”\nIf you have a emergency involving your self, family, pet, friend, classmate, or anything/one deemed important to you, please do not worry about immediately contacting me. We can work something out after your emergency. If I contact you during an emergency, it is only because I am worried, and you do NOT need to respond until you are able. \n\n\n\nRegrade Policy\nIf you think a question was incorrectly graded, first compare your answer to the answer key. If you believe a re-grade would be appropriate, write an email to me containing the question and a short explanation as to why the question(s) was/were incorrectly graded. Deadline: One week after assignments were returned to class (late requests will not be considered).\n\n\nAttendance Policy\nYou are expected to attend class, participate in-class polls, and complete the exit ticket. For students who miss class or need a review, I will make video and audio recordings of lectures available. There are no guarantees against technical or other challenges for the recording availability or quality. For students who are unable to attend the class in-person and synchronously, viewing the recording within 7 days is acceptable. This is meant to keep you on track within the course and prevent a pile up of material. Make sure to complete the exit ticket to demonstrate attendance.\n\n\nPlagiarism and Attribution\nPlease note that this section has been motivated by Dr. Steven Bedrick’s Course Policies and Grading site for BMI 525. (Note that this is a good example of informal attribution of someone else’s work.)\nIn this class, it is easy to use ChatGPT or other AI tools to solve your homework for you. Many problems follow a basic structure that is especially easy for ChatGPT to solve. In this class, you may use ChatGPT to help with your homework. You may even ask for direct answers. However, there are a few things I do not want you to do:\n\nDo not copy ChatGPT’s answer directly into your homework. Your homework is graded for full credit if you turn it in, in any state, so turning in ChatGPT’s answers is unacceptable. I rather see half-written answers that show what you’re thinking than see a correct answer from ChatGPT.\nDo not stop once ChatGPT answered a question. If it gives an explanation, interact with it! Make sure you understand the thought process of ChatGPT. Try writing out the process to help cement it in your head. Check the answer with what we learn in class.\nDo not use ChatGPT on our quizzes! Hence, you need to really understand how to solve these problems even if you use ChatGPT on the homework.\n\nAt the end of the day, ChatGPT is a resource that will be available to you in a job and outside of school. Thus, we should use it as a tool in school as well! Let me know if ChatGPT helped you understand something! I would love to incorporate it into future classes!\n\n\n\n\n\n\nImportant\n\n\n\nYou can think of this class as assembling a toolbox. When a handyperson starts working for the first time, they need to buy their tools. For their first few jobs, they might need help finding their tools, or remembering which tool is best used for what action. Eventually, they get to know their tools well, and using them appropriately becomes second nature.\nFor now, ChatGPT can help us find and use our tools, but we need to work towards using them as second nature!"
  },
  {
    "objectID": "syllabus.html#course-expectations",
    "href": "syllabus.html#course-expectations",
    "title": "BSTA 513/613 Syllabus",
    "section": "Course Expectations",
    "text": "Course Expectations\n\nInstructor Expectations\nCommitment to your learning and your success\nI believe that everyone has the ability to be successful in this course and I have put a lot of effort into designing the course in a way that maximizes your learning to ensure your success. Please talk to me before or after class or stop by my office if there is anything you want to discuss or about which you are unclear. I want to be supportive of your learning and growth.\nInclusive & supportive learning community\nI believe that learning happens best when we all learn together, as a community. This means creating a space characterized by generous listening, civility, humility, patience, and hospitality. I will attempt to promote a safe climate where we examine content from multiple cultural perspectives, and I will strive to create and maintain a classroom atmosphere in which you feel free to both listen to others and express your views and ask questions to increase your learning.\nOpenness to feedback\nI appreciate straightforward feedback from you regarding how well the class is meeting your needs. Let me know if material is not clear or when its relevance to the student learning outcomes for the course is not apparent. In particular, let me know if you identify bias or stereotyping in my teaching materials as I will seek to continuously improve. Please also let me know if there’s an aspect of the class you find particularly interesting, helpful, or enjoyable!\nResponsiveness\nI will monitor email as well as the discussion board daily and try respond to all messages within 24 hours Monday-Friday.\nClear guidelines and prompt feedback on assignments\nI will provide clear instructions for all assignments, and a grading rubric when applicable. I will provide detailed feedback on your submissions and will update grades promptly in Sakai.\n\n\nStudent Expectations and Resources\nAttend class\nYou are expected to attend all scheduled class meetings synchronously or watch the recording within 7 days. Attendance is taken through exit tickets. If you have issues accessing the poll on a specific day, please let me know. \nParticipate\nI encourage you to participate actively in class and online discussions. I will expect all students, and all instructors, to be respectful of each other’s contributions, whether I agree with them or not. Professional interactions are expected.\nBuild rapport\nIf you find that you have any trouble keeping up with assignments or other aspects of the course, make sure you let me know as early as possible. As you will find, building rapport and effective relationships are key to becoming an effective professional. Make sure that you are proactive in informing me when difficulties arise during the quarter so that I can help you find a solution.\nComplete assignments\nAll assignments for this course will be submitted electronically through Sakai unless otherwise instructed.  I encourage you to make your best effort to submit all assignments on time, but I understand that sometimes circumstances arise that are beyond our control. If you need an extension, please contact me in congruence with the Late Policy.\nSeek help if you need it\nI believe it is important to support the physical and emotional well‐being of my students. If you are experiencing physical or mental health issues, I encourage you to use the resources on campus such as those listed below. If you have a health issue that is affecting your performance or participation in the course, and/or if you need help connecting with these resources, please contact me.\n\nStudent Health and Wellness Center (SHW), Website, 503-494-8665 (OHSU Students only)\nStudent Health and Counseling (SHAC), Website, 503-725-2800\n\nInform your instructor of any accommodations needed\nYou should speak with or email me before or during the first week of classes regarding any special needs. Students seeking academic accommodations should register with the appropriate service under the School policies below.\nSome religious holidays may occur on regularly scheduled class days. Because available class hours are so limited in number, we will have to hold class on all such days. Class video recordings will be available and you are encouraged to engage with the material outside of the regular class time. You are also encouraged to come to office hours with questions from the session.\nCommit to integrity\nAs a student in this course (and at PSU or OHSU) you are expected to maintain high degrees of professionalism, commitment to active learning and participation in this class and also integrity in your behavior in and out of the classroom.\nCheating and other forms of academic misconduct will not be tolerated in this course and will be dealt with firmly. Student academic misconduct refers to behavior that includes plagiarism, cheating on assignments, fabrication of data, falsification of records or official documents, intentional misuse of equipment or materials (including library materials), or aiding and abetting the perpetration of such acts. Preparation of exams, assigned on an individual basis, must represent each student’s own individual effort. When used, resource materials should be cited in conventional reference format."
  },
  {
    "objectID": "syllabus.html#course-communications",
    "href": "syllabus.html#course-communications",
    "title": "BSTA 513/613 Syllabus",
    "section": "Course Communications",
    "text": "Course Communications\nSakai/Slack announcements\nFor important/urgent matters, I will communicate with you using announcements via Sakai that will be delivered to your OHSU Email account as well as displayed in the Sakai course site Announcements section. I will copy these announcements in Slack if they do not involve changes to the schedule. Unfortunately, there are certain announcements that OHSU requires I initiate behind the firewall.\nGeneral course questions\nIt is normal to have many questions about things that relate to the course, such as clarification about assignments, course materials, or assessments. Please post these on our Slack Workspace. Please use the channels that I created for questions. You are encouraged to give answers and help each other. I will monitor these threads, so I will endorse or correct responses as needed. Please give me 24 hours to respond to questions within Monday-Friday. Work-life balance is important for me as well, so I will try to respond as quickly as I can within my healthy limits. \nE-mail\nE-mail should be used only for messages that are private in nature. Please send private messages to my OHSU email address (wakim@ohsu.edu). Messages sent through Sakai Inbox will not be answered. Do not send messages asking general information about the class; please post those on Slack instead."
  },
  {
    "objectID": "syllabus.html#further-student-resources",
    "href": "syllabus.html#further-student-resources",
    "title": "BSTA 513/613 Syllabus",
    "section": "Further Student Resources",
    "text": "Further Student Resources\n\nSPH Writing Lab\nThe School of Public Health Writing Support serves graduate students (master’s and PhD) in SPH, offering help on all professional writing tasks, including class papers, dissertations, job application documents, personal statements, and grant applications, to name a few. Leslie Bienen, MFA, DVM offers one-on-one writing support and other workshops. Appointments are virtual for the time being. You can make an appointment by contacting writingsupportsph@pdx.edu or making an appointment through Calendly.\n\n\nGrammarly Subscription\nThe School of Public Health students have access to a subscription version of Grammarly. While Grammarly cannot improve the argument and flow of your work, it can help with spelling, grammar, and sentence structure. If you are interested in this tool, please add your name to this email form and they will get you added to the subscription. Be sure to use your PSU login credentials to access the form.\n\n\nStudent Wellness\nI am committed to supporting the physical and emotional well-being of my students. Both PSU and OHSU have designated centers for student health. For OHSU, students can visit the Behavioral Health site, where you can find more information including the number to make an appointment. All student visits are free. OHSU students also have access to PSU’s Counseling Services through the school’s Student Health & Counseling. Information on additional student resources for OHSU students are available on the OHSU Health and Wellness Resource page. \n\n\nSupport for Food Insecurity\nStudents across the country experience food insecurity at alarming rates. OHSU and PSU both provide a list of resources to help combat food insecurity. Of note, the Committee to Improve Student Food Security (CISFS) at PSU provides a Free Food Market on the second Monday of each month. OHSU also provides SNAP Enrollment Assistance. The Supplemental Nutrition Assistance Program (SNAP) allocates money towards food for individuals below a certain income level. If you make less than $2,430 monthly, you may wish to enroll.\n\n\nSupport for Students with Children\nStudents who have children can use the PSU resource: Resource Center for Students with Children. Resources are mostly focused on students with younger children. There are several great resources available, including: family-friendly study spaces, new baby starter packs, free kids clothing, and further information on financial resources for childcare."
  },
  {
    "objectID": "syllabus.html#school-policies-and-resources",
    "href": "syllabus.html#school-policies-and-resources",
    "title": "BSTA 513/613 Syllabus",
    "section": "School Policies and Resources",
    "text": "School Policies and Resources\n\nSchool of Public Health Handbook\nAll students are responsible for following the policies and expectations outlined in the student handbook for their program of study. Students are responsible for their own academic work and are expected to have read and practice principles of academic honesty, as presented in the handbook.\n\n\nStudent Access & Accommodations\nThe School of Public Health values diversity and inclusion; we are committed to fostering mutual respect and full participation for all students. My goal is to create a learning environment that is equitable, usable, inclusive, and welcoming. If any aspects of instruction or course design result in barriers to your inclusion or learning, please notify me. \n\nIf you are already registered with disability services at either OHSU or PSU and you are taking a course at the opposite institution, you need to contact the office you’re registered with to transfer your accommodations.\nIf you are not already registered with a disability services office, and you have, or think you may have, a disability that may affect your work in this class, and feel you need accommodations, use the following table for guidance about which office to contact to initiate accommodations.\n\nResource Table\n\n\n\nEnrollment University and Standing\nWhere to Seek Accommodations\n\n\nUndergraduate School of Public Health major\nPSU’s Disability Resource Center\n\n503-725-4150\nSmith Memorial Student Union, Room 116\ndrc@pdx.edu\n\n\n\nAll PSU-registering Dual Degree (MSW/MPH and MURP/MPH) Graduate School of Public Health Majors and all PSU-registering PhD students admitted prior to fall 2016.\nPSU’s Disability Resource Center\n\n503-725-4150\nSmith Memorial Student Union, Room 116\ndrc@pdx.edu\nwww.pdx.edu/drc\n\n\n\nGraduate School of Public Health major (irrespective of institution at which you register)\nOHSU’s Office for Student Access\n(503) 494-0082\nStudentAccess@OHSU.edu\nOHSU Auditorium Building 330\n\n\nNon-SPH major, PSU-enrolled student\nPSU’s Disability Resource Center\n503-725-4150\nSmith Memorial Student Union, Room 116\ndrc@pdx.edu\nwww.pdx.edu/drc\n\n\nNon-SPH major, OHSU-enrolled student\nOHSU’s Office for Student Access\n(503) 494-0082\nStudentAccess@OHSU.edu\nOHSU Auditorium Building 330\n\n\n\n \nFor more information related accessibility and accommodations, please see the “Statement Regarding Students with Disabilities” within the Institutional Policies section of this syllabus.\n\n\nTitle IX\nThe School of Public Health is committed to providing an environment free of all forms of prohibited discrimination and discriminatory harassment. The School of Public Health students who have questions about an incident related to Title IX are welcome to contact either the OHSU or PSU’s Title IX Coordinator and they will direct you to the appropriate resource or office. Title IX pertains to any form of sex/gender discrimination, discriminatory harassment, sexual harassment or sexual violence.\n\nPSU’s Title IX Coordinator is Julie Caron, she may be reached at titleixccordinator@pdx.edu or 503-725-4410. Julie’s office is located at 1600 SW 4th Ave, In the Richard and Maureen Neuberger Center RMNC - Suite 830.\nThe OHSU Title IX Coordinator’s may be reachedat 503-494-0258 or titleix@ohsu.edu and is located at 2525 SW 3rd St.\n\nPlease note that faculty and the Title IX Coordinators will keep the information you disclose private but are not confidential. If you would like to speak with a confidential advocate, who will not disclose the information to a university official without your written consent, you may contact an advocate at PSU or OHSU.\n\nPSU’s confidential advocates are available in Women’s Resource Center (serving all genders) in Smith Student Memorial Union 479. You may schedule an appointment by (503-725-5672) or schedule on line at https://psuwrc.youcanbook.me. For more information about resources at PSU, please see PSU’s Response to Sexual Misconduct website.\nOHSU’s advocates are available through the Confidential Advocacy Program (CAP) at 833-495-CAPS (2277) or by email CAPsupport@ohsu.edu, but please note, email is not a secure form of communication. Also visit www.ohsu.edu/CAP.\n\nAt OHSU, if you encounter any harassment, or discrimination based on race, color, religion, age, national origin or ancestry, veteran or military status, sex, marital status, pregnancy or parenting status, sexual orientation, gender identity or expression, disability or any other protected status, please contact the Affirmative Action and Equal Opportunity (AAEO) Department at 503-494-5148 or aaeo@ohsu.edu.\nAt PSU, you may contact the Office of Equity and Compliance if you experience any form of discrimination or discriminatory harassment as listed above at equityandcompliance@pdx.edu or by calling 503-725-5919.\n\n\nTechnical Support\nThe OHSU ITG Help Desk is available to assist students with email account or network account access issues between 6 a.m. and 6 p.m., Monday through Friday at 503-494-2222. For technical support in using the Sakai Course Management System, please contact the Sakai Help Desk at 877-972-5249 or email us at sakai@ohsu.edu"
  },
  {
    "objectID": "syllabus.html#ohsu-competencies",
    "href": "syllabus.html#ohsu-competencies",
    "title": "BSTA 513/613 Syllabus",
    "section": "OHSU Competencies",
    "text": "OHSU Competencies\n\nList of OHSU Graduation Core Competencies\n\nProfessional Knowledge and Skills\nProfessionalism\nInformation Literacy\nCommunication\nTeamwork\nCommunity Engagement, Social Justice and Equity\nPatient Centered Care\n\nTo access a descriptive list of OHSU Graducation Core Competencies: OHSU Graduation Core Competencies"
  },
  {
    "objectID": "syllabus.html#institutional-policies-and-resources",
    "href": "syllabus.html#institutional-policies-and-resources",
    "title": "BSTA 513/613 Syllabus",
    "section": "Institutional Policies and Resources",
    "text": "Institutional Policies and Resources\n\nStatement Regarding Students with Disabilities\nOHSU is committed to inclusive and accessible learning environments in compliance with federal and state law. If you have a disability or think you may have a disability (mental health, attention-related, learning, vision, hearing, physical or health impacts) contact the Office for Student Access at (503) 494-0082 or OHSU Student Access to have a confidential conversation about academic accommodations. Information is also available at Student Access Website. Because accommodations may take time to implement and cannot be applied retroactively, it is important to have this discussion as soon as possible.\nPortland State students also have similar resources available via the PSU Disability Resource Center (website http://www.pdx.edu/drc ). Please contact the DRC at tel. (503) 725-4150 or email at drc@pdx.edu\n\n\nStudent Evaluation of Courses\nCourse evaluation results are extremely important and used to help improve courses and the learning experience of future students. Responses will always remain anonymous and will only be available to instructors after grades have been posted. The results of scaled questions and comments go to both the instructor and their unit head/supervisor. Refer to Student Evaluation of Courses and Instructional Effectiveness, *Policy No. 02-50-035.\n*To access the OHSU Student Evaluation of Courses and Instructional Effectiveness Policy, you must log into the OHSU O2 website.\n\n\nCopyright Information\nCopyright laws and fair use policies protect the rights of those who have produced the material. The copy in this course has been provided for private study, scholarship, or research. Other uses may require permission from the copyright holder. The user of this work is responsible for adhering to copyright law of the U.S. (Title 17, U.S. Code). To help you familiarize yourself with copyright and fair use policies, the University encourages you to visit its Copyright Web Page\nSakai course web sites contain material protected by copyrights held by the instructor, other individuals or institutions. Such material is used for educational purposes in accord with copyright law and/or with permission given by the owners of the original material. You may download one copy of the materials on any single computer for non-commercial, personal, or educational purposes only, provided that you (1) do not modify it, (2) use it only for the duration of this course, and (3) include both this notice and any copyright notice originally included with the material. Beyond this use, no material from the course web site may be copied, reproduced, re-published, uploaded, posted, transmitted, or distributed in any way without the permission of the original copyright holder. The instructor assumes no responsibility for individuals who improperly use copyrighted material placed on the web site.\n\n\nSyllabi Changes and Retention\nSyllabi are considered to be a learning agreement between students and the faculty of record. Information contained in syllabi, other than the minimum requirements, may be subject to change as deemed appropriate by the faculty of record in concurrence with the academic program and the Office of the Provost. Refer to the *Course Syllabi Policy, 02-50-050.\n*To access the OHSU Course Syllabus Policy, you must log into the OHSU O2 website.\n\n\nCommitment to Diversity & Inclusion\nOHSU is committed to creating and fostering a learning and working environment based on open communication and mutual respect. If you encounter sexual harassment, sexual misconduct, sexual assault, or discrimination based on race, color, religion, age, national origin, veteran’s status, ancestry, sex, marital status, pregnancy or parenting status, sexual orientation, gender identity, disability or any other protected status please contact the Affirmative Action and Equal Opportunity Department at 503-494-5148 or aaeo@ohsu.edu. Inquiries about Title IX compliance or sex/gender discrimination and harassment may be directed to the OHSU Title IX Coordinator at 503-494-0258 or titleix@ohsu.edu.\n\n\nModified Operations, Policy 01-40-010\nPortland Campus:  Marquam Hill and South Waterfront\nStudents should review O2 or call OHSU’s weather alert line at 503-494-9021 for the most up-to-date information on OHSU-wide modified operations which include but are not limited to delays or closures for inclement weather.\nIf your home institution is not on the Portland campus (Marquam Hill or South Waterfront, contact your home institution for more information.\n\n\nOHSU Resources Available to Students*:\nRemote Learning Resources\nThe Remote Learning webpage on O2 contains concise, practical resources, and strategies for students that need to quickly transition to a fully remote instructional format.\nRegistrar’s Office\nMackenzie Hall, Rm. 1120\n503-494-7800; Email the Registrar\nStudent Registration Information: \nTo Register for Classes\nOHSU ITG Help Desk\nRegular staff hours are 6 a.m. to 6 p.m., Monday through Friday, but phones are answered seven days a week, 24 hours a day. Call 503 494-2222.\nTeaching and Learning Center\nAcademic Support Counseling and Sakai Course Management System, please contact the TLC Help Desk at 877-972-5249 or email TLC Help Desk\nStudent Academic Support Services\nFor resources on improving student’s study strategies, time management, motivation, test-taking skills and more, Please access the Student Academic Support Services Sakai page. For one-on-one appointments or to arrange a workshop for students, please contact Emily Hillhouse.\nConfidential Advocacy Program\nSupport for OHSU employees, students, and volunteers who have experienced any form of sexual misconduct, including sexual harassment, sexual assault, intimate-partner violence, stalking, relationship/dating violence, and other forms — regardless of when or where it took place. Contact Us.\nConcourse Syllabus Management\nFor help with accessing your Concourse Syllabus:  Please contact the Sakai help Desk for all other Concourse inquiries please visit the Concourse Support - Sakai or please contact the Mark Rivera at rivermar@ohsu.edu or call 503-494-0934\nPublic Safety\nOHSU Public Safety-Portland Campus (Marquam Hill and South Waterfront)\n\nEmergency on Campus: 503-494-4444 (Portland)\nNon-emergency: 503-494-7744; Contact Public Safety\n\nStudent Health & Wellness Center \nBaird Hall, Rm. 18 (Primary Care) and Rm. 6 (Behavioral Health)\n503-494-8665; For urgent care after hours, 503-494-8311 and ask for the Nurse on call.\nWellness Center Information  \nWellness Center Website\nIf your home institution is not on the Portland campus, contact your home institution student support services for more information.\nOmbudsman Office\nGaines Hall, Rm. 117\n707 SW Gaines Street, Portland, OR 97239\n503-494-5397; Contact Ombudsman; Ombudsman Website\nLibrary: Biomedical Information Communication Center\nBICC Library Hours of Operation\n\n\nPrivacy While Learning\nStudents may be asked to take classes remotely through videoconferencing software like WebEx. Some of these remote classes will be recorded. Any recording will capture the presenter’s audio, video, and computer screen. Student video and audio will be recorded if and when you unmute your audio and share your video during the recorded sessions. These recordings will not be shared with or accessible to the public without prior written consent. \n\n\nStudent Central\nKey information for students across OHSU’s Schools of Dentistry, Medicine, Nursing, the OHSU-PSU School of Public Health and the College of Pharmacy. Student Central helps you find out more about student services, resources, policies and technology."
  },
  {
    "objectID": "instructors.html",
    "href": "instructors.html",
    "title": "Instructors",
    "section": "",
    "text": "Email: wakim@ohsu.edu\nOffice: VPT 622A\n\nPronouns: she/her/hers\nYou are welcome to address me as Nicky (pronounced “nik-EE”), Dr. Wakim (pronounced “wah-KEEM”), Dr. W, Dr. Nicky, Professor, Professor Wakim, or any combination of the prior.\nBest method to contact: Office hours or Slack for general course questions or E-mail/Calendly appointments for private communication.\n\n\n\n\n\n\n\n\n\n\nBrief professor statement: As a professor, my main goal is to instill a growth mindset into my students. Growth mindset means we are NOT stuck in our abilities or knowledge, and that we all can and will grow! This course aims to be as transparent as possible. I want you to understand my motivation for assessments, questions, and lessons. I also want those assessments to be clear, so please ask for clarification whenever needed.\n\n\nLink to Webex!!\n\nWednesdays 3 - 4pm"
  },
  {
    "objectID": "instructors.html#instructor-nicole-nicky-wakim-phd",
    "href": "instructors.html#instructor-nicole-nicky-wakim-phd",
    "title": "Instructors",
    "section": "",
    "text": "Email: wakim@ohsu.edu\nOffice: VPT 622A\n\nPronouns: she/her/hers\nYou are welcome to address me as Nicky (pronounced “nik-EE”), Dr. Wakim (pronounced “wah-KEEM”), Dr. W, Dr. Nicky, Professor, Professor Wakim, or any combination of the prior.\nBest method to contact: Office hours or Slack for general course questions or E-mail/Calendly appointments for private communication.\n\n\n\n\n\n\n\n\n\n\nBrief professor statement: As a professor, my main goal is to instill a growth mindset into my students. Growth mindset means we are NOT stuck in our abilities or knowledge, and that we all can and will grow! This course aims to be as transparent as possible. I want you to understand my motivation for assessments, questions, and lessons. I also want those assessments to be clear, so please ask for clarification whenever needed.\n\n\nLink to Webex!!\n\nWednesdays 3 - 4pm"
  },
  {
    "objectID": "instructors.html#teaching-assistants",
    "href": "instructors.html#teaching-assistants",
    "title": "Instructors",
    "section": "Teaching Assistants",
    "text": "Teaching Assistants\nAntara and Ariel were great students in my BSTA 513 class last year! They will have the following office hours and will help answer questions on Slack.\n\nAntara Vidyarthi\nLink to Zoom!!\n\nTuesdays 5:30 - 7pm\n\n\n\nAriel Weingarten\nLink to Webex!!\n\nThursdays 3:30 - 5pm"
  },
  {
    "objectID": "instructors.html#statistics-tutor-for-epidemiology-students",
    "href": "instructors.html#statistics-tutor-for-epidemiology-students",
    "title": "Instructors",
    "section": "Statistics Tutor for Epidemiology Students",
    "text": "Statistics Tutor for Epidemiology Students\n\nBecky Lanford\n\nEmail: lanford@ohsu.edu\nLink to Becky’s Calendly\n\nBecky can help with:\n\nStatistical coding support\nSupport with stats concepts you are learning in class\nData management and analysis plan scheming during your PE\n\nIntroduction from Becky:\n\nHello fellow MPH classmates! My name is Becky Lanford. I’m looking forward to helping support you in your coursework this quarter. A little about my background: I am currently in my final year in the MPH Epidemiology track and have completed most of my coursework including the biostatistics and epidemiology series (mostly working in R). I enrolled at the SPH as someone re-entering the workforce and quite new to statistical programming. Though I had previously completed a graduate degree (as a Physician Assistant/Associate), re-acclimating to graduate work and learning programming skills made for a steep learning curve my first academic year. I credit the collaborative learning environment at SPH - support of TA’s and classmates and availability of instructors - for helping me be successful. I hope I can help answer course-content questions, problem solve with you and find answers if I don’t have them myself. I know there are many challenges to being a graduate student and I am excited to help our public health student community grow stronger and more knowledgeable together."
  },
  {
    "objectID": "homeworks.html",
    "href": "homeworks.html",
    "title": "Homework Assignments and Solutions",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\n4/11/24\n\n\nHomework 1\n\n\n6 min\n\n\n\n\n4/25/24\n\n\nHomework 2\n\n\n7 min\n\n\n\n\n5/9/24\n\n\nHomework 3\n\n\n5 min\n\n\n\n\n5/23/24\n\n\nHomework 4\n\n\n6 min\n\n\n\n\n6/6/24\n\n\nHomework 5\n\n\n5 min\n\n\n\n\n\nNo matching items\n\n\n\nHomework 4 Part e help"
  },
  {
    "objectID": "homeworks.html#assignments",
    "href": "homeworks.html#assignments",
    "title": "Homework Assignments and Solutions",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\n4/11/24\n\n\nHomework 1\n\n\n6 min\n\n\n\n\n4/25/24\n\n\nHomework 2\n\n\n7 min\n\n\n\n\n5/9/24\n\n\nHomework 3\n\n\n5 min\n\n\n\n\n5/23/24\n\n\nHomework 4\n\n\n6 min\n\n\n\n\n6/6/24\n\n\nHomework 5\n\n\n5 min\n\n\n\n\n\nNo matching items\n\n\n\nHomework 4 Part e help"
  },
  {
    "objectID": "homeworks.html#solutions",
    "href": "homeworks.html#solutions",
    "title": "Homework Assignments and Solutions",
    "section": "Solutions",
    "text": "Solutions\nPlease note that you need to download the .html file to see the LaTeX math properly.\n\n\n\nHomework\n.qmd file\n.html file\n\n\n\n\n1\n\n\n\n\n2\n\n\n\n\n3\n\n\n\n\n4\n\n\n\n\n5"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#what-is-categorical-data-analysis",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#what-is-categorical-data-analysis",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "What is Categorical Data Analysis?",
    "text": "What is Categorical Data Analysis?\n\nIn BSTA 512/612 (linear regression), we focused on continuous responses/outcomes\n\nWe included categorical variables only as covariates (aka predictors, independent variables, explanatory variables)\nExamples from 512/612: life expectancy (in years), IAT score (ranging from -2 to 2)\n\n\n   \n\nCategorical data analysis focuses on the statistical methods for categorical responses/outcomes\n\nExplanatory (or ‘independent’) variable can be of any type (continuous or categorical)"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#types-of-variables",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#types-of-variables",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Types of Variables",
    "text": "Types of Variables"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#types-of-variables-outcomes-we-will-cover-in-this-course",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#types-of-variables-outcomes-we-will-cover-in-this-course",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Types of Variables: Outcomes we will cover in this course",
    "text": "Types of Variables: Outcomes we will cover in this course"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#what-does-this-course-cover",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#what-does-this-course-cover",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "What does this course cover?",
    "text": "What does this course cover?\n\nStrategies for assessing association between categorical response variable and a one explanatory variable\n\nHypothesis testing\nMeasure of association\nSimple logistic regression\n\n\n   \n\nStatistical modeling strategies for assessing association between the categorical response variable and a set of explanatory variables\n\nLogistic regression\n\nFor binary, ordinal, and multinomial outcomes\n\nPoisson regression\n\nFor counts outcomes"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#poll-everywhere-question-1",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#poll-everywhere-question-1",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Poll everywhere question 1",
    "text": "Poll everywhere question 1"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#binomial-distribution",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#binomial-distribution",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\nConsider a sample of \\(n\\) independent trials, each of which can have only two possible outcomes (“success” and “failure”)\nFor each trial: \\[\\begin{align} P( \\text{success}) & = p \\\\\n                  P( \\text{failure}) & = 1- p = q \\end{align}\\]\nBinomial distribution: distribution of the number of successes in n independent trials\nThe probability mass function for the binomial distribution is: \\[P(X=k) = {n \\choose k} p^k q^{n-k}, \\text{ for } k = 0, 1, ..., n\\]\n\n\\(E(X) = np\\)\n\\(Var(X) = npq = np(1-p)\\)"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#binomial-distribution-1",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#binomial-distribution-1",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\nConsider a sample of \\(n\\) independent trials, each of which can have only two possible outcomes (“success” and “failure”)\nFor each trial: \\[\\begin{align} P( \\text{success}) & = p \\\\\n                  P( \\text{failure}) & = 1- p = q \\end{align}\\]\nBinomial distribution: distribution of the number of successes in n independent trials\nThe probability mass function for the binomial distribution is: \\[P(X=k) = {n \\choose k} p^k q^{n-k}, \\text{ for } k = 0, 1, ..., n\\]\n\n\\(E(X) = np\\)\n\\(Var(X) = npq = np(1-p)\\)"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#binomial-distribution-r-commands",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#binomial-distribution-r-commands",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Binomial Distribution: R commands",
    "text": "Binomial Distribution: R commands\nR commands with their input and output:\n\n\n\n\n\n\n\nR code\nWhat does it return?\n\n\n\n\nrbinom()\nreturns sample of random variables with specified binomial distribution\n\n\ndbinom()\nreturns probability of getting certain number of successes\n\n\npbinom()\nreturns cumulative probability of getting certain number or less successes\n\n\nqbinom()\nreturns number of successes corresponding to desired quantile"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#binomial-distribution-example",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#binomial-distribution-example",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Binomial Distribution Example",
    "text": "Binomial Distribution Example\n\n\nExample\n\n\nIf the probability that one white blood cell is a lymphocyte is 0.2, compute the probability of 2 lymphocytes out of 10 white blood cells\n\n\n\\[P(X=2) = {10 \\choose 2} 0.2^2 (1-0.2)^{10-2}  = 0.3020\\]\n\ndbinom(2, 10, 0.2) %&gt;% round(4)\n\n[1] 0.302"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#normal-approximation-of-the-binomial-distribution",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#normal-approximation-of-the-binomial-distribution",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Normal Approximation of the Binomial Distribution",
    "text": "Normal Approximation of the Binomial Distribution\n\nAlso known as: Sampling distribution of \\(\\widehat{p}\\)\nIF \\(X\\sim \\text{Binomial}(n,p)\\) and \\(np&gt;10\\) and \\(nq = n(1-p) &gt; 10\\)\n\nEnsures sample size (\\(n\\)) is moderately large and the \\(p\\) is not too close to 0 or 1\nOther resources use other criteria (like \\(npq&gt;5\\) or \\(np&gt;5\\))\nWhen looking at a sample, we use \\(\\widehat{p}\\) instead of \\(p\\) to check this!!\n\n\n \n\nTHEN approximately \\(𝑋\\sim \\text{Normal}\\big(\\mu_X = np, \\sigma_X = \\sqrt{np(1-p)} \\big)\\)\n\nOr we often write this as the sampling distribution of \\(\\widehat{p}\\): \\[\\widehat{p} \\sim \\text{Normal}\\bigg(\\mu_{\\widehat{p}} = p, \\sigma_{\\widehat{p}} = \\sqrt{\\dfrac{p(1-p)}{n}}\\bigg)\\]\n\nPretty good video behind the intuition of this (Watch 00:00 - 05:40)"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#estimate-and-confidence-interval-of-single-proportion",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#estimate-and-confidence-interval-of-single-proportion",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Estimate and Confidence Interval of Single Proportion",
    "text": "Estimate and Confidence Interval of Single Proportion\n\nEstimate of proportion:\n\n\\[\n\\widehat{p} = \\dfrac{\\# \\text{successes}}{\\# \\text{successes} + \\# \\text{failures}}\n\\]\n\nUse the sampling distribution of \\(\\widehat{p}\\) to construct the confidence interval:\n\n\\((1-\\alpha)\\%\\) confidence interval for estimate proportion:\n\n\n\\[\\begin{align} \\widehat{p} &\\pm z^*_{(1-\\alpha/2)} \\cdot SE_{\\hat{p}} \\\\ \\widehat{p} &\\pm z^*_{(1-\\alpha/2)} \\cdot \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\end{align}\\]\n\nUsing \\(SE_{\\hat{p}} = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\) - instead of \\(\\sigma_{p} = \\sqrt{\\frac{p(1-p)}{n}}\\) - because we don’t know exactly what \\(p\\) is"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#section",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#section",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "",
    "text": "Smoking status example\n\n\nA cross-sectional study of 8681 patients was conducted to evaluate the nature of smoking status among people. Of the 8681 people, 4840 were nonsmokers and 3841 were smokers.\n\n\nNeeded steps:\n\nEstimate proportion \\(\\widehat{p}\\)\nCheck that \\(n\\widehat{p}&gt;10\\) and \\(n(1-\\widehat{p})&gt;10\\) in order to make normal approximation\nConstruct 95% confidence interval\nWrite interpretation"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#section-1",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#section-1",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "",
    "text": "Smoking status example\n\n\nA cross-sectional study of 8681 patients was conducted to evaluate the nature of smoking status among people. Of the 8681 people, 4840 were nonsmokers and 3841 were smokers.\n\n\n\n\n\nEstimate proportion \\(\\widehat{p}\\) \\[ \\widehat{p} = \\dfrac{3841}{3841 + 4840} = \\dfrac{3841}{8681} = 0.44246\\]\nCheck that \\(n\\widehat{p}&gt;10\\) and \\(n(1-\\widehat{p})&gt;10\\) in order to make normal approximation \\[ n\\widehat{p} = 8681\\cdot0.4425 = 3841 &gt; 10\\] \\[ n(1-\\widehat{p}) = 8681\\cdot(1-0.4425) = 4840 &gt; 10\\]\n\n\n\nConstruct 95% confidence interval\n\n\\[ \\widehat{p} \\pm z^*_{0.975} \\cdot \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\]\n\nprop.test(x = 3841, n = 8681, correct = T)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  3841 out of 8681, null probability 0.5\nX-squared = 114.73, df = 1, p-value &lt; 2.2e-16\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.4319827 0.4529896\nsample estimates:\n        p \n0.4424605 \n\n\n\nWrite interpretation of estimate\n\nThe estimated proportion of smokers is 0.442 (95% CI: 0.432, 0.453).\nAdditional interpretation of CI: We are 95% confident that the (population) proportion of smokers is between 0.432 and 0.453."
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#poll-everywhere-question-2",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#poll-everywhere-question-2",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Poll everywhere question 2",
    "text": "Poll everywhere question 2"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#estimate-and-confidence-interval-for-difference-in-proportions",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#estimate-and-confidence-interval-for-difference-in-proportions",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Estimate and Confidence Interval for Difference in Proportions",
    "text": "Estimate and Confidence Interval for Difference in Proportions\n\nUse the sampling distribution of \\(\\widehat{p}_1\\) and \\(\\widehat{p}_2\\) to construct the confidence interval:\n\n\\((1-\\alpha)\\%\\) confidence interval for estimate difference in proportions:\n\n\n\\[\\begin{align} \\widehat{p}_1 - \\widehat{p}_2 &\\pm z^*_{(1-\\alpha/2)} \\cdot SE_{\\hat{p}_1 - \\hat{p}_2} \\\\ \\widehat{p}_1 - \\widehat{p}_2 &\\pm z^*_{(1-\\alpha/2)} \\cdot \\sqrt{\n\\frac{\\hat{p}_1\\cdot(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2\\cdot(1-\\hat{p}_2)}{n_2}} \\end{align}\\]\n\nUsing \\(SE_{\\hat{p}_1 - \\hat{p}_2} = \\sqrt{\\frac{\\hat{p}_1\\cdot(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2\\cdot(1-\\hat{p}_2)}{n_2}}\\) because we don’t know exactly what \\(p_1\\) and \\(p_2\\) are"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#poll-everywhere-question-3",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#poll-everywhere-question-3",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#example-strong-heart-study-12",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#example-strong-heart-study-12",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Example: Strong Heart Study (1/2)",
    "text": "Example: Strong Heart Study (1/2)\n\n\nThe Strong Heart Study is an ongoing study of American Indians residing in 13 tribal communities in three geographic areas (AZ, OK, and SD/ND) to study prevalence and incidence of cardiovascular disease and to identify risk factors. We will be examining the 4-year cumulative incidence of diabetes with one risk factor, glucose tolerance.\n \n\nImpaired glucose: normal or impaired glucose tolerance at baseline visit (between 1988 and 1991)\n \nDiabetes: Indicator of diabetes at follow-up visit (roughly four years after baseline) according to two-hour oral glucose tolerance test"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#example-strong-heart-study-22",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#example-strong-heart-study-22",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Example: Strong Heart Study (2/2)",
    "text": "Example: Strong Heart Study (2/2)\nThere is a total of 1664 American Indians in the dataset, with the following distribution of folks with diabetes and glucose tolerance:\n \n\n\n\n#shs_data = read.csv(file = here(\"./data/SHS_data.csv\"))\n\n\nSHS = tibble(Diabetes = c(rep(\"Not diabetic\", \n                   1338), \n                   rep(\"Diabetic\", 326)),\n              Glucose = c(rep(\"Normal\", \n                  1004),#Not diabetic\n          rep(\"Impaired\", 334),\n          rep(\"Normal\", \n              128), #Diabetic\n          rep(\"Impaired\", 198)))\n\n\n\n\nDisplaying the contingency table in R\nSHS %&gt;% tabyl(Glucose, Diabetes) %&gt;% \n  adorn_totals(where = c(\"row\", \"col\")) %&gt;% \n  gt() %&gt;% \n  tab_stubhead(label = \"Glucose Impairment\") %&gt;%\n  tab_spanner(label = \"Diabetes\", \n              columns = c(\"Not diabetic\", \"Diabetic\")) %&gt;%\n  tab_options(table.font.size = 45)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose\n\nDiabetes\n\nTotal\n\n\nNot diabetic\nDiabetic\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#section-2",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#section-2",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "",
    "text": "Strong Heart Study\n\n\nWhat is the difference in proportions for American Indians that have diabetes comparing individuals with normal vs. impaired glucose?\n\n\nNeeded steps:\n\nEstimate the difference in proportions\nCheck that each cell has at least 10 individuals\nConstruct 95% confidence interval\nWrite interpretation of estimate"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#section-3",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#section-3",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "",
    "text": "Strong Heart Study\n\n\nWhat is the difference in proportions for American Indians that have diabetes comparing individuals with normal vs. impaired glucose?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose\n\nDiabetes\n\nTotal\n\n\nNot diabetic\nDiabetic\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\nEstimate the difference in proportions \\[ \\widehat{p}_1 -\\widehat{p}_2 = \\dfrac{198}{532} - \\dfrac{128}{1132} = 0.2591\\]\nCheck that each cell has at least 10 individuals\n\n\n\nConstruct 95% confidence interval\n\n\nprop.test(x = table(SHS$Glucose, SHS$Diabetes), \n          correct = T)\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  table(SHS$Glucose, SHS$Diabetes)\nX-squared = 152.6, df = 1, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n95 percent confidence interval:\n 0.2126963 0.3055162\nsample estimates:\n   prop 1    prop 2 \n0.3721805 0.1130742 \n\n\n\nWrite interpretation of estimate\n\nThe estimated difference in proportion of diabetic American Indians comparing is 0.259 (95% CI: 0.213, 0.306).\nAdditional interpretation of CI: We are 95% confident that the difference in (population) proportions of American Indians who have normal glucose tolerance and impaired glucose tolerance that developed diabetes is between 0.213 and 0.306."
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#mcnemars-test",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#mcnemars-test",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "McNemar’s Test",
    "text": "McNemar’s Test\n\nMcNemar’s test should be used if data is from a matched pairs study\n\n \n\nWhat is a matched-pairs study?\n\nParticipants are paired based on key characteristics\nEach participant within a pair will be assigned to different treatment groups\n\nCategorical test that is parallel to the “paired t-test”\n\n \n\nR packages and functions\n\nNormal approximation: mcnemar.test() in built-in stats package\nExact test: mcnemar.exact() in exact2x2 package\n\n\n \n\nIf you would like more information of McNemar’s test, please see Rosner TB: 10.4 and 10.5: Paired Samples"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#summary-so-far",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#summary-so-far",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Summary so far",
    "text": "Summary so far\n\nIntroduced categorical data as the response in analysis\nReviewed an important distribution (Binomial distribution) for categorical data analysis\nEstimated a single proportion from a sample with its confidence interval\nEstimated a difference in proportions from a sample with its confidence interval\n\n   \n\nCan we expand this to ask a more general question about association between a response and explanatory variable?\n\nWhat if there is more than 2 categories for either variable?"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#contingency-tables-r-x-c",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#contingency-tables-r-x-c",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Contingency Tables (R x C)",
    "text": "Contingency Tables (R x C)\n\nR X C contingency tables\n\nContains information for two discrete variables: one has R categories and the other has C categories.\nRefers to the number of rows (R) and number of columns (C) in the table\n\n\n \n\nFor two proportions: focused on 2 X 2 contingency tables\n\nR = 2, C = 2\n\n\n \n\nExpand our contingency tables to variables with 2 or more categories\n\nCategories can be ordinal or nominal"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#contingency-table-example",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#contingency-table-example",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Contingency Table: Example",
    "text": "Contingency Table: Example\nLet’s say we are interested in learning the association between the development of breast cancer and age at first birth. Our first step is typically to present the observed data:\n\n\nThis is a 2 x 5 contingency table"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-associationtrend-of-r-x-c-contingency-table",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-associationtrend-of-r-x-c-contingency-table",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Test Association/Trend of R X C Contingency Table",
    "text": "Test Association/Trend of R X C Contingency Table\n \n\n\nIf both variables are nominal, a test of general association will be sufficient\n\nTest of general association is the same regardless of R and C\nTest used for 2x2 contingency table same as 5x3 contingency table\nWe will cover:\n\nChi-squared test\nFisher Exact test\n\n\n\n\n\nIf one or both variables are ordinal, a test of trend may be of interest\n\nTreats ordinal variables as quantitative rather than qualitative (nominal scale)\nTest of trend has greater power than the test of general association\nWe will cover:\n\nCochran-Armitage test\nMantel-Haenszel test"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#poll-everywhere-question-4",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#poll-everywhere-question-4",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-general-association",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-general-association",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Test of General Association",
    "text": "Test of General Association\n\nGeneral research question: Are two variables (both categorical, nominal) associated with each other?\n\n \n\nTranslated to a hypothesis test:\n\n\\(H_0\\) : There is no association between the two variables / The variables are independent\n\\(H_1\\) : There is an association between the two variables / The variables are not independent\n\n\n   \n\nWe have two options for testing general association:\n\nChi-squared test\nFisher’s Exact test"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-general-association-shs-example",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-general-association-shs-example",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Test of General Association: SHS Example",
    "text": "Test of General Association: SHS Example\n\nMain question: Do American Indians with impaired glucose tolerance have a different incidence of diabetes?\n\nIs glucose tolerance associated with diabetes incidence among American Indians?\n\nWe have two variables, and both variables have two nominal categories\n\nDiabetes outcome: Not diabetic and Diabetic\nGlucose tolerance: Normal or Impaired\n\n\n \n\nAnswer research question with a test of general association\nHypothesis:\n\n\\(H_0\\) : There is no association between glucose tolerance and diabetes / Glucose tolerance and diabetes are independent\n\\(H_1\\) : There is an association between glucose tolerance and diabetes / Glucose tolerance and diabetes are not independent"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Chi-squared test",
    "text": "Chi-squared test\n\nTest to see how likely is it that we observe our data given the null hypothesis (no association)\n\n \n\nWe use the null to calculate the expected cell counts and compare them to the observed cell counts\n\n \n\nRequirements to conduct Chi-squared test (expected cell counts)\n\nFor 2 x 2 contingency table:\n\nNo expected cell counts should be less than 10\n\nFor contingency table with 3x2, 3x3, 4x4, etc.:\n\nNo more than 20% of expected cell counts are less than 5\nNo expected cell counts are less than 1"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test-process",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test-process",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Chi-squared test: Process",
    "text": "Chi-squared test: Process\n\nCheck that the expected cell counts threshold is met\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic and p-value for Chi-squared test in R\n\nWe will not discuss the test statistic’s equation\n\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test-expected-cell-counts",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test-expected-cell-counts",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Chi-squared test: Expected Cell Counts",
    "text": "Chi-squared test: Expected Cell Counts\n\nIs the sample size big enough for the chi-square test to be adequate? What are the expected cell counts?\n\n \n\nIf you want an explanation of how to calculate by hand, please see Vu and Harringtion TB (section 8.3.1, page 405)\n\n \n\nToo time consuming for this class, but R does it quickly using the expected() function in the epitools package"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test-expected-cell-counts-1",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test-expected-cell-counts-1",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Chi-squared test: Expected Cell Counts",
    "text": "Chi-squared test: Expected Cell Counts\n\nIn the Strong Heart Study…\n\n\nSHS_table = table(SHS$Glucose, SHS$Diabetes)\nSHS_table\n\n          \n           Diabetic Not diabetic\n  Impaired      198          334\n  Normal        128         1004\n\nlibrary(epitools)\nexpected(SHS_table)\n\n          \n           Diabetic Not diabetic\n  Impaired  104.226      427.774\n  Normal    221.774      910.226\n\n\n \n\n\n\n\n\nAll expected counts &gt; 5"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test-shs-example",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test-shs-example",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Chi-squared test: SHS Example",
    "text": "Chi-squared test: SHS Example\n\n\n\nCheck expected cell counts threshold\n\n\nexpected(SHS_table)\n\n          \n           Diabetic Not diabetic\n  Impaired  104.226      427.774\n  Normal    221.774      910.226\n\n\nAll expected cells are greater than 5.\n\n\\(\\alpha = 0.05\\)\nHypothesis test:\n\n\\(H_0\\) : There is no association between glucose tolerance and diabetes\n\\(H_1\\) : There is an association between glucose tolerance and diabetes\n\n\n\n\nCalculate the test statistic and p-value for Chi-squared test in R\n\n\nchisq.test(x = SHS_table, correct = T)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  SHS_table\nX-squared = 152.6, df = 1, p-value &lt; 2.2e-16\n\n\n\nConclusion to the hypothesis test\n\nWe reject the null hypothesis that glucose tolerance and diabetes are not associated (\\(p&lt;2.2\\cdot10^{-16}\\)). There is sufficient evidence that glucose tolerance and diabetes incidence are associated among American Indians."
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Fisher’s Exact Test",
    "text": "Fisher’s Exact Test\n\nOnly necessary when expected counts in one or more cells is less than 5\nGiven row and column totals fixed, computes exact probability that we observe our data or more extreme data\nConsider a general 2 x 2 table:\n\n\n\nThe exact probability of observing a table with cells (a, b, c, d) can be computed based on the hypergeometric distribution\n\n\\[P(a, b, c, d) = \\dfrac{(a+b)!\\cdot(c+d)!\\cdot(a+c)!\\cdot(b+d)!}{n!\\cdot a!\\cdot b!\\cdot c!\\cdot d!}\\]\n\nNumerator is fixed and denominator changes"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-process",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-process",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Fisher’s Exact test: Process",
    "text": "Fisher’s Exact test: Process\n\nCheck the expected cell counts\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic and p-value for Fisher Exact test in R\n\nWe will not discuss the test statistic’s equation\n\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-shs-example-not-appropriate-use-of-fishers-exact",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-shs-example-not-appropriate-use-of-fishers-exact",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Fisher’s Exact test: SHS Example (not appropriate use of Fisher’s Exact)",
    "text": "Fisher’s Exact test: SHS Example (not appropriate use of Fisher’s Exact)\n\n\n\nCheck expected cell counts threshold\n\n\nexpected(SHS_table)\n\n          \n           Diabetic Not diabetic\n  Impaired  104.226      427.774\n  Normal    221.774      910.226\n\n\nWe’re going to pretend they are less than 5.\n\n\\(\\alpha = 0.05\\)\nHypothesis test:\n\n\\(H_0\\) : There is no association between glucose tolerance and diabetes\n\\(H_1\\) : There is an association between glucose tolerance and diabetes\n\n\n\n\nCalculate the test statistic and p-value for Chi-squared test in R\n\n\nfisher.test(x = SHS_table)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  SHS_table\np-value &lt; 2.2e-16\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 3.576595 6.048639\nsample estimates:\nodds ratio \n  4.644825 \n\n\n\nConclusion to the hypothesis test\n\nWe reject the null hypothesis that glucose tolerance and diabetes are not associated (\\(p&lt;2.2\\cdot10^{-16}\\)). There is sufficient evidence that glucose tolerance and diabetes incidence are associated among American Indians."
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-trend",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-trend",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Test of Trend",
    "text": "Test of Trend\n\nIf one or both variables are ordinal, a test of trend may be of interest\n\nTreats ordinal variables as quantitative rather than qualitative (nominal scale)\nTest of trend has greater power than the test of general association\nYou can use a test of general association for non-ordinal variables\n\n\n \n\nTwo tests of trend that we we learn:\n\nCochran-Armitage test\n\nTests association between a binary response and an ordinal explanatory variable\n\nMantel-Haenszel test\n\nTest association between an ordinal response and an ordinal explanatory variable"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Cochran-Armitage test",
    "text": "Cochran-Armitage test\n\nCochran-Armitage test for trend will determine if there is association between a binary response variable and an ordinal variable with 3 or more categories\n\n \n\nIt will test the trend of the proportions over the ordinal variable\n\nAnswers the question: Does the proportion of people with a “successful” outcome increase as the ordinal explanatory variable increases?\n\n\n \n\nCochran-Armitage test for trend is only suitable for 2 x C contingency tables"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-hypothesis-test",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-hypothesis-test",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Cochran-Armitage test: Hypothesis Test",
    "text": "Cochran-Armitage test: Hypothesis Test\n\n\n\n\nNull Hypothesis (\\(H_0\\))\n\n\nThe proportions of successes are the same across all C ordinal values of the explanatory variable. \\[p_1 = p_2 = ... = p_C\\]\n\n\n\n\n\nAlternative Hypothesis (\\(H_1\\))\n\n\nThe proportions of successes tend to increase as ordinal value of the explanatory variable increases\n\\[p_1 \\leq p_2 \\leq ... \\leq p_C\\]\nOR\nThe proportions of successes tend to decrease as ordinal value of the explanatory variable increases\n\\[p_1 \\geq p_2 \\geq ... \\geq p_C\\]"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-process",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-process",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Cochran-Armitage test: Process",
    "text": "Cochran-Armitage test: Process\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic and p-value for Cochran-Armitage test in R\n\nWe will not discuss the test statistic’s equation\nJust know it follows a Normal distribution\n\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-example-13",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-example-13",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Cochran-Armitage test: Example (1/3)",
    "text": "Cochran-Armitage test: Example (1/3)\nWe are interested in learning the association between the development of breast cancer and age at first birth among people who have given birth"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-example-23",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-example-23",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Cochran-Armitage test: Example (2/3)",
    "text": "Cochran-Armitage test: Example (2/3)\n\nBefore we go into the hypothesis test procedure, we need to construct the contingency table in R\n\n\nCancer = c(320, 1206, 1011, 463, 220)\nNo_Cancer = c(1422, 4432, 2893, 1092, 406)\nbscancer = matrix (c(Cancer, No_Cancer), nrow = 2, byrow = T)\nrownames(bscancer) = c(\"Cancer\",\"No Cancer\")\ncolnames(bscancer) = c(\"&lt;20\",\"20-24\",\"25-29\",\"30-34\",\"&gt;=35\")\nbscancer\n\n           &lt;20 20-24 25-29 30-34 &gt;=35\nCancer     320  1206  1011   463  220\nNo Cancer 1422  4432  2893  1092  406\n\n\n\nIt does not need to be pretty to use in function"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-example-33",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-example-33",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Cochran-Armitage test: Example (3/3)",
    "text": "Cochran-Armitage test: Example (3/3)\n\n\n\n\\(\\alpha = 0.05\\)\nHypothesis test:\n\n\\(H_0\\): The proportions of breast cancer are the same for all age levels of first birth. \\[p_1 = p_2 = ... = p_5\\]\n\\(H_1\\): The proportions of breast cancer tends to increase as level of age of first birth increases\n\n\n\\[p_1 \\leq p_2 \\leq ... \\leq p_5\\]\n\n\nCalculate the test statistic and p-value for Cochran-Armitage test in R\n\n\nlibrary(DescTools)\nCochranArmitageTest(bscancer)\n\n\n    Cochran-Armitage test for trend\n\ndata:  bscancer\nZ = 11.358, dim = 5, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\nConclusion to the hypothesis test\n\nWe reject the null hypothesis that proportions of breast cancer are the same for all age levels of first birth (\\(p&lt;2.2\\cdot10^{-16}\\)). There is sufficient evidence that the proportion of of breast cancer increase as the the age at first birth increases."
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Mantel-Haenszel test",
    "text": "Mantel-Haenszel test\n\nWhen both variables are ordinal, we can conduct Mantel-Haenszel test of trend for linear association\nMantel-Haenszel test for linear trend is suitable for any R x C contingency tables with two ordinal variables\nHypothesis test:\n\n\n\n\n\nNull Hypothesis (\\(H_0\\))\n\n\nThere is no correlation between the two variables \\[ \\rho = 0\\]\n\n\n\n\n\nAlternative Hypothesis (\\(H_1\\))\n\n\nThere is correlation between the two variables\n\\[ \\rho \\neq 0\\]"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-process",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-process",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Mantel-Haenszel test: Process",
    "text": "Mantel-Haenszel test: Process\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic and p-value for Mantel-Haenszel test in R\n\nWe will not discuss the test statistic’s equation\n\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-example-13",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-example-13",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Mantel-Haenszel test: Example (1/3)",
    "text": "Mantel-Haenszel test: Example (1/3)\nA water treatment company is studying water additives and investigating how they affect clothes washing (through measurements of abrasions, wearing, and color loss).\nThe treatments studies where no treatment (plain water), the standard treatment, and a double dose of the standard treatment, called super. Washability was measured as low, medium and high.\nAre levels of washability associated with treatment?"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-example-23",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-example-23",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Mantel-Haenszel test: Example (2/3)",
    "text": "Mantel-Haenszel test: Example (2/3)\n\nBefore we go into the hypothesis test procedure, we need to construct the contingency table in R\n\n\nwater = matrix (c(27, 14, 5, 10, 17, 26, 5, 12, 50), nrow = 3, byrow = T)\nrownames(water) = c(\"plain\",\"standard\",\"super\")\ncolnames(water) = c(\"low\",\"medium\",\"high\")\nwater\n\n         low medium high\nplain     27     14    5\nstandard  10     17   26\nsuper      5     12   50\n\n\n\nIt does not need to be pretty to use in function"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-example-33",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-example-33",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Mantel-Haenszel test: Example (3/3)",
    "text": "Mantel-Haenszel test: Example (3/3)\n\n\n\n\\(\\alpha = 0.05\\)\nHypothesis test:\n\n\\(H_0\\): Water treatment and washability are not correlated. \\[ \\rho = 0\\]\n\\(H_1\\): Water treatment and washability are correlated.\n\n\n\\[ \\rho \\neq 0\\]\n\n\nCalculate the test statistic and p-value for Mantel-Haenszel test in R\n\n\nlibrary(DescTools)\nMHChisqTest(water)\n\n\n    Mantel-Haenszel Chi-Square\n\ndata:  water\nX-squared = 50.602, df = 1, p-value = 1.132e-12\n\n\n\nConclusion to the hypothesis test\n\nWe reject the null hypothesis that there is no correlation between washability and water treatment (\\(p = 1.13 \\cdot 10^{-12} &lt; 0.05\\)). There is sufficient evidence that level of water treatment is associated with washability."
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#more-resources",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#more-resources",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "More resources",
    "text": "More resources\n\nFor a refresher or review of one proportion and differences in proportions\n\nAnd their power calculations\nFrom Meike’s BSTA 511 course (see Day 12!)\n\nFor a refresher or review of Chi-squared test or Fisher’s Exact test\n\nFrom Meike’s BSTA 511 course (see Day 13!)"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#last-class",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#last-class",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Last class",
    "text": "Last class\n\nLooked at simple logistic regression for binary outcome with\n\nOne continuous predictor \\[\\text{logit}(\\pi(X)) = \\beta_0 + \\beta_1 \\cdot X\\]\nOne binary predictor \\[\\text{logit}\\left(\\pi(X) \\right) = \\beta_0 + \\beta_1 \\cdot I(X=1)\\]\nOne multi-level predictor \\[\\text{logit}\\left(\\pi(X) \\right) = \\beta_0 + \\beta_1 \\cdot I(X=b) + \\beta_2 \\cdot I(X=c) + \\beta_3 \\cdot I(X=d)\\]"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#breast-cancer-example",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#breast-cancer-example",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Breast Cancer example",
    "text": "Breast Cancer example\n\nFor breast cancer diagnosis example, recall:\n\nOutcome: early or late stage breast cancer diagnosis (binary, categorical)\n\n\n \n\nPrimary covariate: Race/ethnicity\n\nNon-Hispanic white individuals are more likely to be diagnosed with breast cancer\n\nBut POC are more likely to be diagnosed at a later stage\n\n\n\n \n\nAdditional covariate: Age\n\nRisk factor for cancer diagnosis\n\n\n \n\nWe want to fit a multiple logistic regression model with both risk factors included as independent variables"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#introduction-to-multiple-logistic-regression",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#introduction-to-multiple-logistic-regression",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Introduction to Multiple Logistic Regression",
    "text": "Introduction to Multiple Logistic Regression\n\nIn multiple logistic regression model, we have &gt; 1 independent variable\n\nSometimes referred to as the “multivariable regression”\nThe independent variable can be any type:\n\nContinuous\nCategorical (ordinal or nominal)\n\n\n\n \n\nWe will follow similar procedures as we did for simple logistic regression\n\nBut we need to change our interpretation of estimates because we are adjusting for other variables"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#multiple-logistic-regression-model",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#multiple-logistic-regression-model",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Multiple Logistic Regression Model",
    "text": "Multiple Logistic Regression Model\n\n\n\nAssume we have a collection of \\(k\\) independent variables, denoted by \\(\\mathbf{X}=\\left( X_1, X_2, ..., X_k \\right)\\)\n\n \n\nThe conditional probability is \\(P(Y=1 | \\mathbf{X}) = \\pi(\\mathbf{X})\\)\n\n \n\nWe then model the probability with logistic regression: \\[\\text{logit}\\left(\\pi(\\mathbf{X})\\right) = \\beta_0\n+\\beta_1 \\cdot X_1 +\\beta_2 \\cdot X_2  +\\beta_3 \\cdot X_3 + ... + \\beta_k \\cdot X_k\\]\n\n\n\n\n\n\nWhy the bold \\(X\\)?\n\n\n\\(\\mathbf{X}\\) represents the vector of all the \\(X\\)’s. This is how we represent our group of covariates in our model."
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#fitting-the-multiple-logistic-regression-model",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#fitting-the-multiple-logistic-regression-model",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Fitting the Multiple Logistic Regression Model",
    "text": "Fitting the Multiple Logistic Regression Model\n\nFor a multiple logistic regression model with \\(k\\) independent variables, the vector of coefficients can be denoted by \\[\\boldsymbol{\\beta}^{T} = \\left(\\beta_0, \\beta_1, \\beta_2, ..., \\beta_k \\right)\\]\n\n \n\nAs with the simple logistic regression, we use maximum likelihood method for estimating coefficients\n\nVector of estimated coefficients: \\[\\widehat{\\boldsymbol{\\beta}}^{T} = \\left(\\widehat{\\beta}_0, \\widehat{\\beta}_1, \\widehat{\\beta}_2, ..., \\widehat{\\beta}_k \\right)\\]\n\n\n \n\nFor a model with \\(k\\) independent variables, there is \\(k+1\\) coefficients to estimate\n\nUnless one of those independent variables is a multi-level categorical variables, then we need more than \\(k+1\\) coefficients"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#breast-cancer-example-population-model",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#breast-cancer-example-population-model",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Breast Cancer Example: Population Model",
    "text": "Breast Cancer Example: Population Model\n\nWe can fit a logistic regression model with both race and ethnicity and age:\n\n\\[ \\begin{aligned}\n\\text{logit}\\left(\\pi(\\mathbf{X})\\right) = & \\beta_0\n+\\beta_1 \\cdot I \\left( R/E = H/L \\right)\n+\\beta_2 \\cdot I \\left( R/E = NH AIAN \\right) \\\\\n& +\\beta_3 \\cdot I \\left( R/E = NH API \\right)\n+\\beta_4 \\cdot I \\left( R/E = NH B \\right)\n+\\beta_5 \\cdot Age\n\\end{aligned}\\]\n \n \n\nNote that race and ethnicity requires 4 coefficients to include the indicator for each category\n\n \n\nCan replace \\(\\pi(\\mathbf{X})\\) with \\(\\pi({\\text{Race/ethnicity, Age}})\\)\n\n \n\n6 total coefficients (\\(\\beta_0\\) to \\(\\beta_5\\))"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#fitting-multiple-logistic-regression-model",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#fitting-multiple-logistic-regression-model",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Fitting Multiple Logistic Regression Model",
    "text": "Fitting Multiple Logistic Regression Model\n\nmulti_bc = glm(Late_stage_diag ~ Race_Ethnicity + Age_c, data = bc, family = binomial)\nsummary(multi_bc)\n\n\nCall:\nglm(formula = Late_stage_diag ~ Race_Ethnicity + Age_c, family = binomial, \n    data = bc)\n\nCoefficients:\n                                                 Estimate Std. Error z value\n(Intercept)                                     -1.038389   0.027292 -38.048\nRace_EthnicityHispanic-Latino                   -0.015424   0.083653  -0.184\nRace_EthnicityNH American Indian/Alaskan Native -0.085704   0.484110  -0.177\nRace_EthnicityNH Asian/Pacific Islander          0.133965   0.083797   1.599\nRace_EthnicityNH Black                           0.357692   0.071789   4.983\nAge_c                                            0.057151   0.003209  17.811\n                                                Pr(&gt;|z|)    \n(Intercept)                                      &lt; 2e-16 ***\nRace_EthnicityHispanic-Latino                      0.854    \nRace_EthnicityNH American Indian/Alaskan Native    0.859    \nRace_EthnicityNH Asian/Pacific Islander            0.110    \nRace_EthnicityNH Black                          6.27e-07 ***\nAge_c                                            &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 11861  on 9999  degrees of freedom\nResidual deviance: 11484  on 9994  degrees of freedom\nAIC: 11496\n\nNumber of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#breast-cancer-example-fitted-model",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#breast-cancer-example-fitted-model",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Breast Cancer Example: Fitted Model",
    "text": "Breast Cancer Example: Fitted Model\n\nWe now have the fitted logistic regression model with both race and ethnicity and age:\n\n\\[ \\begin{aligned}\n\\text{logit}\\left(\\widehat{\\pi}(\\mathbf{X})\\right) = & \\widehat{\\beta}_0\n+ \\widehat{\\beta}_1 \\cdot I \\left( R/E = H/L \\right)\n+ \\widehat{\\beta}_2 \\cdot I \\left( R/E = NH AIAN \\right) \\\\\n& + \\widehat{\\beta}_3 \\cdot I \\left( R/E = NH API \\right)\n+ \\widehat{\\beta}_4 \\cdot I \\left( R/E = NH B \\right)\n+ \\widehat{\\beta}_5 \\cdot Age \\\\\n\\\\\n\\text{logit}\\left(\\widehat{\\pi}(\\mathbf{X})\\right) = &-4.56\n-0.02 \\cdot I \\left( R/E = H/L \\right)\n-0.09 \\cdot I \\left( R/E = NH AIAN \\right) \\\\\n& +0.13 \\cdot I \\left( R/E = NH API \\right)\n+0.36 \\cdot I \\left( R/E = NH B \\right)\n+0.06 \\cdot Age\n\\end{aligned}\\]\n\n6 total coefficients (\\(\\widehat{\\beta}_0\\) to \\(\\widehat{\\beta}_5\\))"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#testing-significance-of-the-coefficients",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#testing-significance-of-the-coefficients",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Testing Significance of the Coefficients",
    "text": "Testing Significance of the Coefficients\n\nRefer to Lesson 6 for more information on each test!!\n\n \n\nWe use the same three tests that we discussed in Simple Logistic Regression to test individual coefficients\n\nWald test\n\nCan be used to test a single coefficient\n\nScore test\nLikelihood ratio test (LRT)\n\nCan be used to test a single coefficient or multiple coefficients\n\n\n\n \n\nTextbook and our class focuses on Wald and LRT only"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#a-note-on-wording",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#a-note-on-wording",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "A note on wording",
    "text": "A note on wording\n\nWhen I say “test a single coefficient” or “test multiple coefficients” I am referring to the \\(\\beta\\)’s\n\nA single variable can have a single coefficient\n\nExample: testing age\n\nA single variable can have multiple coefficients\n\nExample: testing race and ethnicity\n\nMuliple variables will have multiple coefficients\n\nExample: testing age and race and ethnicity together\n\n\n \nWhen I say “test a variable” I mean “determine if the model with the variable is more likely than the model without that variable”\n\nWe can use the Wald test to do this is some scenarios (single, continuous covariate)\nBUT I advise you practice using the LRT whenever comparing models (aka testing variables)"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#all-three-tests-together",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#all-three-tests-together",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "All three tests together",
    "text": "All three tests together"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#from-lesson-6-wald-test",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#from-lesson-6-wald-test",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "From Lesson 6: Wald test",
    "text": "From Lesson 6: Wald test\n\nAssumes test statistic W follows a standard normal distribution under the null hypothesis\nTest statistic: \\[W=\\frac{{\\hat{\\beta}}_j}{SE_{\\hat{\\beta}_j}}\\sim N(0,1)\\]\n\nwhere \\(\\widehat{\\beta}_j\\) is a MLE of coefficient \\(j\\)\n\n95% Wald confidence interval: \\[{\\hat{\\beta}}_1\\pm1.96 \\cdot SE_{{\\hat{\\beta}}_j}\\]\nThe Wald test is a routine output in R (summary() of glm() output)\n\nIncludes \\(SE_{{\\hat{\\beta}}_j}\\) and can easily find confidence interval with tidy()\n\nImportant note: Wald test is best for confidence intervals of our coefficient estimates or estimated odds ratios."
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#wald-test-procedure-with-confidence-intervals",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#wald-test-procedure-with-confidence-intervals",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Wald test procedure with confidence intervals",
    "text": "Wald test procedure with confidence intervals\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the confidence interval and determine if it overlaps with null\n\nOverlap with null (usually 0 for coefficient) = fail to reject null\nNo overlap with null (usually 0 for coefficient) = reject null\n\nWrite a conclusion to the hypothesis test\n\nWhat is the estimate and its confidence interval?\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#wald-test-in-our-example",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#wald-test-in-our-example",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Wald test in our example",
    "text": "Wald test in our example\n\nFrom our multiple logistic regression model:\n\n\\[ \\begin{aligned}\n\\text{logit}\\left(\\pi(x_i)\\right) = & \\beta_0\n+\\beta_1 \\cdot I \\left( R/E = H/L \\right)\n+\\beta_2 \\cdot I \\left( R/E = NH AIAN \\right) \\\\\n& +\\beta_3 \\cdot I \\left( R/E = NH API \\right)\n+\\beta_4 \\cdot I \\left( R/E = NH B \\right)\n+\\beta_5 \\cdot Age\n\\end{aligned}\\]\n \n\nWald test can help us construct the confidence interval for ALL coefficient estimates\n\n \n\nIf we want to use the Wald test to determine if a covariate is significant in our model\n\nCan only do so for age"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\nNeeded steps:\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\nCalculate the confidence interval and determine if it overlaps with null\nWrite a conclusion to the hypothesis test"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-1",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\n\nSet the level of significance \\(\\alpha\\)\n\n\\(\\alpha = 0.05\\)\n\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\n\\(H_0: \\beta_5 = 0\\)\n\\(H_1: \\beta_5 \\neq 0\\)"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-2",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-2",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\n\nCalculate the confidence interval and determine if it overlaps with null\n\n\nLook at coefficient estimates (CI should not contain 0) OR estimated odds ratio (CI should not contain 1)\n\n\n\n\ntidy(multi_bc, conf.int=T) %&gt;% \n  gt() %&gt;% \n  tab_options(table.font.size = 28) %&gt;%\n  fmt_number(decimals = 2)\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n−1.04\n0.03\n−38.05\n0.00\n−1.09\n−0.99\n\n\nRace_EthnicityHispanic-Latino\n−0.02\n0.08\n−0.18\n0.85\n−0.18\n0.15\n\n\nRace_EthnicityNH American Indian/Alaskan Native\n−0.09\n0.48\n−0.18\n0.86\n−1.12\n0.81\n\n\nRace_EthnicityNH Asian/Pacific Islander\n0.13\n0.08\n1.60\n0.11\n−0.03\n0.30\n\n\nRace_EthnicityNH Black\n0.36\n0.07\n4.98\n0.00\n0.22\n0.50\n\n\nAge_c\n0.06\n0.00\n17.81\n0.00\n0.05\n0.06"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-3",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-3",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\n\nCalculate the confidence interval and determine if it overlaps with null\n\n\nLook at coefficient estimates (CI should not contain 0) OR estimated odds ratio (CI should not contain 1)\n\n\n\n\ntbl_regression(multi_bc, \n               exponentiate = TRUE) %&gt;%\n  as_gt() %&gt;% \n  tab_options(table.font.size = 30)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\n\nRace_Ethnicity\n\n\n\n\n\n\n\n\n    NH White\n—\n—\n\n\n\n\n    Hispanic-Latino\n0.98\n0.83, 1.16\n0.9\n\n\n    NH American Indian/Alaskan Native\n0.92\n0.33, 2.25\n0.9\n\n\n    NH Asian/Pacific Islander\n1.14\n0.97, 1.35\n0.11\n\n\n    NH Black\n1.43\n1.24, 1.65\n&lt;0.001\n\n\nAge_c\n1.06\n1.05, 1.07\n&lt;0.001\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-4",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-4",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\n\nWrite a conclusion to the hypothesis test\n\nGiven race and ethnicity is already in the model, the regression model with age is more likely than the model without age (p-val &lt; 0.001)."
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#likelihood-ratio-test",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#likelihood-ratio-test",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Likelihood ratio test",
    "text": "Likelihood ratio test\n\nLikelihood ratio test answers the question:\n\nFor a specific covariate, which model tell us more about the outcome variable: the model including the covariate (or set of covariates) or the model omitting the covariate (or set of covariates)?\nAka: Which model is more likely given our data: model including the covariate or the model omitting the covariate?\n\n\n \n\nTest a single coefficient by comparing different models\n\nVery similar to the F-test\n\n\n \n\nImportant: LRT can be used conduct hypothesis tests for multiple coefficients\n\nJust like F-test, we can test a single coefficient, continuous/binary covariate, multi-level covariate, or multiple covariates"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#likelihood-ratio-test-33",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#likelihood-ratio-test-33",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Likelihood ratio test (3/3)",
    "text": "Likelihood ratio test (3/3)\n\nIf testing single variable and it’s continuous or binary, still use this hypothesis test:\n\n\\(H_0\\): \\(\\beta_j = 0\\)\n\\(H_1\\): \\(\\beta_j \\neq 0\\)\n\n\n \n\nIf testing single variable and it’s categorical with mroe than 2 groups, use this hypothesis test:\n\n\\(H_0\\): \\(\\beta_j=\\beta_{j+1}=\\ldots=\\beta_{j+i-1}=0\\)\n\\(H_1\\): at least one of the above \\(\\beta\\)’s is not equal to 0\n\n\n \n\nIf testing a set of variables, use this hypothesis test:\n\n\\(H_0\\): \\(\\beta_1=\\beta_{2}=\\ldots=\\beta_{k}=0\\)\n\\(H_1\\): at least one of the above \\(\\beta\\)’s is not equal to 0"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#lrt-procedure",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#lrt-procedure",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "LRT procedure",
    "text": "LRT procedure\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic and p-value\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#likelihood-ratio-test-33-1",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#likelihood-ratio-test-33-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Likelihood ratio test (3/3)",
    "text": "Likelihood ratio test (3/3)\n\nFrom our multiple logistic regression model:\n\n\\[ \\begin{aligned}\n\\text{logit}\\left(\\pi(\\mathbf{X})\\right) = & \\beta_0\n+\\beta_1 \\cdot I \\left( R/E = H/L \\right)\n+\\beta_2 \\cdot I \\left( R/E = NH AIAN \\right) \\\\\n& +\\beta_3 \\cdot I \\left( R/E = NH API \\right)\n+\\beta_4 \\cdot I \\left( R/E = NH B \\right)\n+\\beta_5 \\cdot Age\n\\end{aligned}\\]\n\nWe can test a single coefficient or multiple coefficients\n \n\nExample 1: Single, continuous variable: Age\n\n \n\nExample 2: Single, &gt;2 categorical variable: Race and Ethnicity\n\n \n\nExample 3: Set of variables: Race and Ethnicity, and Age"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#reminder-on-nested-models",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#reminder-on-nested-models",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Reminder on nested models",
    "text": "Reminder on nested models\n\nLikelihood ratio test is only suitable to test “nested” models\n“Nested” models means the bigger model (full model) contains all the independent variables of the smaller model (reduced model)\nWe cannot compare the following two models using LRT:\n\nModel 1: \\[ \\begin{aligned} \\text{logit}\\left(\\pi(\\mathbf{X})\\right) = & \\beta_0 +\\beta_1 \\cdot I \\left( R/E = H/L \\right) +\\beta_2 \\cdot I \\left( R/E = NH AIAN \\right) \\\\ & +\\beta_3 \\cdot I \\left( R/E = NH API \\right) +\\beta_4 \\cdot I \\left( R/E = NH B \\right) \\end{aligned}\\]\nModel 2: \\[\\begin{aligned} \\text{logit}\\left(\\pi(Age)\\right) = & \\beta_0+\\beta_1 \\cdot Age  \\end{aligned}\\]\n\nIf the two models to be compared are not nested, likelihood ratio test should not be used"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-5",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-5",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\nNeeded steps:\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\nCalculate the test statistic and p-value\nWrite a conclusion to the hypothesis test"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-6",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-6",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\n\nSet the level of significance \\(\\alpha\\)\n\n\\(\\alpha = 0.05\\)\n\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\n\\(H_0: \\beta_5 = 0\\) or model without age is more likely\n\\(H_1: \\beta_5 \\neq 0\\) or model with age is more likely"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-7",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-7",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\n\nCalculate the test statistic and p-value\n\n\nmulti_bc = glm(Late_stage_diag ~ Race_Ethnicity + Age_c, data = bc, family = binomial)\nre_bc = glm(Late_stage_diag ~ Race_Ethnicity, data = bc, family = binomial)\n\nlmtest::lrtest(multi_bc, re_bc)\n\nLikelihood ratio test\n\nModel 1: Late_stage_diag ~ Race_Ethnicity + Age_c\nModel 2: Late_stage_diag ~ Race_Ethnicity\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n1   6 -5741.8                         \n2   5 -5918.1 -1 352.63  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-8",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-8",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\n\nWrite a conclusion to the hypothesis test\n\nGiven race and ethnicity is already in the model, the regression model with age is more likely than the model without age (p-val &lt; \\(2.2\\cdot10^{-16}\\))."
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-2-single-2-categorical-variable-race-and-ethnicity",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-2-single-2-categorical-variable-race-and-ethnicity",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 2: Single, >2 categorical variable: Race and Ethnicity",
    "text": "Example 2: Single, &gt;2 categorical variable: Race and Ethnicity\n\n\nSingle, &gt;2 categorical variable: Race and Ethnicity\n\n\nGiven age is already in the model, is the regression model with race and ethnicity more likely than the model without race and ethnicity?\n\n\nNeeded steps:\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\nCalculate the test statistic and p-value\nWrite a conclusion to the hypothesis test"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-2-single-2-categorical-variable-race-and-ethnicity-1",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-2-single-2-categorical-variable-race-and-ethnicity-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 2: Single, >2 categorical variable: Race and Ethnicity",
    "text": "Example 2: Single, &gt;2 categorical variable: Race and Ethnicity\n\n\nSingle, &gt;2 categorical variable: Race and Ethnicity\n\n\nGiven age is already in the model, is the regression model with race and ethnicity more likely than the model without race and ethnicity?\n\n\n\nSet the level of significance \\(\\alpha\\)\n\n\\(\\alpha = 0.05\\)\n\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\n\\(H_0: \\beta_1 = \\beta_2 = \\beta_3 = \\beta_4 = 0\\) or model without race and ethnicity is more likely\n$H_1: at least one \\(\\beta\\) is not 0 or model with race and ethnicity is more likely"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-2-single-2-categorical-variable-race-and-ethnicity-2",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-2-single-2-categorical-variable-race-and-ethnicity-2",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 2: Single, >2 categorical variable: Race and Ethnicity",
    "text": "Example 2: Single, &gt;2 categorical variable: Race and Ethnicity\n\n\nSingle, &gt;2 categorical variable: Race and Ethnicity\n\n\nGiven age is already in the model, is the regression model with race and ethnicity more likely than the model without race and ethnicity?\n\n\n\nCalculate the test statistic and p-value\n\n\nmulti_bc = glm(Late_stage_diag ~ Race_Ethnicity + Age_c, data = bc, family = binomial)\nage_bc = glm(Late_stage_diag ~ Age_c, data = bc, family = binomial)\n\nlmtest::lrtest(multi_bc, age_bc)\n\nLikelihood ratio test\n\nModel 1: Late_stage_diag ~ Race_Ethnicity + Age_c\nModel 2: Late_stage_diag ~ Age_c\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n1   6 -5741.8                         \n2   2 -5754.8 -4 26.053  3.087e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-2-single-2-categorical-variable-race-and-ethnicity-3",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-2-single-2-categorical-variable-race-and-ethnicity-3",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 2: Single, >2 categorical variable: Race and Ethnicity",
    "text": "Example 2: Single, &gt;2 categorical variable: Race and Ethnicity\n\n\nSingle, &gt;2 categorical variable: Race and Ethnicity\n\n\nGiven age is already in the model, is the regression model with race and ethnicity more likely than the model without race and ethnicity?\n\n\n\nWrite a conclusion to the hypothesis test\n\nGiven age is already in the model, the regression model with race and ethnicity is more likely than the model without race and ethnicity (p-val = \\(3.1\\cdot10^{-5}\\) &lt; 0.05)."
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-3-set-of-variables-race-and-ethnicity-and-age",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-3-set-of-variables-race-and-ethnicity-and-age",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 3: Set of variables: Race and Ethnicity, and Age",
    "text": "Example 3: Set of variables: Race and Ethnicity, and Age\n\n\nSet of variables: Race and Ethnicity, and Age\n\n\nIs the regression model with race and ethnicity and age more likely than the model without race and ethnicity nor age?\n\n\nNeeded steps:\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\nCalculate the test statistic and p-value\nWrite a conclusion to the hypothesis test"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-3-set-of-variables-race-and-ethnicity-and-age-1",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-3-set-of-variables-race-and-ethnicity-and-age-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 3: Set of variables: Race and Ethnicity, and Age",
    "text": "Example 3: Set of variables: Race and Ethnicity, and Age\n\n\nSet of variables: Race and Ethnicity, and Age\n\n\nIs the regression model with race and ethnicity and age more likely than the model without race and ethnicity nor age?\n\n\n\nSet the level of significance \\(\\alpha\\)\n\n\\(\\alpha = 0.05\\)\n\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\n\\(H_0: \\beta_1 = \\beta_2 = \\beta_3 = \\beta_4 = \\beta_5 = 0\\) or model without race and ethnicity and age is more likely\n$H_1: at least one \\(\\beta\\) is not 0 or model with race and ethnicity and age is more likely"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-3-set-of-variables-race-and-ethnicity-and-age-2",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-3-set-of-variables-race-and-ethnicity-and-age-2",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 3: Set of variables: Race and Ethnicity, and Age",
    "text": "Example 3: Set of variables: Race and Ethnicity, and Age\n\n\nSet of variables: Race and Ethnicity, and Age\n\n\nIs the regression model with race and ethnicity and age more likely than the model without race and ethnicity nor age?\n\n\n\nCalculate the test statistic and p-value\n\n\nmulti_bc = glm(Late_stage_diag ~ Race_Ethnicity + Age_c, data = bc, family = binomial)\nintercept_bc = glm(Late_stage_diag ~ 1, data = bc, family = binomial)\n\nlmtest::lrtest(multi_bc, intercept_bc)\n\nLikelihood ratio test\n\nModel 1: Late_stage_diag ~ Race_Ethnicity + Age_c\nModel 2: Late_stage_diag ~ 1\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n1   6 -5741.8                         \n2   1 -5930.5 -5 377.32  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-3-set-of-variables-race-and-ethnicity-and-age-3",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-3-set-of-variables-race-and-ethnicity-and-age-3",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 3: Set of variables: Race and Ethnicity, and Age",
    "text": "Example 3: Set of variables: Race and Ethnicity, and Age\n\n\nSet of variables: Race and Ethnicity, and Age\n\n\nIs the regression model with race and ethnicity and age more likely than the model without race and ethnicity nor age?\n\n\n\nWrite a conclusion to the hypothesis test\n\nThe regression model with race and ethnicity and age is more likely than the model omitting race and ethnicity and age (p-val &lt; \\(2.2\\cdot10^{-16}\\))."
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#estimatedpredicted-probability-for-mlr",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#estimatedpredicted-probability-for-mlr",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Estimated/Predicted Probability for MLR",
    "text": "Estimated/Predicted Probability for MLR\n\nBasic idea for predicting/estimating probability stays the same\n\n \n\nCalculations will be slightly different\n\nEspecially for the confidence interval\n\n\n \n\nRecall our fitted model for late stage breast cancer diagnosis: \\[ \\begin{aligned}\n\\text{logit}\\left(\\widehat{\\pi}(\\mathbf{X})\\right) = &-4.56\n-0.02 \\cdot I \\left( R/E = H/L \\right)\n-0.09 \\cdot I \\left( R/E = NH AIAN \\right) \\\\\n& +0.13 \\cdot I \\left( R/E = NH API \\right)\n+0.36 \\cdot I \\left( R/E = NH B \\right)\n+0.06 \\cdot Age\n\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicted-probability",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicted-probability",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Predicted Probability",
    "text": "Predicted Probability\n\nWe may be interested in predicting probability of having a late stage breast cancer diagnosis for a specific age.\nThe predicted probability is the estimated probability of having the event for given values of covariate(s)\nRecall our fitted model for late stage breast cancer diagnosis: \\[ \\begin{aligned}\n\\text{logit}\\left(\\widehat{\\pi}(\\mathbf{X})\\right) = &-4.56\n-0.02 \\cdot I \\left( R/E = H/L \\right)\n-0.09 \\cdot I \\left( R/E = NH AIAN \\right) \\\\\n& +0.13 \\cdot I \\left( R/E = NH API \\right)\n+0.36 \\cdot I \\left( R/E = NH B \\right)\n+0.06 \\cdot Age\n\\end{aligned}\\]\nWe can convert it to the predicted probability: \\[\\hat{\\pi}(\\mathbf{X})=\\dfrac{\\exp \\left( \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot I \\left( R/E = H/L \\right) + ... + \\widehat{\\beta}_5 \\cdot Age \\right)}  {1+\\exp \\left(\\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot I \\left( R/E = H/L \\right) + ... + \\widehat{\\beta}_5 \\cdot Age \\right)}\\]\n\nThis is an inverse logit calculation\n\nWe can calculate this using the the predict() function like in BSTA 512\n\nAnother option: taking inverse logit of fitted values from augment() function"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Predicting probability in R",
    "text": "Predicting probability in R\n\n\nPredicting probability of late stage breast cancer diagnosis\n\n\nFor someone who is 60 years old and Non-Hispanic Asian/Pacific Islander, what is the predicted probability for late stage breast cancer diagnosis (with confidence intervals)?\n\n\nNeeded steps:\n\nCalculate probability prediction\nCheck if we can use Normal approximation\nCalculate confidence interval\n\nUsing logit scale then converting\nUsing Normal approximation\n\nInterpret results"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r-1",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Predicting probability in R",
    "text": "Predicting probability in R\n\n\nPredicting probability of late stage breast cancer diagnosis\n\n\nFor someone who is 60 years old and Non-Hispanic Asian/Pacific Islander, what is the predicted probability for late stage breast cancer diagnosis (with confidence intervals)?\n\n\n\nCalculate probability prediction\n\n\nnewdata = data.frame(Age_c = 60 - mean_age, \n                     Race_Ethnicity = \"NH Asian/Pacific Islander\")\npred1 = predict(multi_bc, newdata, se.fit = T, type=\"response\")\npred1\n\n$fit\n        1 \n0.2685667 \n\n$se.fit\n         1 \n0.01572695 \n\n$residual.scale\n[1] 1"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r-2",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r-2",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Predicting probability in R",
    "text": "Predicting probability in R\n\n\nPredicting probability of late stage breast cancer diagnosis\n\n\nFor someone who is 60 years old and Non-Hispanic Asian/Pacific Islander, what is the predicted probability for late stage breast cancer diagnosis (with confidence intervals)?\n\n\n\nCheck if we can use Normal approximation\n\nWe can use the Normal approximation if: \\(\\widehat{p}n = \\widehat{\\pi}(X)\\cdot n &gt; 10\\) and \\((1-\\widehat{p})n = (1-\\widehat{\\pi}(X))\\cdot n &gt; 10\\).\n\nn = nobs(multi_bc)\np = pred1$fit\nn*p\n\n       1 \n2685.667 \n\nn*(1-p)\n\n       1 \n7314.333 \n\n\nWe can use the Normal approximation!"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r-3",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r-3",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Predicting probability in R",
    "text": "Predicting probability in R\n\n\nPredicting probability of late stage breast cancer diagnosis\n\n\nFor someone who is 60 years old and Non-Hispanic Asian/Pacific Islander, what is the predicted probability for late stage breast cancer diagnosis (with confidence intervals)?\n\n\n3b. Calculate confidence interval (Option 2: with Normal approximation)\n\npred = predict(multi_bc, newdata, se.fit = T, type = \"response\")\n\nLL_CI = pred$fit - qnorm(1-0.05/2) * pred$se.fit\nUL_CI = pred$fit + qnorm(1-0.05/2) * pred$se.fit\n\nc(Pred = pred$fit, LL_CI, UL_CI) %&gt;% round(digits=3)\n\nPred.1      1      1 \n 0.269  0.238  0.299"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r-4",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r-4",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Predicting probability in R",
    "text": "Predicting probability in R\n\n\nPredicting probability of late stage breast cancer diagnosis\n\n\nFor someone who is 60 years old and Non-Hispanic Asian/Pacific Islander, what is the predicted probability for late stage breast cancer diagnosis (with confidence intervals)?\n\n\n\nInterpret results\n\nFor someone who is 60 years old and Non-Hispanic Asian/Pacific Islander, the predicted probability of late stage breast cancer diagnosis is 0.269 (95% CI: 0.238, 0.299)."
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-table",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-table",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "How to present odds ratios: Table",
    "text": "How to present odds ratios: Table\n\ntbl_regression() in the gtsummary package is helpful for presenting the odds ratios in a clean way\n\n\nlibrary(gtsummary)\ntbl_regression(multi_bc, exponentiate = TRUE) %&gt;% \n  as_gt() %&gt;% # allows us to use tab_options()\n  tab_options(table.font.size = 38)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\n\nRace_Ethnicity\n\n\n\n\n\n\n\n\n    NH White\n—\n—\n\n\n\n\n    Hispanic-Latino\n0.98\n0.83, 1.16\n0.9\n\n\n    NH American Indian/Alaskan Native\n0.92\n0.33, 2.25\n0.9\n\n\n    NH Asian/Pacific Islander\n1.14\n0.97, 1.35\n0.11\n\n\n    NH Black\n1.43\n1.24, 1.65\n&lt;0.001\n\n\nAge_c\n1.06\n1.05, 1.07\n&lt;0.001\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-forest-plot-setup",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-forest-plot-setup",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "How to present odds ratios: Forest Plot Setup",
    "text": "How to present odds ratios: Forest Plot Setup\n\nlibrary(broom.helpers)\n\n\nAttaching package: 'broom.helpers'\n\n\nThe following objects are masked from 'package:gtsummary':\n\n    all_categorical, all_continuous, all_contrasts, all_dichotomous,\n    all_interaction, all_intercepts\n\nMLR_tidy = tidy_and_attach(multi_bc, conf.int=T, exponentiate = T) %&gt;%\n  tidy_remove_intercept() %&gt;%\n  tidy_add_reference_rows() %&gt;%\n  tidy_add_estimate_to_reference_rows() %&gt;%\n  tidy_add_term_labels()\nglimpse(MLR_tidy)\n\nRows: 6\nColumns: 16\n$ term           &lt;chr&gt; \"Race_EthnicityNH White\", \"Race_EthnicityHispanic-Latin…\n$ variable       &lt;chr&gt; \"Race_Ethnicity\", \"Race_Ethnicity\", \"Race_Ethnicity\", \"…\n$ var_label      &lt;chr&gt; \"Race_Ethnicity\", \"Race_Ethnicity\", \"Race_Ethnicity\", \"…\n$ var_class      &lt;chr&gt; \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"nume…\n$ var_type       &lt;chr&gt; \"categorical\", \"categorical\", \"categorical\", \"categoric…\n$ var_nlevels    &lt;int&gt; 5, 5, 5, 5, 5, NA\n$ contrasts      &lt;chr&gt; \"contr.treatment\", \"contr.treatment\", \"contr.treatment\"…\n$ contrasts_type &lt;chr&gt; \"treatment\", \"treatment\", \"treatment\", \"treatment\", \"tr…\n$ reference_row  &lt;lgl&gt; TRUE, FALSE, FALSE, FALSE, FALSE, NA\n$ label          &lt;chr&gt; \"NH White\", \"Hispanic-Latino\", \"NH American Indian/Alas…\n$ estimate       &lt;dbl&gt; 1.0000000, 0.9846940, 0.9178662, 1.1433526, 1.4300256, …\n$ std.error      &lt;dbl&gt; NA, 0.083653090, 0.484110085, 0.083796726, 0.071788616,…\n$ statistic      &lt;dbl&gt; NA, -0.1843845, -0.1770333, 1.5986877, 4.9825778, 17.81…\n$ p.value        &lt;dbl&gt; NA, 8.537118e-01, 8.594822e-01, 1.098900e-01, 6.274274e…\n$ conf.low       &lt;dbl&gt; NA, 0.8344282, 0.3262638, 0.9688184, 1.2414629, 1.05221…\n$ conf.high      &lt;dbl&gt; NA, 1.158411, 2.254643, 1.345732, 1.645053, 1.065538"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-forest-plot-setup-1",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-forest-plot-setup-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "How to present odds ratios: Forest Plot Setup",
    "text": "How to present odds ratios: Forest Plot Setup\n\nMLR_tidy = MLR_tidy %&gt;%\n  mutate(var_label = case_match(var_label, \n                               \"Race_Ethnicity\" ~ \"Race and ethnicity\", \n                               \"Age_c\" ~ \"\"), \n         label = case_match(label, \n                            \"NH White\" ~ \"Non-Hispanic White\", \n                            \"Hispanic-Latino\" ~ \"Hispanic-Latinx\", \n                            \"NH American Indian/Alaskan Native\" ~ \"Non-Hispanic American \\n Indian/Alaskan Native\", \n                            \"NH Asian/Pacific Islander\" ~ \"Non-Hispanic \\n Asian/Pacific Islander\",\n                            \"NH Black\" ~ \"Non-Hispanic Black\", \n                               \"Age_c\" ~ \"Age (yrs)\"))\n  # %&gt;%\n  #                       fct_relevel(\"Age (yrs)\", \"Non-Hispanic \\n Asian/Pacific Islander\", \"Non-Hispanic American \\n Indian/Alaskan Native\", \"Non-Hispanic White\", \"Hispanic-Latinx\", \"Non-Hispanic Black\"))"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-forest-plot",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-forest-plot",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "How to present odds ratios: Forest Plot",
    "text": "How to present odds ratios: Forest Plot\n\nMLR_tidy = MLR_tidy %&gt;% mutate(label = fct_reorder(label, term))\n\nplot_MLR = ggplot(data=MLR_tidy, \n       aes(y=label, x=estimate, xmin=conf.low, xmax=conf.high)) + \n  geom_point(size = 3) +  geom_errorbarh(height=.2) + \n  \n  geom_vline(xintercept=1, color='#C2352F', linetype='dashed', alpha=1) +\n  theme_classic() +\n  \n  facet_grid(rows = vars(var_label), scales = \"free\",\n             space='free_y', switch = \"y\") + \n  \n  labs(x = \"OR (95% CI)\", \n       title = \"Odds ratios of Late Stage Breast Cancer Diagnosis\") +\n  theme(axis.title = element_text(size = 25), \n        axis.text = element_text(size = 25), \n        title = element_text(size = 25), \n        axis.title.y=element_blank(), \n        strip.text = element_text(size = 25), \n        strip.placement = \"outside\", \n        strip.background = element_blank())\n# plot_MLR"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-forest-plot-1",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-forest-plot-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "How to present odds ratios: Forest Plot",
    "text": "How to present odds ratios: Forest Plot\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_errorbarh()`)."
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#adding-odds-ratios",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#adding-odds-ratios",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Adding odds ratios",
    "text": "Adding odds ratios\n\nMLR_tidy = MLR_tidy %&gt;% \n  mutate(estimate_r = round(estimate, 2), \n         conf.low_r = round(conf.low, 2), \n         conf.high_r = round(conf.high, 2), \n         OR_char = paste0(estimate_r, \" (\", conf.low_r, \", \", conf.high_r, \")\"), \n         OR_char = ifelse(reference_row == F | is.na(reference_row), OR_char, NA))\n\n \n\n“Plot” of the text for odds ratios estimates\n\n\nOR_labs = ggplot(data=MLR_tidy, aes(y=label)) +\n  geom_text(aes(x = -1, label = OR_char), hjust = 0, size=8) +   \n  xlim(-1, 1) +\n  facet_grid(rows = vars(var_label), scales = \"free\",\n             space='free_y') +\n  theme_void() + \n    theme(strip.text = element_blank())"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#combine-them",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#combine-them",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Combine them!!",
    "text": "Combine them!!\n\nlibrary(cowplot)\n\n\nAttaching package: 'cowplot'\n\n\nThe following object is masked from 'package:ggpubr':\n\n    get_legend\n\n\nThe following object is masked from 'package:gt':\n\n    as_gtable\n\n\nThe following object is masked from 'package:lubridate':\n\n    stamp\n\nplot_grid(plot_MLR, OR_labs, ncol=2, align = \"h\", rel_widths = c(4, 1))\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_errorbarh()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_text()`)."
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#multivariable-logistic-regression-model",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#multivariable-logistic-regression-model",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Multivariable Logistic Regression Model",
    "text": "Multivariable Logistic Regression Model\n\nThe multivariable model of logistic regression (called multiple logistic regression) is useful in that it statistically adjusts the estimated effect of each variable in the model\n\n \n\nEach estimated coefficient provides an estimate of the log odds adjusting for all other variables included in the model\n \n\nThe adjusted odds ratio can be different from or similar to the unadjusted odds ratio\n\n \n\nComparing adjusted vs. unadjusted odds ratios can be a useful activity"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#interpretation-of-coefficients-in-mlr",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#interpretation-of-coefficients-in-mlr",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Interpretation of Coefficients in MLR",
    "text": "Interpretation of Coefficients in MLR\n\nThe interpretation of coefficients in multiple logistic regression is essentially the same as the interpretation of coefficients in simple logistic regression\n\n \n\nFor interpretation, we need to\n\npoint out that these are adjusted estimates\nprovide a list of other variables in the model"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-race-and-ethnicity-and-age-model-fit-fixed",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-race-and-ethnicity-and-age-model-fit-fixed",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example: Race and Ethnicity and Age model fit (FIXED)",
    "text": "Example: Race and Ethnicity and Age model fit (FIXED)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\n\nRace_Ethnicity\n\n\n\n\n\n\n\n\n    NH White\n—\n—\n\n\n\n\n    Hispanic-Latino\n0.98\n0.83, 1.16\n0.9\n\n\n    NH American Indian/Alaskan Native\n0.92\n0.33, 2.25\n0.9\n\n\n    NH Asian/Pacific Islander\n1.14\n0.97, 1.35\n0.11\n\n\n    NH Black\n1.43\n1.24, 1.65\n&lt;0.001\n\n\nAge_c\n1.06\n1.05, 1.07\n&lt;0.001\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\n\n\nThe estimated odds of late stage breast cancer diagnosis for Hispanic-Latinx individuals is 0.98 times that of Non-Hispanic White individuals, controlling for age (95% CI: 0.83, 1.16).\nThe estimated odds of late stage breast cancer diagnosis for Non-Hispanic American Indian/Alaskan Natives is 0.92 times that of Non-Hispanic White individuals, controlling for age (95% CI: 0.33, 2.25).\nThe estimated odds of late stage breast cancer diagnosis for Non-Hispanic Asian/Pacific Islanders is 1.14 times that of Non-Hispanic White individuals, controlling for age (95% CI: 0.97, 1.35).\nThe estimated odds of late stage breast cancer diagnosis for Non-Hispanic Black individuals is 1.43 times that of Non-Hispanic White individuals, controlling for age (95% CI: 1.24, 1.65).\nFor every one year increase in age, there is an 6% increase in the estimated odds of late stage breast cancer diagnosis, adjusting for race and ethnicity (95% CI: 5%, 7%)."
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#last-class-glow-study-with-interactions",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#last-class-glow-study-with-interactions",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Last Class: GLOW Study with interactions",
    "text": "Last Class: GLOW Study with interactions\n\nOutcome variable: any fracture in the first year of follow up (FRACTURE: 0 or 1)\nRisk factor/variable of interest: history of prior fracture (PRIORFRAC: 0 or 1)\nPotential confounder or effect modifier: age (AGE, a continuous variable)\nFitted model with interactions: \\[\\begin{aligned} \\text{logit}\\left(\\widehat\\pi(\\mathbf{X})\\right) & = \\widehat\\beta_0 &+ &\\widehat\\beta_1\\cdot I(\\text{PF}) & + &\\widehat\\beta_2\\cdot Age& + &\\widehat\\beta_3 \\cdot I(\\text{PF}) \\cdot Age \\\\  \n\\text{logit}\\left(\\widehat\\pi(\\mathbf{X})\\right) & = -1.376 &+ &1.002\\cdot I(\\text{PF})& + &0.063\\cdot Age& -&0.057 \\cdot I(\\text{PF}) \\cdot Age\n\\end{aligned}\\]\n\n \n\nToday: determine the overall fit of this model"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#last-class-reporting-results-of-glow-study-with-interactions",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#last-class-reporting-results-of-glow-study-with-interactions",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Last Class: Reporting results of GLOW Study with interactions",
    "text": "Last Class: Reporting results of GLOW Study with interactions\n\nRemember our main covariate is prior fracture, so we want to focuse on how age changes the relationship between prior fracture and a new fracture!\n\n\n\nFor individuals 69 years old, the estimated odds of a new fracture for individuals with prior fracture is 2.72 times the estimated odds of a new fracture for individuals with no prior fracture (95% CI: 1.70, 4.35). As seen in Figure 1 (a), the odds ratio of a new fracture when comparing prior fracture status decreases with age, indicating that the effect of prior fractures on new fractures decreases as individuals get older. In Figure 1 (b), it is evident that for both prior fracture statuses, the predict probability of a new fracture increases as age increases. However, the predicted probability of new fracture for those without a prior fracture increases at a higher rate than that of individuals with a prior fracture. Thus, the predicted probabilities of a new fracture converge at age [insert age here].\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n(a) Odds ratio of fracture outcome comparing prior fracture to no prior fracture\n\n\n\n\n\n\n\n\n\n\n\n(b) Predicted probability of fracture\n\n\n\n\n\n\n\nFigure 1: Plots of odds ratio and predicted probability from fitted interaction model"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#overview-12",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#overview-12",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Overview (1/2)",
    "text": "Overview (1/2)\n\nOnce a potential final model has been determined, we need to assess the fit of the model\n\n \n\nVariable selection is no longer our focus at this stage\n\nWe want to find answer to whether the model fits the data adequately\n\n\n \n\nAssessing the Goodness of Fit or Assessing model fit\n\nAssess how well our fitted logistic regression model predicts/estimates the observed outcomes\nComparison: fitted/estimated outcome vs. observed outcome"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#some-good-measurements-for-our-final-models",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#some-good-measurements-for-our-final-models",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Some good measurements for our final model(s)",
    "text": "Some good measurements for our final model(s)\n\nPearson residual statistic\n\n \n\nHosmer-Lemeshaw goodness-of-fit statistic\n\n \n\nAUC-ROC (area under the curve of the receiver operating characteristic)\n\n \n\nAIC/BIC"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#overview-22",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#overview-22",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Overview (2/2)",
    "text": "Overview (2/2)\n\nTo assess the fit of the model, it is good to have a mixture of measurements\n\n \n\nWe want to measure the absolute fit: not comparing to any models, but determining if the model fits the data well\n\nPearson residual statistic\nHosmer-Lemeshaw goodness-of-fit statistic\nAUC-ROC (kind of, often do not use a hypothesis test but you can!)\n\n\n \n\nWe want comparable measures of fit: if we have candidate models that are not nested\n\nAUC-ROC\nAIC/BIC"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#poll-everywhere-question-1",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#poll-everywhere-question-1",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#components-to-assess-model-fit",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#components-to-assess-model-fit",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Components to Assess Model Fit",
    "text": "Components to Assess Model Fit\n\nThe model fits the data well if\n\nSummary measures of the distance between the predicted/estimated/fitted and observed Y are small\n\nToday’s lecture!!\n\nThe contribution of each pair (predicted and observed) to these summary measures is unsystematic and is small relative to the error structure of the model\n\nModel Diagnostics that will be covered in another lecture!\n\n\n\n \n\nNeed both components\n\nIt is possible to see a “good” summary measure of the distance between predicted and observed Y with some substantial deviation from fit for a few subjects"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#summary-measures-of-goodness-of-fit",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#summary-measures-of-goodness-of-fit",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Summary Measures of Goodness of Fit",
    "text": "Summary Measures of Goodness of Fit\n\nAka overall measure of fit\n\n \n\nWhat do we need to calculate them?\n\nNeed to define what the fitted outcome is\nNeed to calculate how close the fitted outcome is to the observed outcome\nSummarize across all observations (or individuals’ data)\n\n\n \n\nTwo tests of goodness-of-fit\n\nPearson residual statistic\nHosmer-Lemeshaw goodness-of-fit statistic"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#comparing-fitted-outcome-to-observed-outcome",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#comparing-fitted-outcome-to-observed-outcome",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Comparing fitted outcome to observed outcome",
    "text": "Comparing fitted outcome to observed outcome\n\nIn logistic regression model, we estimate \\(\\pi(\\mathbf{X}) = P(Y=1|\\mathbf{X})\\)\n\nPredicted value, \\(\\widehat\\pi(\\mathbf{X})\\), is between 0 and 1 for each subject\n\nHowever, we always observe \\(Y=1\\) or \\(Y=0\\)\n\nNot an observed \\(\\pi(\\mathbf{X})\\)\n\n\n \n\nWe can deterimine the fitted outcome by sampling Y’s from a Bernoulli distribution with the fitted probability\n\n\\(\\widehat{Y} \\sim \\text{Bernoulli}(\\widehat\\pi(\\mathbf{X}))\\)\n\nIf there are groups of individuals that share the same covariate observations, then we can use the same \\(\\widehat\\pi(\\mathbf{X})\\)\n\n\\(\\sum_j \\widehat{Y} \\sim \\text{Binomial}(\\sum_j, \\widehat\\pi(\\mathbf{X}))\\)\n\n\n \n\nInstead of comparing the expected vs. observed at individual level, we can compare them at “group” level"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#number-of-covariate-patterns",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#number-of-covariate-patterns",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Number of Covariate Patterns",
    "text": "Number of Covariate Patterns\n\nWhen the logistic regression model contains only categorical covariates, we can think of the number of covariate patterns\nFor example: model contains two binary covariates (history of fracture and smoking status), there will be 4 unique combination of these factors\n\nThis model has 4 covariate patterns\nSubjects can be divided into 4 groups based on the covariates’ values\n\nWe can then compute the predicted number of individuals with Y=1 in each group, and compare that with the actual observed number of individuals with Y=1 in that group\n\nWe don’t need to sample this\nWe use the expected value (mean) of the Binomial to determine the \\(\\widehat{Y}\\) for each covariate pattern\nFor covariate pattern \\(j\\) with \\(m_j\\) observations: \\[\\widehat{Y}_j = m_j \\widehat\\pi(\\mathbf{X_j}) = m_j{\\hat{\\pi}}_j\\]"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#pearson-residual",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#pearson-residual",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Pearson Residual",
    "text": "Pearson Residual\n\nIn logistic regression model, can use Pearson residual for summary measure of goodness-of-fit Uses the \\(\\widehat{Y}_j\\) fitted value from previous slide\nPearson residual for jth covariate pattern is: \\[r\\left(Y_j,{\\hat{\\pi}}_j\\right)=\\frac{(Y_j-m_j{\\hat{\\pi}}_j)}{\\sqrt{m_j{\\hat{\\pi}}_j(1-{\\hat{\\pi}}_j)}}=\\frac{(Y_j-{\\hat{Y}}_i)}{\\sqrt{{\\hat{Y}}_i(1-{\\hat{\\pi}}_j)}}\\]\nThe summary statistics of Pearson residual is thus: \\[X^2=\\sum_{j=1}^{J}{r\\left(Y_j,{\\hat{\\pi}}_j\\right)^2}\\]"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#pearson-residual-procedure",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#pearson-residual-procedure",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Pearson Residual procedure",
    "text": "Pearson Residual procedure\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses: same for all data\n\n\\(H_0\\): model fits well\n\\(H_1\\): model does not fits well\n\nCalculate the test statistic and p-value\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#not-going-to-bother-going-through-an-example",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#not-going-to-bother-going-through-an-example",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Not going to bother going through an example",
    "text": "Not going to bother going through an example\n\nWe can calculate this by hand and test against a chi-squared distribution\n\n \n\nNo set R code to do this\n\n \n\nI do not see this as the main way to determine goodness of fit… for a binary outcome!\n\nOften because of the bigger issues with it…"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#issues-with-pearson-residuals",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#issues-with-pearson-residuals",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Issues with Pearson Residuals",
    "text": "Issues with Pearson Residuals\n\nAssume current model has p covariates…\n\nthen \\(X^2\\) (Pearson residual) follows a chi-squared distribution\n\nunder the null hypothesis based on large sample theory\n\nOnly appropriate if the number of covariate patterns is less than the number of observations\n\n\n \n\nWhen the logistic regression model contains one or more continuous covariates, it is likely that the number of covariate patterns equals to the sample size n\n\n \n\nWe should not use Pearson Residuals to evaluate goodness-of-fit test when the fitted model contains one or more continuous variables"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#hosmer-lemeshow-test",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#hosmer-lemeshow-test",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Hosmer-Lemeshow test",
    "text": "Hosmer-Lemeshow test\n\nIf number of covariate patterns is roughly same as the number of observations\n\nWhenever you include a continuous variable in your model\nHosmer-Lemeshow (HL) goodness-of-fit test should be used instead\n\n\n \n\nHowever, HL test does not work well if the number of covariate patterns is small\n\nHL test should not be used if the number of covariate patterns ≤ 6\n\nFor reference: 3 binary predictors makes 8 covariate patterns\n\nPearson residuals \\(X^2\\) should be used when the number of covariate patterns is small\n\n\n \n\nA large p-value from HL test suggests the model fits well"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#poll-everwhere-question-2",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#poll-everwhere-question-2",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Poll Everwhere question 2",
    "text": "Poll Everwhere question 2"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#hosmer-lemeshow-test-1",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#hosmer-lemeshow-test-1",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Hosmer-Lemeshow test",
    "text": "Hosmer-Lemeshow test\n\nHL test uses groupings from percentiles to basically measure what Pearson residual measures\n\n \n\nSteps to compute HL test statistic:\n\nCompute estimated probability \\(\\widehat\\pi(\\mathbf{X}))\\) for all n subjects (\\(n=1, 2, ..., n\\))\nOrder \\(\\widehat\\pi(\\mathbf{X}))\\) from largest to smallest values\nDivide ordered values into g percentile grouping (usually \\(g = 10\\) based on H-L’s suggestion)\nForm table of observed and expected counts\nCalculate HL test statistic from table\nCompare HL test statistic to chi-squared distribution (\\(\\chi^2_{g-2}\\))"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#hosmer-lemeshow-test-statistic",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#hosmer-lemeshow-test-statistic",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Hosmer-Lemeshow test statistic",
    "text": "Hosmer-Lemeshow test statistic\n\nThe test statistic of Hosmer-Lemeshow goodness-of-fit test is denoted by \\(\\widehat{C}\\), which is obtained by calculating the Pearson chi-squared statistic from the \\(g \\times 2\\) table of observed and estimated expected frequencies \\[\\hat{C}=\\sum_{k=1}^{g}\\frac{\\left(o_k-n_k^\\prime{\\bar{\\pi}}_k\\right)^2}{n_k^\\prime{\\bar{\\pi}}_k(1-{\\bar{\\pi}}_k)}\\]\n\nwhere \\(n'_k\\) is the total number of subjects in the \\(k\\)th group\n\nLet \\(c_k\\) be the number of covariate patterns in the \\(k\\)th decile: \\[o_k=\\sum_{j=1}^{c_k}y_j\\] and \\[{\\bar{\\pi}}_k=\\sum_{j=1}^{c_k}\\frac{m_j{\\hat{\\pi}}_j}{n_k^\\prime}\\]"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#hosmer-lemeshow-test-procedure",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#hosmer-lemeshow-test-procedure",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Hosmer-Lemeshow test procedure",
    "text": "Hosmer-Lemeshow test procedure\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses: same for all data\n\n\\(H_0\\): model fits well\n\\(H_1\\): model does not fits well\n\nCalculate the test statistic and p-value\n\nNote: \\(\\widehat{C} \\sim \\chi^2_{df=g-2}\\)\n\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#glow-study-hosmer-lemeshow-test",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#glow-study-hosmer-lemeshow-test",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "GLOW Study: Hosmer-Lemeshow test",
    "text": "GLOW Study: Hosmer-Lemeshow test\n\nOkay, so let’s look at the interaction model from last class \\[\\text{logit}\\left(\\pi(\\mathbf{X})\\right) = \\beta_0 + \\beta_1\\cdot I(\\text{PF}) +\\beta_2\\cdot Age + \\beta_3 \\cdot I(\\text{PF}) \\cdot Age\\]\nWe need to fit the model and use a new command:\n\n\nglow_m3 = glm(fracture ~ priorfrac + age_c + priorfrac*age_c, \n              data = glow, family = binomial)\nlibrary(ResourceSelection)\n\nResourceSelection 0.3-6      2023-06-27\n\nobs_vals = as.numeric(glow$fracture) -1\nfit_vals = fitted(glow_m3)\nhoslem.test(obs_vals, fit_vals, g = 10)\n\n\n    Hosmer and Lemeshow goodness of fit (GOF) test\n\ndata:  obs_vals, fit_vals\nX-squared = 6.778, df = 8, p-value = 0.5608\n\n\nNote to Nicky: do NOT make conclusion yet! In the poll everywhere!"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#poll-everywhere-question-3",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#poll-everywhere-question-3",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Poll Everywhere question 3",
    "text": "Poll Everywhere question 3"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#glow-study-hosmer-lemeshow-test-1",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#glow-study-hosmer-lemeshow-test-1",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "GLOW Study: Hosmer-Lemeshow test",
    "text": "GLOW Study: Hosmer-Lemeshow test\n\nConclusion: The p-value is 0.5608, so we fail to reject the null hypothesis that the model fits the data well. Thus, the preliminary final model for the GLOW dataset fits the data well\n\n \n\nDon’t forget that we still need to check individual observations (Model Diagnostics!)\n\n \n\nR may give results for the HL test even if it is not appropriate to use it!\n\nIf number of covariate patterns ≤ 6, do not use HL test"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#big-data-issue-in-goodness-of-fit-test",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#big-data-issue-in-goodness-of-fit-test",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Big Data Issue in Goodness-of-fit Test",
    "text": "Big Data Issue in Goodness-of-fit Test\n\nWhen the sample size is really big (&gt; 1000), it is much more likely to find the H-L reject the model fit (even when the expected vs. observed in each decile categories looks pretty similar)\n\n \n\nThis is due to “too much” power in hypothesis testing.\n\nPaul et al. (2012) for samples sizes from 1000 to 25,000, the number of groups g should be equal to \\[g=\\max{\\left(10,\\min{\\left\\{\\frac{n_1}{2},\\ \\frac{n-n_1}{2},\\ 2+8\\left(\\frac{n}{1000}\\right)^2\\right\\}}\\right)}\\]\n\n\n \n\nFor example, if one has a sample with \\(n=10, 000\\) (sample size) and \\(n_1=1,000\\) (number of events) then \\(g=500\\) groups are suggested\nFor n &gt; 25000, other methods, such as partitioning data into a developmental data set (with smaller n) and a validation set"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#final-notes-on-goodness-of-fit-test",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#final-notes-on-goodness-of-fit-test",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Final Notes on Goodness-of-fit Test",
    "text": "Final Notes on Goodness-of-fit Test\n\nThey should not be used for variable selection\n\nThe likelihood ratio tests for significance of coefficients are much more powerful and appropriate (when nested)\n\n\n \n\nThey are not for model comparison\n\nOne should not use the p-value from goodness of fit tests of different models to decide which model is better than the other\nSomething like AUC-ROC, AIC, or BIC can be used"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#roc-curve-and-auc-12",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#roc-curve-and-auc-12",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "ROC Curve and AUC (1/2)",
    "text": "ROC Curve and AUC (1/2)\n\n\n\nReceiver Operating Characteristics (ROC) curve is useful tool to quantify how good is our model predicting binary outcome\n\n \n\nIt is a plot of sensitivity (true positive rate) versus (1-specificity) or false positive rate of fitted binary values\n\nTrue Positive Rate \\(= \\dfrac{TP}{TP + FN}\\)\nFalse Positive Rate \\(=  \\dfrac{FP}{FP + TN}\\)\n\n\n \n\nThe ROC curve shows the tradeoff between sensitivity and specificity"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#roc-curve-and-auc-22",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#roc-curve-and-auc-22",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "ROC Curve and AUC (2/2)",
    "text": "ROC Curve and AUC (2/2)\n\n\n\nArea under the ROC curve (AUC ROC) is a reasonable summary of the overall predictive accuracy of the test\n\nAccuracy means how well the predicted value matches the observed value\n\n\n \n\nThe closer the curve follows the left-hand border and top border of the ROC space, the more accurate the test\n\nAn AUC =1 represents 100% accuracy\n\n\n \n\nThe closer the curve comes to the 45-degree diagonal line, the less accurate the test\n\nAn AUC = 0.5 represents an unhelpful model\n\nRandom predictions"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#poll-everywhere-question-4",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#poll-everywhere-question-4",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#roc-curve-and-auc-33",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#roc-curve-and-auc-33",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "ROC Curve and AUC (3/3)",
    "text": "ROC Curve and AUC (3/3)\n\nOften only report the AUC\n\n \n\nSuggestions of how to interpret model fit through AUC values:"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#glow-study-roc-of-interaction-model",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#glow-study-roc-of-interaction-model",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "GLOW Study: ROC of interaction model",
    "text": "GLOW Study: ROC of interaction model\n\n\n\nlibrary(pROC)\npredicted &lt;- predict(glow_m3, glow, type=\"response\")\n\n# define object to plot and calculate AUC\nrocobj &lt;- roc(glow$fracture, predicted)\nauc &lt;- round(auc(glow$fracture, predicted),4)\n\n#create ROC plot\nggroc(rocobj, colour = 'steelblue', \n      size = 2, legacy.axes = TRUE) +\n  ggtitle(paste0('ROC Curve ','(AUC = ',auc,')')) +\n  theme(text = element_text(size = 23)) +\n  xlab(\"False Positive Rate (1 - Specificity)\") +\n  ylab(\"True Positive Rate (Sensitivity)\")\n\n\n\n\nType 'citation(\"pROC\")' for a citation.\n\n\n\nAttaching package: 'pROC'\n\n\nThe following object is masked from 'package:epiDisplay':\n\n    ci\n\n\nThe following objects are masked from 'package:stats':\n\n    cov, smooth, var\n\n\nSetting levels: control = No, case = Yes\n\n\nSetting direction: controls &lt; cases\n\n\nSetting levels: control = No, case = Yes\n\n\nSetting direction: controls &lt; cases\n\n\n\n\n\n\n\n\n\n\n\nWe have a poorly fitting model\nWe can take auc and compare it to other models: good way to pick a model based on predictive power"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#another-way-to-think-about-auc",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#another-way-to-think-about-auc",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Another way to think about AUC",
    "text": "Another way to think about AUC\n\nGLOW Study: Consider the situation in which the fracture status of each individual is known\n\n \n\nRandomly pick one individual from fractured group and one from non-fractured outcome group\n\nBased on their age, height, prior fracture, and all other covariates, we will correctly predict which is from fractured group\n\n\n \n\nThe AUC is the percentage of randomly drawn pairs for which we predict the pair correctly\n\n \n\nTherefore, AUC represents the ability of our covariates to discriminate between individuals with the outcome (fracture) and those without the outcome"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#aic-and-bic",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#aic-and-bic",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "AIC and BIC",
    "text": "AIC and BIC\n\nTwo widely used non-hypothesis testing based measurements that helps select a good model\n\nAkaike Information Criterion (AIC)\nBayesian Information Criterion (BIC)\n\n\n \n\nUnlike likelihood ratio test which is only suitable for nested model, AIC and BIC are suitable for both nested and non-nested model\n\n \n\nThere is no hypothesis/conclusion testing for the comparison between two models\n\nSo not the best for selecting covariates to include in model\nBUT helpful if you have a few preliminary final models that you want to compare"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#poll-everywhere-question-5",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#poll-everywhere-question-5",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Poll Everywhere Question 5",
    "text": "Poll Everywhere Question 5"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#aic-and-bic-1",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#aic-and-bic-1",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "AIC and BIC",
    "text": "AIC and BIC\n\nBoth AIC and BIC penalize a model for having many parameters\n\n \n\n\n\n\n\n\n\n\nMeasure of fit\nEquation\nR code\n\n\n\n\nAkaike information criterion (AIC)\n\\(AIC = -2 \\cdot \\text{log-likelihood} + 2q\\)\nAIC(model_name)\n\n\nBayesian information criterion (BIC)\n\\(AIC = -2 \\cdot \\text{log-likelihood} + q\\text{log}(n)\\)\nBIC(model_name)\n\n\n\n \n\nWhere q is the number of parameters in the model and n is the sample size\nBoth AIC and BIC can only be used to compare models fitting the same data set\nIn comparing two models, the model with smaller AIC and/or BIC is preferred\n\nWhen the difference in AIC between two models exceeds 3, the difference is viewed as “meaningful”"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#aic-and-bic-in-r",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#aic-and-bic-in-r",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "AIC and BIC in R",
    "text": "AIC and BIC in R\n\nAfter fitting the logistic regression model, can calculate AIC and BIC\nLet’s look at the AIC and BIC of our interaction model:\n\n\nAIC(glow_m3)\n\n[1] 531.2716\n\nBIC(glow_m3)\n\n[1] 548.13"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#summary-12",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#summary-12",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Summary (1/2)",
    "text": "Summary (1/2)\n\n\n\n\n\n\n\n\n\nMeasure of fit\nHypothesis tested?\nEquation\nR code\n\n\n\n\nPearson residual\nYes\n\\(X^2=\\sum_{j=1}^{J}{r\\left(Y_j,{\\hat{\\pi}}_j\\right)^2}\\)\nNot given\n\n\nHosmer-Lemeshow test\nYes\n\\(\\hat{C}=\\sum_{k=1}^{g}\\frac{\\left(o_k-n_k^\\prime{\\bar{\\pi}}_k\\right)^2}{n_k^\\prime{\\bar{\\pi}}_k(1-{\\bar{\\pi}}_k)}\\)\nhoslem.test()\n\n\nAUC-ROC\nKinda\nNot given\nauc(observed, predicted)\n\n\nAIC\nOnly to compare models\n\\(AIC = -2 \\cdot \\text{log-likelihood} + 2q\\)\nAIC(model_name)\n\n\nBIC\nOnly to compare models\n\\(AIC = -2 \\cdot \\text{log-likelihood} + q\\text{log}(n)\\)\nBIC(model_name)\n\n\n\nSpecial notes:\n\nUse Hosmer-Lemshow test over Pearson residual unless number of covariate patterns is less than 6\nCannot use Pearson residual when there is a continuous variable in the model"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#summary-22",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#summary-22",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Summary (2/2)",
    "text": "Summary (2/2)\n\nFor our interaction model: \\[\\begin{aligned} \\text{logit}\\left(\\widehat\\pi(\\mathbf{X})\\right) & = \\widehat\\beta_0 &+ &\\widehat\\beta_1\\cdot I(\\text{PF}) & + &\\widehat\\beta_2\\cdot Age& + &\\widehat\\beta_3 \\cdot I(\\text{PF}) \\cdot Age \\\\  \n\\text{logit}\\left(\\widehat\\pi(\\mathbf{X})\\right) & = -1.376 &+ &1.002\\cdot I(\\text{PF})& + &0.063\\cdot Age& -&0.057 \\cdot I(\\text{PF}) \\cdot Age\n\\end{aligned}\\]\nWe can examine the overall model fit using:\n\nNot comparing to any other models:\n\nPearson residual: Not appropriate for this model\nHosmer-Lemeshow: \\(\\hat{C}=6.778\\), p-value = 0.56\nAUC-ROC: 0.6819\n\nCan be used to compare to other models:\n\nAUC-ROC: 0.6819\nAIC: 531.27\nBIC: 548.13"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#review-of-model-assessment-so-far-12",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#review-of-model-assessment-so-far-12",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Review of model assessment so far (1/2)",
    "text": "Review of model assessment so far (1/2)\n\nOverall measurements of fit\n\nHow well does the fitted logistic regression model predict the outcome?\nDifferent ways to measure the answer to this question\n\n\n\n\n\n\n\n\n\n\n\nMeasure of fit\nHypothesis tested?\nEquation\nR code\n\n\n\n\nPearson residual\nYes\n\\(X^2=\\sum_{j=1}^{J}{r\\left(Y_j,{\\hat{\\pi}}_j\\right)^2}\\)\nNot given\n\n\nHosmer-Lemeshow test\nYes\n\\(\\hat{C}=\\sum_{k=1}^{g}\\frac{\\left(o_k-n_k^\\prime{\\bar{\\pi}}_k\\right)^2}{n_k^\\prime{\\bar{\\pi}}_k(1-{\\bar{\\pi}}_k)}\\)\nhoslem.test()\n\n\nAUC-ROC\nKinda\nNot given\nauc(observed, predicted)\n\n\nAIC\nOnly to compare models\n\\(AIC = -2 \\cdot \\text{log-likelihood} + 2q\\)\nAIC(model_name)\n\n\nBIC\nOnly to compare models\n\\(BIC = -2 \\cdot \\text{log-likelihood} + q\\text{log}(n)\\)\nBIC(model_name)"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#review-of-model-assessment-so-far-22",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#review-of-model-assessment-so-far-22",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Review of model assessment so far (2/2)",
    "text": "Review of model assessment so far (2/2)\n\nNumerical problems\n\nAssess pre and post model fit\nNumerical problems often depend on the final model (which variables and interactions are included)\n\nDifferent numerical problems to look out for\n\nZero cell count\nComplete separation\nMulticollinearity"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#today",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#today",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Today",
    "text": "Today\n\nWe now use model diagnostics to identify any observations that the model does not fit well"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#review-of-number-of-covariate-patterns",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#review-of-number-of-covariate-patterns",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Review of Number of Covariate Patterns",
    "text": "Review of Number of Covariate Patterns\n\nCovariate patterns are the unique covariate combinations that are observed\n\n \n\nFor example: model contains two binary covariates (history of fracture and smoking status), there will be 4 unique combination of these factors\n\nThis model has 4 covariate patterns\nSubjects can be divided into 4 groups based on the covariates’ values\n\n\n \n\nWhen we have continuous covariates, the number of covariate patterns will be close to the number of individuals in the dataset"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#from-overall-measure-to-diagnostics",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#from-overall-measure-to-diagnostics",
    "title": "Lesson 14: Model Diagnostics",
    "section": "From overall measure to diagnostics",
    "text": "From overall measure to diagnostics\n\nNow we need to investigate diagnostics looking at individual data or covariate pattern data\n\nMake sure the overall measure has not been influenced by certain observations\n\n\n \n\nThe key quantities from logistic regression diagnostics are the components of “residual sum-of-squares”\n\nThe same idea as in the linear regression\nAssessed for each covariate pattern \\(j\\), by computing standardized Pearson residuals and Deviance residuals\n\nStandardization using \\(h_j\\), the leverage values"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#hat-matrix-and-leverage-values-linear-regression",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#hat-matrix-and-leverage-values-linear-regression",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Hat Matrix and Leverage Values: Linear regression",
    "text": "Hat Matrix and Leverage Values: Linear regression\n\nWe have learned “hat” matrix and leverage values from linear regression diagnostics\n\n \n\nIn linear regression, the hat matrix projects the outcome variable onto the covariate space:\n \n\n\\(H=X\\left(X^\\prime X\\right)^{-1}X^\\prime\\) and \\(\\hat{y}=Hy\\)\n\n \n\nThe linear regression residuals is thus \\(y - \\widehat{y}\\), or \\((I-H)y\\)\n\n\n \n\nThe leverage is just the diagonal elements of the hat matrix, which is proportional to the distance of \\(x_j\\) to the mean of the data \\(\\overline{x}\\)"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#hat-matrix-and-leverage-values-logistic-regression",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#hat-matrix-and-leverage-values-logistic-regression",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Hat Matrix and Leverage Values: Logistic regression",
    "text": "Hat Matrix and Leverage Values: Logistic regression\n\nIn logistic regression model, the hat matrix is: \\[H=V^\\frac{1}{2}X\\left(X^\\prime V\\ X\\right)^{-1}X^\\prime V^\\frac{1}{2}\\]\nThe leverage is \\[h_j=m_j\\cdot\\hat{\\pi}\\left(\\textbf{x}_j\\right)\\left[1-\\hat{\\pi}\\left(\\textbf{x}_j\\right)\\right]\\textbf{x}_j^\\prime\\left(\\textbf{X}^\\prime\\textbf{VX}\\right)^{-1}\\textbf{x}_j=v_j\\cdot b_j\\]\n\n\\(b\\): weighted distance of \\(x_j\\) from \\(\\overline{x}\\)\n\\(v_j\\): model based estimator of the variance of \\(y_j\\)\n\n\\(v_j=m_j\\cdot\\hat{\\pi}\\left(\\textbf{x}_j\\right)\\left[1-\\hat{\\pi}\\left(\\textbf{x}_j\\right)\\right]\\)\n\n\n\\(h_j\\) reflects the relative influence of each covariate pattern on the model’s fit"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#poll-everywhere-question-1",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#poll-everywhere-question-1",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#poll-everywhere-question-2",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#poll-everywhere-question-2",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#diagnostic-statistics-computation-12",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#diagnostic-statistics-computation-12",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Diagnostic Statistics Computation (1/2)",
    "text": "Diagnostic Statistics Computation (1/2)\n\nTwo diagnostic statistics computation approach\n\nApproach 1: computed by covariate pattern\n\nRecommendation of Hosmer-Lemeshow textbook\nR uses this approach\nIdentify outliers as group that shares the same covariate values (in the same covariate pattern)\n\nApproach 2: individual observation approach\n\nSAS uses this approach\nIdentify outliers as individual\n\n\nWhy prefer covariate patterns approach?\n\nWhen the number of covariate pattern is much smaller than n, there is risk that we may fail to identify influential and/or poorly fit covariate patterns using individual based on residual"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#diagnostic-statistics-computation-22",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#diagnostic-statistics-computation-22",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Diagnostic Statistics Computation (2/2)",
    "text": "Diagnostic Statistics Computation (2/2)\nConsider a covariate pattern with \\(m_j\\) subjects, all did not have event (some \\(y_i = 0\\)). So the estimated logistic probability is \\(\\widehat\\pi_j\\)\n\nPearson residual computed by individual \\[r_i=-\\sqrt{\\frac{{\\hat{\\pi}}_j}{(1-{\\hat{\\pi}}_j)}}\\]\nPearson residual computed by covariate pattern \\[r_i=-\\sqrt{m_j}\\sqrt{\\frac{{\\hat{\\pi}}_j}{(1-{\\hat{\\pi}}_j)}}\\]\nDifference between aboveresiduals will be large if \\(m_j\\) is large: usually a problem if less covariate patterns\n\nResidual from covariate pattern will identify poorly fit covariate patterns"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#diagnostics-of-logistic-regression",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#diagnostics-of-logistic-regression",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Diagnostics of Logistic Regression",
    "text": "Diagnostics of Logistic Regression\n\nModel diagnostics of logistic regression can be assessed by checking how influential a covariate pattern is:\n \n\nLook at change in residuals if a covariate pattern is excluded\n\nStandardized Pearson residual\nStandardized Deviance residual\n\n\n \n\nLook at change in coefficients if a covariate pattern is excluded"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#change-of-standardized-residuals",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#change-of-standardized-residuals",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Change of Standardized Residuals",
    "text": "Change of Standardized Residuals\n\nChange in standardized Pearson Chi-square statistic due to deletion of subjects with covariate pattern \\(x_j\\): \\[\\Delta X_j^2 = r_{sj}^2 = \\dfrac{r_j^2}{1-h_j}\\]\nDon’t need to know this: change in standardized deviance statistic due to deletion of subjects with covariate pattern \\(x_j\\): \\[\\Delta D_j = \\dfrac{d_j^2}{1-h_j}\\]\nRefer to Lesson 12: Assessing Model Fit for expression of Pearson residual"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#change-of-estimated-coefficients",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#change-of-estimated-coefficients",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Change of Estimated Coefficients",
    "text": "Change of Estimated Coefficients\n\nChange in estimated coefficients due to deletion of subjects with covariate pattern \\(x_j\\): \\[\\Delta \\widehat{\\beta}_j = \\dfrac{r_j^2 h_j}{(1-h_j)^2}\\]\n\n \n\nThis is the logistic regression analog of Cook’s influence statistic (in linear regression)"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#visual-assessment-for-diagnostics-of-logistic-regression-i",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#visual-assessment-for-diagnostics-of-logistic-regression-i",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Visual Assessment for Diagnostics of Logistic Regression (I)",
    "text": "Visual Assessment for Diagnostics of Logistic Regression (I)\n\nIn logistic regression, we mainly rely on graphical methods\n\nBecause the distribution of diagnostic measures under null hypothesis (that the model fits) is only known in certain limited settings\n\n\n \n\nFour plots for analysis of diagnostics in logistic regression:\n\n\\(\\Delta X_j^2\\) vs. \\({\\hat{\\pi}}_j\\)\n\\(\\Delta D_j\\) vs. \\({\\hat{\\pi}}_j\\)\n\\(\\Delta\\widehat{\\beta}_j\\) vs. \\({\\hat{\\pi}}_j\\)\n\\(h_j\\) vs. \\({\\hat{\\pi}}_j\\)"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#recall-the-model-we-fit-glow-study-with-interactions",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#recall-the-model-we-fit-glow-study-with-interactions",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Recall the model we fit: GLOW Study with interactions",
    "text": "Recall the model we fit: GLOW Study with interactions\n\nOutcome variable: any fracture in the first year of follow up (FRACTURE: 0 or 1)\nRisk factor/variable of interest: history of prior fracture (PRIORFRAC: 0 or 1)\nPotential confounder or effect modifier: age (AGE, a continuous variable)\nFitted model with interactions: \\[\\begin{aligned} \\text{logit}\\left(\\widehat\\pi(\\mathbf{X})\\right) & = \\widehat\\beta_0 &+ &\\widehat\\beta_1\\cdot I(\\text{PF}) & + &\\widehat\\beta_2\\cdot Age& + &\\widehat\\beta_3 \\cdot I(\\text{PF}) \\cdot Age \\\\  \n\\text{logit}\\left(\\widehat\\pi(\\mathbf{X})\\right) & = -1.376 &+ &1.002\\cdot I(\\text{PF})& + &0.063\\cdot Age& -&0.057 \\cdot I(\\text{PF}) \\cdot Age\n\\end{aligned}\\]\n\n \n\nLesson 12: determined the overall fit of this model\nToday: determine the if any observations/covariate patterns that model does not fit well"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#how-do-we-get-these-values-in-r",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#how-do-we-get-these-values-in-r",
    "title": "Lesson 14: Model Diagnostics",
    "section": "How do we get these values in R?",
    "text": "How do we get these values in R?\n\nNice function in the R script Logistic_Dx_Functions.R\n\nHighly suggest you save this R script for future use!!\n\n\n\nsource(here(\"lectures\", \"14_Model_diagnostics\", \"Logistic_Dx_Functions.R\"))\ndx_glow = dx(glow_m3)\nglimpse(dx_glow)\n\nRows: 71\nColumns: 16\n$ `(Intercept)`        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ priorfracYes         &lt;dbl&gt; 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0…\n$ age_c                &lt;dbl&gt; 1, -7, 7, -2, 10, 20, 1, -2, 2, 8, 18, -8, 11, 10…\n$ `priorfracYes:age_c` &lt;dbl&gt; 1, 0, 7, 0, 10, 0, 0, -2, 2, 0, 0, 0, 0, 0, -3, 0…\n$ y                    &lt;dbl&gt; 2, 2, 3, 2, 2, 1, 3, 3, 1, 5, 1, 3, 2, 1, 1, 4, 1…\n$ P                    &lt;dbl&gt; 0.4088354, 0.1402159, 0.4162991, 0.1822879, 0.420…\n$ n                    &lt;int&gt; 5, 15, 7, 10, 5, 2, 12, 8, 3, 15, 2, 18, 7, 4, 3,…\n$ yhat                 &lt;dbl&gt; 2.0441770, 2.1032389, 2.9140936, 1.8228786, 2.100…\n$ Pr                   &lt;dbl&gt; -0.04018670, -0.07677228, 0.06586860, 0.14507476,…\n$ dr                   &lt;dbl&gt; -0.04023255, -0.07730975, 0.06577949, 0.14332786,…\n$ h                    &lt;dbl&gt; 0.008844090, 0.003811004, 0.008725450, 0.00290085…\n$ sPr                  &lt;dbl&gt; -0.04036559, -0.07691899, 0.06615786, 0.14528564,…\n$ sdr                  &lt;dbl&gt; -0.04041165, -0.07745749, 0.06606836, 0.14353620,…\n$ dChisq               &lt;dbl&gt; 0.001629381, 0.005916530, 0.004376863, 0.02110791…\n$ dDev                 &lt;dbl&gt; 0.001633102, 0.005999662, 0.004365028, 0.02060264…\n$ dBhat                &lt;dbl&gt; 1.453897e-05, 2.263418e-05, 3.852626e-05, 6.14091…"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#key-to-the-values",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#key-to-the-values",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Key to the values",
    "text": "Key to the values\n\n\n\ncolnames(dx_glow)\n\n [1] \"(Intercept)\"        \"priorfracYes\"       \"age_c\"             \n [4] \"priorfracYes:age_c\" \"y\"                  \"P\"                 \n [7] \"n\"                  \"yhat\"               \"Pr\"                \n[10] \"dr\"                 \"h\"                  \"sPr\"               \n[13] \"sdr\"                \"dChisq\"             \"dDev\"              \n[16] \"dBhat\"             \n\n\n\nFor each covariate pattern (which is each row) …\n\ny: Number of events\nP: Estimated probability of events\nn: Number of observations\nyhat: Estimated number of events\nPr: Pearson residual\ndr: Deviance\nh: leverage\nsPr: Standardized Pearson residual\nsdr: Standardized deviance\ndChisq: Change in standardized Pearson residual\ndDev: Change in standardized deviance\ndBhat: Change in coefficient estimates"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#poll-everywhere-question-3",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#poll-everywhere-question-3",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#visual-assessment-for-diagnostics-of-logistic-regression",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#visual-assessment-for-diagnostics-of-logistic-regression",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Visual Assessment for Diagnostics of Logistic Regression",
    "text": "Visual Assessment for Diagnostics of Logistic Regression\n\nThe plots allow us to identify those covariate patterns that are…\n\nPoorly fit\n\nLarge values of \\(\\Delta X_j^2\\) (and/or \\(\\Delta D_j\\) if we looked at those)\n\nInfluential on estimated coefficients\n\nLarge values of \\(\\Delta\\widehat{\\beta}_j\\)\n\n\nIf you are interested to look at the contribution of leverage (ℎ_𝑗) to the values of the diagnostic statistic, you may also look at plots of:\n\n\\(\\Delta X_j^2\\) vs. \\({\\hat{\\pi}}_j\\)\n\\(\\Delta D_j\\) vs. \\({\\hat{\\pi}}_j\\)\n\\(\\Delta\\widehat{\\beta}_j\\) vs. \\({\\hat{\\pi}}_j\\)"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-standardized-pearson-residuals",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-standardized-pearson-residuals",
    "title": "Lesson 14: Model Diagnostics",
    "section": "GLOW study: standardized Pearson residuals",
    "text": "GLOW study: standardized Pearson residuals\n\n\n\nGenerally, the points that curve from top left to bottom right of plot correspond to covariate patterns with \\(y_j = 1\\)\n\nOpposite corresponds to \\(y_j = 0\\)\n\n\n\n\n\nTo make the plot\nggplot(dx_glow) + geom_point(aes(x=P, y=dChisq), size = 3) + \n  xlab(\"Estimated/Predicted Probability of Fracture\") +\n  ylab(\"Change in Std. Pearson Residual\") +\n  theme(text = element_text(size = 26))"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-standardized-pearson-residuals-1",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-standardized-pearson-residuals-1",
    "title": "Lesson 14: Model Diagnostics",
    "section": "GLOW study: standardized Pearson residuals",
    "text": "GLOW study: standardized Pearson residuals\n\n\n\nGenerally, the points that curve from top left to bottom right of plot correspond to covariate patterns with \\(y_j = 1\\)\n\nOpposite corresponds to \\(y_j = 0\\)\n\nPoints in the top left or top right corners identify the covariate patterns that are poorly fit\nWe may use 4 as a crude approximation to the upper 95th percentile for \\(\\Delta X_j^2\\)\n\n95th percentile of chi-squared distribution is 3.84\n\n\n\n\n\nTo make the plot\nggplot(dx_glow) + geom_point(aes(x=P, y=dChisq), size = 3) + \n  xlab(\"Estimated/Predicted Probability of Fracture\") +\n  ylab(\"Change in Std. Pearson Residual\") +\n  theme(text = element_text(size = 26))"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-standardized-pearson-residuals-2",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-standardized-pearson-residuals-2",
    "title": "Lesson 14: Model Diagnostics",
    "section": "GLOW study: standardized Pearson residuals",
    "text": "GLOW study: standardized Pearson residuals\n\n\n\nGenerally, the points that curve from top left to bottom right of plot correspond to covariate patterns with \\(y_j = 1\\)\n\nOpposite corresponds to \\(y_j = 0\\)\n\nPoints in the top left or top right corners identify the covariate patterns that are poorly fit\nWe may use 4 as a crude approximation to the upper 95th percentile for \\(\\Delta X_j^2\\)\n\n95th percentile of chi-squared distribution is 3.84\n\nWhich point is over 4?\n\n\ndx_glow %&gt;% filter(dChisq &gt; 4) %&gt;% select(priorfracYes, age_c, P, dChisq)\n\n   priorfracYes age_c         P   dChisq\n          &lt;num&gt; &lt;num&gt;     &lt;num&gt;    &lt;num&gt;\n1:            0    -4 0.1643855 4.413937\n\n\n\n\n\nTo make the plot\nggplot(dx_glow) + geom_point(aes(x=P, y=dChisq), size = 3) + \n  xlab(\"Estimated/Predicted Probability of Fracture\") +\n  ylab(\"Change in Std. Pearson Residual\") +\n  theme(text = element_text(size = 26))"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-standardized-deviance-residuals",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-standardized-deviance-residuals",
    "title": "Lesson 14: Model Diagnostics",
    "section": "GLOW study: standardized Deviance residuals",
    "text": "GLOW study: standardized Deviance residuals\n\n\n\nSame investigation as Pearson residuals\nPoints in the top left or top right corners identify the covariate patterns that are poorly fit\nUse 4 as a crude approximation to the upper 95th percentile\nWhich point is over 4?\n\n\ndx_glow %&gt;% filter(dDev &gt; 4) %&gt;% \n  select(priorfracYes, age_c, P, dDev)\n\n   priorfracYes age_c         P     dDev\n          &lt;num&gt; &lt;num&gt;     &lt;num&gt;    &lt;num&gt;\n1:            0   -10 0.1190935 4.841217\n2:            0     7 0.2812460 5.313540\n3:            1     6 0.4150524 4.325664\n\n\n\n\n\nTo make the plot\nggplot(dx_glow) + geom_point(aes(x=P, y=dDev), size = 3) + \n  xlab(\"Estimated/Predicted Probability of Fracture\") +\n  ylab(\"Change in Std. Deviance Residual\") +\n  theme(text = element_text(size = 26))"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-change-in-coefficient-estimates",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-change-in-coefficient-estimates",
    "title": "Lesson 14: Model Diagnostics",
    "section": "GLOW Study: Change in coefficient estimates",
    "text": "GLOW Study: Change in coefficient estimates\n\n\n\nBook recommends flagging certain covariate patterns if change in coefficient estimates are greater than 1\nAll values of \\(\\Delta\\widehat{\\beta}_j\\) are below 0.09\n\n\ndx_glow %&gt;% filter(dBhat &gt; 0.075) %&gt;% \n  select(priorfracYes, age_c, P, dBhat)\n\n   priorfracYes age_c         P      dBhat\n          &lt;num&gt; &lt;num&gt;     &lt;num&gt;      &lt;num&gt;\n1:            1    20 0.4325984 0.08926472\n\n\n\n\n\nTo make the plot\nggplot(dx_glow) + geom_point(aes(x=P, y=dBhat), size = 3) + \n  xlab(\"Estimated/Predicted Probability of Fracture\") +\n  ylab(\"Change in Coefficient Estimates\") +\n  theme(text = element_text(size = 26))"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-leverage",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-leverage",
    "title": "Lesson 14: Model Diagnostics",
    "section": "GLOW Study: Leverage",
    "text": "GLOW Study: Leverage\n\n\n\nWe can use the same rule as linear regression: \\(h_j &gt; 3p/n\\)\n\nFlag these points as high leverage\n\nPoints with high leverage\n\n\\(p=4\\): four regression coefficients\n\\(n=500\\): 500 total observations\nLook for \\(h_j &gt; 3p/n = 3\\cdot4 /500 = 0.024\\)\n\n\n\ndx_glow %&gt;% filter(h &gt; 3*4/500) %&gt;% \n  select(priorfracYes, age_c, P, h) %&gt;% \n  head()\n\n   priorfracYes age_c         P          h\n          &lt;num&gt; &lt;num&gt;     &lt;num&gt;      &lt;num&gt;\n1:            0    20 0.4686423 0.02688958\n2:            1   -12 0.3928116 0.03186122\n3:            0    19 0.4531105 0.02451738\n4:            1   -11 0.3940365 0.02900675\n5:            1    19 0.4313389 0.02895824\n6:            1    18 0.4300804 0.02621708\n\n\n\n\n\nTo make the plot\nggplot(dx_glow) + geom_point(aes(x=P, y=h), size=3) + \n  xlab(\"Estimated/Predicted Probability of Fracture\") +\n  ylab(\"Leverage\") +\n  theme(text = element_text(size = 26))"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#find-out-the-influential-observation-from-the-data-set",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#find-out-the-influential-observation-from-the-data-set",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Find Out the “Influential” Observation From the Data Set",
    "text": "Find Out the “Influential” Observation From the Data Set\n\n\n\nWe identified covariate patterns that may be poorly fit or influential\n\n \n\nLet’s identify the covariate patterns that were not fit well\n\n\n\ndx_glow %&gt;% mutate(Cov_patt = 1:nrow(.)) %&gt;%\n  filter(dChisq &gt; 4 | dDev &gt; 4 | dBhat &gt; 1 | \n          h &gt; 3*4/500) %&gt;%\n  select(Cov_patt, y, P, h, dChisq, dDev, dBhat, h) %&gt;%\n  round(., 3)\n\n    Cov_patt     y     P     h dChisq  dDev dBhat\n       &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;  &lt;num&gt; &lt;num&gt; &lt;num&gt;\n 1:        6     1 0.469 0.027  0.008 0.008 0.000\n 2:       22     1 0.393 0.032  0.046 0.047 0.002\n 3:       36     1 0.453 0.025  0.178 0.183 0.004\n 4:       43     0 0.119 0.005  2.581 4.841 0.012\n 5:       45     6 0.164 0.003  4.414 3.554 0.014\n 6:       47     0 0.281 0.006  3.148 5.314 0.018\n 7:       48     0 0.394 0.029  0.670 1.032 0.020\n 8:       49     2 0.431 0.029  0.698 0.693 0.021\n 9:       50     0 0.430 0.026  0.775 1.155 0.021\n10:       53     0 0.415 0.008  2.862 4.326 0.024\n11:       57     2 0.395 0.026  0.949 0.924 0.026\n12:       63     0 0.484 0.029  0.967 1.364 0.029\n13:       69     0 0.434 0.035  1.588 2.358 0.058\n14:       70     1 0.392 0.035  1.610 1.943 0.058\n15:       71     2 0.433 0.032  2.710 3.462 0.089"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#after-identifying-points",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#after-identifying-points",
    "title": "Lesson 14: Model Diagnostics",
    "section": "After identifying points",
    "text": "After identifying points\n\nDo a data quality check\n\nUnless you have a very good reason to believe the data are not measured correctly, then we leave it in\nCommon to do nothing\n\n\n \n\nIf only a few covariate pattern does not fit well (\\(y_j\\) differs from \\(m_j\\widehat\\pi_j\\) ), we are not too worried\n\nWe had 15 out of 71 covariate patterns\n\n\n \n\nIf quite a few covariate patterns do not fit well, potential reasons can be considered:\n\nThe link used in logistic regression model is not appropriate for outcome\n\nThis is usually unlikely, since logistic regression model is very flexible (think back to why we transformed our outcome from binary form)\n\nOne or more important covariates missing in the model\n\nAt least one of the covariates in the model has been entered in the wrong scale (think age-squared vs. age)"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#how-would-i-report-this-combining-all-model-assessment",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#how-would-i-report-this-combining-all-model-assessment",
    "title": "Lesson 14: Model Diagnostics",
    "section": "How would I report this? (Combining all model assessment)",
    "text": "How would I report this? (Combining all model assessment)\n\nAssuming I have not checked other final models (no other models to compare AIC/BIC or AUC with)\n\nMethods: To assess the overall model fit, we calculated the AUC-ROC. We also calculated several model diagnostics including standardized Pearson residual, standardized deviance, change in coefficient estimates, and leverage. We identified covariate patterns with high standardized Pearson residual (greater than 4), standardized deviance (greater than 4), change in coefficient estimates (greater than 1), and leverage (greater than 0.024).\n \nResults: Our final logistic regression model consisted of the outcome, fracture, and predictors including prior fracture, age, and their interaction. The AUC-ROC was 0.68. We identified 11 covariate patterns with high leverage and 4 with high standardized Pearson residual, standardized deviance, or change in coefficient estimates. No identified observations were omitted.\n \nDiscussion:\n\nAUC-ROC low: Included covariates were pre-determined\nInfluential points were kept in because all observations were within feasible range of the predictors and outcome. (we could try age-sqaured and see if that helps AUC and/or diagnostics)"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#last-class",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#last-class",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Last class",
    "text": "Last class\n\nLooked at simple logistic regression for binary outcome with\n\nOne continuous predictor \\[\\text{logit}(\\pi(X)) = \\beta_0 + \\beta_1 \\cdot X\\]\nOne binary predictor \\[\\text{logit}\\left(\\pi(X) \\right) = \\beta_0 + \\beta_1 \\cdot I(X=1)\\]\nOne multi-level predictor \\[\\text{logit}\\left(\\pi(X) \\right) = \\beta_0 + \\beta_1 \\cdot I(X=b) + \\beta_2 \\cdot I(X=c) + \\beta_3 \\cdot I(X=d)\\]"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#breast-cancer-example",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#breast-cancer-example",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Breast Cancer example",
    "text": "Breast Cancer example\n\nFor breast cancer diagnosis example, recall:\n\nOutcome: early or late stage breast cancer diagnosis (binary, categorical)\n\n\n \n\nPrimary covariate: Race/ethnicity\n\nNon-Hispanic white individuals are more likely to be diagnosed with breast cancer\n\nBut POC are more likely to be diagnosed at a later stage\n\n\n\n \n\nAdditional covariate: Age\n\nRisk factor for cancer diagnosis\n\n\n \n\nWe want to fit a multiple logistic regression model with both risk factors included as independent variables"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#introduction-to-multiple-logistic-regression",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#introduction-to-multiple-logistic-regression",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Introduction to Multiple Logistic Regression",
    "text": "Introduction to Multiple Logistic Regression\n\nIn multiple logistic regression model, we have &gt; 1 independent variable\n\nSometimes referred to as the “multivariable regression”\nThe independent variable can be any type:\n\nContinuous\nCategorical (ordinal or nominal)\n\n\n\n \n\nWe will follow similar procedures as we did for simple logistic regression\n\nBut we need to change our interpretation of estimates because we are adjusting for other variables"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#multiple-logistic-regression-model",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#multiple-logistic-regression-model",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Multiple Logistic Regression Model",
    "text": "Multiple Logistic Regression Model\n\n\n\nAssume we have a collection of \\(k\\) independent variables, denoted by \\(\\mathbf{X}=\\left( X_1, X_2, ..., X_k \\right)\\)\n\n \n\nThe conditional probability is \\(P(Y=1 | \\mathbf{X}) = \\pi(\\mathbf{X})\\)\n\n \n\nWe then model the probability with logistic regression: \\[\\text{logit}\\left(\\pi(\\mathbf{X})\\right) = \\beta_0\n+\\beta_1 \\cdot X_1 +\\beta_2 \\cdot X_2  +\\beta_3 \\cdot X_3 + ... + \\beta_k \\cdot X_k\\]\n\n\n\n\n\n\nWhy the bold \\(X\\)?\n\n\n\\(\\mathbf{X}\\) represents the vector of all the \\(X\\)’s. This is how we represent our group of covariates in our model."
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#fitting-the-multiple-logistic-regression-model",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#fitting-the-multiple-logistic-regression-model",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Fitting the Multiple Logistic Regression Model",
    "text": "Fitting the Multiple Logistic Regression Model\n\nFor a multiple logistic regression model with \\(k\\) independent variables, the vector of coefficients can be denoted by \\[\\boldsymbol{\\beta}^{T} = \\left(\\beta_0, \\beta_1, \\beta_2, ..., \\beta_k \\right)\\]\n\n \n\nAs with the simple logistic regression, we use maximum likelihood method for estimating coefficients\n\nVector of estimated coefficients: \\[\\widehat{\\boldsymbol{\\beta}}^{T} = \\left(\\widehat{\\beta}_0, \\widehat{\\beta}_1, \\widehat{\\beta}_2, ..., \\widehat{\\beta}_k \\right)\\]\n\n\n \n\nFor a model with \\(k\\) independent variables, there is \\(k+1\\) coefficients to estimate\n\nUnless one of those independent variables is a multi-level categorical variables, then we need more than \\(k+1\\) coefficients"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#breast-cancer-example-population-model",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#breast-cancer-example-population-model",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Breast Cancer Example: Population Model",
    "text": "Breast Cancer Example: Population Model\n\nWe can fit a logistic regression model with both race and ethnicity and age:\n\n\\[ \\begin{aligned}\n\\text{logit}\\left(\\pi(\\mathbf{X})\\right) = & \\beta_0\n+\\beta_1 \\cdot I \\left( R/E = H/L \\right)\n+\\beta_2 \\cdot I \\left( R/E = NH AIAN \\right) \\\\\n& +\\beta_3 \\cdot I \\left( R/E = NH API \\right)\n+\\beta_4 \\cdot I \\left( R/E = NH B \\right)\n+\\beta_5 \\cdot Age\n\\end{aligned}\\]\n \n \n\nNote that race and ethnicity requires 4 coefficients to include the indicator for each category\n\n \n\nCan replace \\(\\pi(\\mathbf{X})\\) with \\(\\pi({\\text{Race/ethnicity, Age}})\\)\n\n \n\n6 total coefficients (\\(\\beta_0\\) to \\(\\beta_5\\))"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#fitting-multiple-logistic-regression-model",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#fitting-multiple-logistic-regression-model",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Fitting Multiple Logistic Regression Model",
    "text": "Fitting Multiple Logistic Regression Model\n\nmulti_bc = glm(Late_stage_diag ~ Race_Ethnicity + Age_c, data = bc, family = binomial)\nsummary(multi_bc)\n\n\nCall:\nglm(formula = Late_stage_diag ~ Race_Ethnicity + Age_c, family = binomial, \n    data = bc)\n\nCoefficients:\n                                                 Estimate Std. Error z value\n(Intercept)                                     -1.038389   0.027292 -38.048\nRace_EthnicityHispanic-Latino                   -0.015424   0.083653  -0.184\nRace_EthnicityNH American Indian/Alaskan Native -0.085704   0.484110  -0.177\nRace_EthnicityNH Asian/Pacific Islander          0.133965   0.083797   1.599\nRace_EthnicityNH Black                           0.357692   0.071789   4.983\nAge_c                                            0.057151   0.003209  17.811\n                                                Pr(&gt;|z|)    \n(Intercept)                                      &lt; 2e-16 ***\nRace_EthnicityHispanic-Latino                      0.854    \nRace_EthnicityNH American Indian/Alaskan Native    0.859    \nRace_EthnicityNH Asian/Pacific Islander            0.110    \nRace_EthnicityNH Black                          6.27e-07 ***\nAge_c                                            &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 11861  on 9999  degrees of freedom\nResidual deviance: 11484  on 9994  degrees of freedom\nAIC: 11496\n\nNumber of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#breast-cancer-example-fitted-model",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#breast-cancer-example-fitted-model",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Breast Cancer Example: Fitted Model",
    "text": "Breast Cancer Example: Fitted Model\n\nWe now have the fitted logistic regression model with both race and ethnicity and age:\n\n\\[ \\begin{aligned}\n\\text{logit}\\left(\\widehat{\\pi}(\\mathbf{X})\\right) = & \\widehat{\\beta}_0\n+ \\widehat{\\beta}_1 \\cdot I \\left( R/E = H/L \\right)\n+ \\widehat{\\beta}_2 \\cdot I \\left( R/E = NH AIAN \\right) \\\\\n& + \\widehat{\\beta}_3 \\cdot I \\left( R/E = NH API \\right)\n+ \\widehat{\\beta}_4 \\cdot I \\left( R/E = NH B \\right)\n+ \\widehat{\\beta}_5 \\cdot Age \\\\\n\\\\\n\\text{logit}\\left(\\widehat{\\pi}(\\mathbf{X})\\right) = &-4.56\n-0.02 \\cdot I \\left( R/E = H/L \\right)\n-0.09 \\cdot I \\left( R/E = NH AIAN \\right) \\\\\n& +0.13 \\cdot I \\left( R/E = NH API \\right)\n+0.36 \\cdot I \\left( R/E = NH B \\right)\n+0.06 \\cdot Age\n\\end{aligned}\\]\n\n6 total coefficients (\\(\\widehat{\\beta}_0\\) to \\(\\widehat{\\beta}_5\\))"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#testing-significance-of-the-coefficients",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#testing-significance-of-the-coefficients",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Testing Significance of the Coefficients",
    "text": "Testing Significance of the Coefficients\n\nRefer to Lesson 6 for more information on each test!!\n\n \n\nWe use the same three tests that we discussed in Simple Logistic Regression to test individual coefficients\n\nWald test\n\nCan be used to test a single coefficient\n\nScore test\nLikelihood ratio test (LRT)\n\nCan be used to test a single coefficient or multiple coefficients\n\n\n\n \n\nTextbook and our class focuses on Wald and LRT only"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#a-note-on-wording",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#a-note-on-wording",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "A note on wording",
    "text": "A note on wording\n\nWhen I say “test a single coefficient” or “test multiple coefficients” I am referring to the \\(\\beta\\)’s\n\nA single variable can have a single coefficient\n\nExample: testing age\n\nA single variable can have multiple coefficients\n\nExample: testing race and ethnicity\n\nMuliple variables will have multiple coefficients\n\nExample: testing age and race and ethnicity together\n\n\n \nWhen I say “test a variable” I mean “determine if the model with the variable is more likely than the model without that variable”\n\nWe can use the Wald test to do this is some scenarios (single, continuous covariate)\nBUT I advise you practice using the LRT whenever comparing models (aka testing variables)"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#all-three-tests-together",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#all-three-tests-together",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "All three tests together",
    "text": "All three tests together"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#from-lesson-6-wald-test",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#from-lesson-6-wald-test",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "From Lesson 6: Wald test",
    "text": "From Lesson 6: Wald test\n\nAssumes test statistic W follows a standard normal distribution under the null hypothesis\nTest statistic: \\[W=\\frac{{\\hat{\\beta}}_j}{SE_{\\hat{\\beta}_j}}\\sim N(0,1)\\]\n\nwhere \\(\\widehat{\\beta}_j\\) is a MLE of coefficient \\(j\\)\n\n95% Wald confidence interval: \\[{\\hat{\\beta}}_1\\pm1.96 \\cdot SE_{{\\hat{\\beta}}_j}\\]\nThe Wald test is a routine output in R (summary() of glm() output)\n\nIncludes \\(SE_{{\\hat{\\beta}}_j}\\) and can easily find confidence interval with tidy()\n\nImportant note: Wald test is best for confidence intervals of our coefficient estimates or estimated odds ratios."
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#wald-test-procedure-with-confidence-intervals",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#wald-test-procedure-with-confidence-intervals",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Wald test procedure with confidence intervals",
    "text": "Wald test procedure with confidence intervals\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the confidence interval and determine if it overlaps with null\n\nOverlap with null (usually 0 for coefficient) = fail to reject null\nNo overlap with null (usually 0 for coefficient) = reject null\n\nWrite a conclusion to the hypothesis test\n\nWhat is the estimate and its confidence interval?\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#wald-test-in-our-example",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#wald-test-in-our-example",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Wald test in our example",
    "text": "Wald test in our example\n\nFrom our multiple logistic regression model:\n\n\\[ \\begin{aligned}\n\\text{logit}\\left(\\pi(x_i)\\right) = & \\beta_0\n+\\beta_1 \\cdot I \\left( R/E = H/L \\right)\n+\\beta_2 \\cdot I \\left( R/E = NH AIAN \\right) \\\\\n& +\\beta_3 \\cdot I \\left( R/E = NH API \\right)\n+\\beta_4 \\cdot I \\left( R/E = NH B \\right)\n+\\beta_5 \\cdot Age\n\\end{aligned}\\]\n \n\nWald test can help us construct the confidence interval for ALL coefficient estimates\n\n \n\nIf we want to use the Wald test to determine if a covariate is significant in our model\n\nCan only do so for age"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\nNeeded steps:\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\nCalculate the confidence interval and determine if it overlaps with null\nWrite a conclusion to the hypothesis test"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-1",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\n\nSet the level of significance \\(\\alpha\\)\n\n\\(\\alpha = 0.05\\)\n\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\n\\(H_0: \\beta_5 = 0\\)\n\\(H_1: \\beta_5 \\neq 0\\)"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-2",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-2",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\n\nCalculate the confidence interval and determine if it overlaps with null\n\n\nLook at coefficient estimates (CI should not contain 0) OR estimated odds ratio (CI should not contain 1)\n\n\n\n\ntidy(multi_bc, conf.int=T) %&gt;% \n  gt() %&gt;% \n  tab_options(table.font.size = 28) %&gt;%\n  fmt_number(decimals = 2)\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n−1.04\n0.03\n−38.05\n0.00\n−1.09\n−0.99\n\n\nRace_EthnicityHispanic-Latino\n−0.02\n0.08\n−0.18\n0.85\n−0.18\n0.15\n\n\nRace_EthnicityNH American Indian/Alaskan Native\n−0.09\n0.48\n−0.18\n0.86\n−1.12\n0.81\n\n\nRace_EthnicityNH Asian/Pacific Islander\n0.13\n0.08\n1.60\n0.11\n−0.03\n0.30\n\n\nRace_EthnicityNH Black\n0.36\n0.07\n4.98\n0.00\n0.22\n0.50\n\n\nAge_c\n0.06\n0.00\n17.81\n0.00\n0.05\n0.06"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-3",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-3",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\n\nCalculate the confidence interval and determine if it overlaps with null\n\n\nLook at coefficient estimates (CI should not contain 0) OR estimated odds ratio (CI should not contain 1)\n\n\n\n\ntbl_regression(multi_bc, \n               exponentiate = TRUE) %&gt;%\n  as_gt() %&gt;% \n  tab_options(table.font.size = 30)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\n\nRace_Ethnicity\n\n\n\n\n\n\n\n\n    NH White\n—\n—\n\n\n\n\n    Hispanic-Latino\n0.98\n0.83, 1.16\n0.9\n\n\n    NH American Indian/Alaskan Native\n0.92\n0.33, 2.25\n0.9\n\n\n    NH Asian/Pacific Islander\n1.14\n0.97, 1.35\n0.11\n\n\n    NH Black\n1.43\n1.24, 1.65\n&lt;0.001\n\n\nAge_c\n1.06\n1.05, 1.07\n&lt;0.001\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-4",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-4",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\n\nWrite a conclusion to the hypothesis test\n\nGiven race and ethnicity is already in the model, the regression model with age is more likely than the model without age (p-val &lt; 0.001)."
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#likelihood-ratio-test",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#likelihood-ratio-test",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Likelihood ratio test",
    "text": "Likelihood ratio test\n\nLikelihood ratio test answers the question:\n\nFor a specific covariate, which model tell us more about the outcome variable: the model including the covariate (or set of covariates) or the model omitting the covariate (or set of covariates)?\nAka: Which model is more likely given our data: model including the covariate or the model omitting the covariate?\n\n\n \n\nTest a single coefficient by comparing different models\n\nVery similar to the F-test\n\n\n \n\nImportant: LRT can be used conduct hypothesis tests for multiple coefficients\n\nJust like F-test, we can test a single coefficient, continuous/binary covariate, multi-level covariate, or multiple covariates"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#likelihood-ratio-test-33",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#likelihood-ratio-test-33",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Likelihood ratio test (3/3)",
    "text": "Likelihood ratio test (3/3)\n\nIf testing single variable and it’s continuous or binary, still use this hypothesis test:\n\n\\(H_0\\): \\(\\beta_j = 0\\)\n\\(H_1\\): \\(\\beta_j \\neq 0\\)\n\n\n \n\nIf testing single variable and it’s categorical with mroe than 2 groups, use this hypothesis test:\n\n\\(H_0\\): \\(\\beta_j=\\beta_{j+1}=\\ldots=\\beta_{j+i-1}=0\\)\n\\(H_1\\): at least one of the above \\(\\beta\\)’s is not equal to 0\n\n\n \n\nIf testing a set of variables, use this hypothesis test:\n\n\\(H_0\\): \\(\\beta_1=\\beta_{2}=\\ldots=\\beta_{k}=0\\)\n\\(H_1\\): at least one of the above \\(\\beta\\)’s is not equal to 0"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#lrt-procedure",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#lrt-procedure",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "LRT procedure",
    "text": "LRT procedure\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic and p-value\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#likelihood-ratio-test-33-1",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#likelihood-ratio-test-33-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Likelihood ratio test (3/3)",
    "text": "Likelihood ratio test (3/3)\n\nFrom our multiple logistic regression model:\n\n\\[ \\begin{aligned}\n\\text{logit}\\left(\\pi(\\mathbf{X})\\right) = & \\beta_0\n+\\beta_1 \\cdot I \\left( R/E = H/L \\right)\n+\\beta_2 \\cdot I \\left( R/E = NH AIAN \\right) \\\\\n& +\\beta_3 \\cdot I \\left( R/E = NH API \\right)\n+\\beta_4 \\cdot I \\left( R/E = NH B \\right)\n+\\beta_5 \\cdot Age\n\\end{aligned}\\]\n\nWe can test a single coefficient or multiple coefficients\n \n\nExample 1: Single, continuous variable: Age\n\n \n\nExample 2: Single, &gt;2 categorical variable: Race and Ethnicity\n\n \n\nExample 3: Set of variables: Race and Ethnicity, and Age"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#reminder-on-nested-models",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#reminder-on-nested-models",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Reminder on nested models",
    "text": "Reminder on nested models\n\nLikelihood ratio test is only suitable to test “nested” models\n“Nested” models means the bigger model (full model) contains all the independent variables of the smaller model (reduced model)\nWe cannot compare the following two models using LRT:\n\nModel 1: \\[ \\begin{aligned} \\text{logit}\\left(\\pi(\\mathbf{X})\\right) = & \\beta_0 +\\beta_1 \\cdot I \\left( R/E = H/L \\right) +\\beta_2 \\cdot I \\left( R/E = NH AIAN \\right) \\\\ & +\\beta_3 \\cdot I \\left( R/E = NH API \\right) +\\beta_4 \\cdot I \\left( R/E = NH B \\right) \\end{aligned}\\]\nModel 2: \\[\\begin{aligned} \\text{logit}\\left(\\pi(Age)\\right) = & \\beta_0+\\beta_1 \\cdot Age  \\end{aligned}\\]\n\nIf the two models to be compared are not nested, likelihood ratio test should not be used"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-5",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-5",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\nNeeded steps:\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\nCalculate the test statistic and p-value\nWrite a conclusion to the hypothesis test"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-6",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-6",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\n\nSet the level of significance \\(\\alpha\\)\n\n\\(\\alpha = 0.05\\)\n\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\n\\(H_0: \\beta_5 = 0\\) or model without age is more likely\n\\(H_1: \\beta_5 \\neq 0\\) or model with age is more likely"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-7",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-7",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\n\nCalculate the test statistic and p-value\n\n\nmulti_bc = glm(Late_stage_diag ~ Race_Ethnicity + Age_c, data = bc, family = binomial)\nre_bc = glm(Late_stage_diag ~ Race_Ethnicity, data = bc, family = binomial)\n\nlmtest::lrtest(multi_bc, re_bc)\n\nLikelihood ratio test\n\nModel 1: Late_stage_diag ~ Race_Ethnicity + Age_c\nModel 2: Late_stage_diag ~ Race_Ethnicity\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n1   6 -5741.8                         \n2   5 -5918.1 -1 352.63  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-8",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-8",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\n\nWrite a conclusion to the hypothesis test\n\nGiven race and ethnicity is already in the model, the regression model with age is more likely than the model without age (p-val &lt; \\(2.2\\cdot10^{-16}\\))."
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-2-single-2-categorical-variable-race-and-ethnicity",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-2-single-2-categorical-variable-race-and-ethnicity",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 2: Single, >2 categorical variable: Race and Ethnicity",
    "text": "Example 2: Single, &gt;2 categorical variable: Race and Ethnicity\n\n\nSingle, &gt;2 categorical variable: Race and Ethnicity\n\n\nGiven age is already in the model, is the regression model with race and ethnicity more likely than the model without race and ethnicity?\n\n\nNeeded steps:\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\nCalculate the test statistic and p-value\nWrite a conclusion to the hypothesis test"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-2-single-2-categorical-variable-race-and-ethnicity-1",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-2-single-2-categorical-variable-race-and-ethnicity-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 2: Single, >2 categorical variable: Race and Ethnicity",
    "text": "Example 2: Single, &gt;2 categorical variable: Race and Ethnicity\n\n\nSingle, &gt;2 categorical variable: Race and Ethnicity\n\n\nGiven age is already in the model, is the regression model with race and ethnicity more likely than the model without race and ethnicity?\n\n\n\nSet the level of significance \\(\\alpha\\)\n\n\\(\\alpha = 0.05\\)\n\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\n\\(H_0: \\beta_1 = \\beta_2 = \\beta_3 = \\beta_4 = 0\\) or model without race and ethnicity is more likely\n$H_1: at least one \\(\\beta\\) is not 0 or model with race and ethnicity is more likely"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-2-single-2-categorical-variable-race-and-ethnicity-2",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-2-single-2-categorical-variable-race-and-ethnicity-2",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 2: Single, >2 categorical variable: Race and Ethnicity",
    "text": "Example 2: Single, &gt;2 categorical variable: Race and Ethnicity\n\n\nSingle, &gt;2 categorical variable: Race and Ethnicity\n\n\nGiven age is already in the model, is the regression model with race and ethnicity more likely than the model without race and ethnicity?\n\n\n\nCalculate the test statistic and p-value\n\n\nmulti_bc = glm(Late_stage_diag ~ Race_Ethnicity + Age_c, data = bc, family = binomial)\nage_bc = glm(Late_stage_diag ~ Age_c, data = bc, family = binomial)\n\nlmtest::lrtest(multi_bc, age_bc)\n\nLikelihood ratio test\n\nModel 1: Late_stage_diag ~ Race_Ethnicity + Age_c\nModel 2: Late_stage_diag ~ Age_c\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n1   6 -5741.8                         \n2   2 -5754.8 -4 26.053  3.087e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-2-single-2-categorical-variable-race-and-ethnicity-3",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-2-single-2-categorical-variable-race-and-ethnicity-3",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 2: Single, >2 categorical variable: Race and Ethnicity",
    "text": "Example 2: Single, &gt;2 categorical variable: Race and Ethnicity\n\n\nSingle, &gt;2 categorical variable: Race and Ethnicity\n\n\nGiven age is already in the model, is the regression model with race and ethnicity more likely than the model without race and ethnicity?\n\n\n\nWrite a conclusion to the hypothesis test\n\nGiven age is already in the model, the regression model with race and ethnicity is more likely than the model without race and ethnicity (p-val = \\(3.1\\cdot10^{-5}\\) &lt; 0.05)."
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-3-set-of-variables-race-and-ethnicity-and-age",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-3-set-of-variables-race-and-ethnicity-and-age",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 3: Set of variables: Race and Ethnicity, and Age",
    "text": "Example 3: Set of variables: Race and Ethnicity, and Age\n\n\nSet of variables: Race and Ethnicity, and Age\n\n\nIs the regression model with race and ethnicity and age more likely than the model without race and ethnicity nor age?\n\n\nNeeded steps:\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\nCalculate the test statistic and p-value\nWrite a conclusion to the hypothesis test"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-3-set-of-variables-race-and-ethnicity-and-age-1",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-3-set-of-variables-race-and-ethnicity-and-age-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 3: Set of variables: Race and Ethnicity, and Age",
    "text": "Example 3: Set of variables: Race and Ethnicity, and Age\n\n\nSet of variables: Race and Ethnicity, and Age\n\n\nIs the regression model with race and ethnicity and age more likely than the model without race and ethnicity nor age?\n\n\n\nSet the level of significance \\(\\alpha\\)\n\n\\(\\alpha = 0.05\\)\n\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\n\\(H_0: \\beta_1 = \\beta_2 = \\beta_3 = \\beta_4 = \\beta_5 = 0\\) or model without race and ethnicity and age is more likely\n$H_1: at least one \\(\\beta\\) is not 0 or model with race and ethnicity and age is more likely"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-3-set-of-variables-race-and-ethnicity-and-age-2",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-3-set-of-variables-race-and-ethnicity-and-age-2",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 3: Set of variables: Race and Ethnicity, and Age",
    "text": "Example 3: Set of variables: Race and Ethnicity, and Age\n\n\nSet of variables: Race and Ethnicity, and Age\n\n\nIs the regression model with race and ethnicity and age more likely than the model without race and ethnicity nor age?\n\n\n\nCalculate the test statistic and p-value\n\n\nmulti_bc = glm(Late_stage_diag ~ Race_Ethnicity + Age_c, data = bc, family = binomial)\nintercept_bc = glm(Late_stage_diag ~ 1, data = bc, family = binomial)\n\nlmtest::lrtest(multi_bc, intercept_bc)\n\nLikelihood ratio test\n\nModel 1: Late_stage_diag ~ Race_Ethnicity + Age_c\nModel 2: Late_stage_diag ~ 1\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n1   6 -5741.8                         \n2   1 -5930.5 -5 377.32  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-3-set-of-variables-race-and-ethnicity-and-age-3",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-3-set-of-variables-race-and-ethnicity-and-age-3",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 3: Set of variables: Race and Ethnicity, and Age",
    "text": "Example 3: Set of variables: Race and Ethnicity, and Age\n\n\nSet of variables: Race and Ethnicity, and Age\n\n\nIs the regression model with race and ethnicity and age more likely than the model without race and ethnicity nor age?\n\n\n\nWrite a conclusion to the hypothesis test\n\nThe regression model with race and ethnicity and age is more likely than the model omitting race and ethnicity and age (p-val &lt; \\(2.2\\cdot10^{-16}\\))."
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#estimatedpredicted-probability-for-mlr",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#estimatedpredicted-probability-for-mlr",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Estimated/Predicted Probability for MLR",
    "text": "Estimated/Predicted Probability for MLR\n\nBasic idea for predicting/estimating probability stays the same\n\n \n\nCalculations will be slightly different\n\nEspecially for the confidence interval\n\n\n \n\nRecall our fitted model for late stage breast cancer diagnosis: \\[ \\begin{aligned}\n\\text{logit}\\left(\\widehat{\\pi}(\\mathbf{X})\\right) = &-4.56\n-0.02 \\cdot I \\left( R/E = H/L \\right)\n-0.09 \\cdot I \\left( R/E = NH AIAN \\right) \\\\\n& +0.13 \\cdot I \\left( R/E = NH API \\right)\n+0.36 \\cdot I \\left( R/E = NH B \\right)\n+0.06 \\cdot Age\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicted-probability",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicted-probability",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Predicted Probability",
    "text": "Predicted Probability\n\nWe may be interested in predicting probability of having a late stage breast cancer diagnosis for a specific age.\nThe predicted probability is the estimated probability of having the event for given values of covariate(s)\nRecall our fitted model for late stage breast cancer diagnosis: \\[ \\begin{aligned}\n\\text{logit}\\left(\\widehat{\\pi}(\\mathbf{X})\\right) = &-4.56\n-0.02 \\cdot I \\left( R/E = H/L \\right)\n-0.09 \\cdot I \\left( R/E = NH AIAN \\right) \\\\\n& +0.13 \\cdot I \\left( R/E = NH API \\right)\n+0.36 \\cdot I \\left( R/E = NH B \\right)\n+0.06 \\cdot Age\n\\end{aligned}\\]\nWe can convert it to the predicted probability: \\[\\hat{\\pi}(\\mathbf{X})=\\dfrac{\\exp \\left( \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot I \\left( R/E = H/L \\right) + ... + \\widehat{\\beta}_5 \\cdot Age \\right)}  {1+\\exp \\left(\\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot I \\left( R/E = H/L \\right) + ... + \\widehat{\\beta}_5 \\cdot Age \\right)}\\]\n\nThis is an inverse logit calculation\n\nWe can calculate this using the the predict() function like in BSTA 512\n\nAnother option: taking inverse logit of fitted values from augment() function"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Predicting probability in R",
    "text": "Predicting probability in R\n\n\nPredicting probability of late stage breast cancer diagnosis\n\n\nFor someone who is 60 years old and Non-Hispanic Asian/Pacific Islander, what is the predicted probability for late stage breast cancer diagnosis (with confidence intervals)?\n\n\nNeeded steps:\n\nCalculate probability prediction\nCheck if we can use Normal approximation\nCalculate confidence interval\n\nUsing logit scale then converting\nUsing Normal approximation\n\nInterpret results"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r-1",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Predicting probability in R",
    "text": "Predicting probability in R\n\n\nPredicting probability of late stage breast cancer diagnosis\n\n\nFor someone who is 60 years old and Non-Hispanic Asian/Pacific Islander, what is the predicted probability for late stage breast cancer diagnosis (with confidence intervals)?\n\n\n\nCalculate probability prediction\n\n\nnewdata = data.frame(Age_c = 60 - mean_age, \n                     Race_Ethnicity = \"NH Asian/Pacific Islander\")\npred1 = predict(multi_bc, newdata, se.fit = T, type=\"response\")\npred1\n\n$fit\n        1 \n0.2685667 \n\n$se.fit\n         1 \n0.01572695 \n\n$residual.scale\n[1] 1"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r-2",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r-2",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Predicting probability in R",
    "text": "Predicting probability in R\n\n\nPredicting probability of late stage breast cancer diagnosis\n\n\nFor someone who is 60 years old and Non-Hispanic Asian/Pacific Islander, what is the predicted probability for late stage breast cancer diagnosis (with confidence intervals)?\n\n\n\nCheck if we can use Normal approximation\n\nWe can use the Normal approximation if: \\(\\widehat{p}n = \\widehat{\\pi}(X)\\cdot n &gt; 10\\) and \\((1-\\widehat{p})n = (1-\\widehat{\\pi}(X))\\cdot n &gt; 10\\).\n\nn = nobs(multi_bc)\np = pred1$fit\nn*p\n\n       1 \n2685.667 \n\nn*(1-p)\n\n       1 \n7314.333 \n\n\nWe can use the Normal approximation!"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r-3",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r-3",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Predicting probability in R",
    "text": "Predicting probability in R\n\n\nPredicting probability of late stage breast cancer diagnosis\n\n\nFor someone who is 60 years old and Non-Hispanic Asian/Pacific Islander, what is the predicted probability for late stage breast cancer diagnosis (with confidence intervals)?\n\n\n3b. Calculate confidence interval (Option 2: with Normal approximation)\n\npred = predict(multi_bc, newdata, se.fit = T, type = \"response\")\n\nLL_CI = pred$fit - qnorm(1-0.05/2) * pred$se.fit\nUL_CI = pred$fit + qnorm(1-0.05/2) * pred$se.fit\n\nc(Pred = pred$fit, LL_CI, UL_CI) %&gt;% round(digits=3)\n\nPred.1      1      1 \n 0.269  0.238  0.299"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r-4",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r-4",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Predicting probability in R",
    "text": "Predicting probability in R\n\n\nPredicting probability of late stage breast cancer diagnosis\n\n\nFor someone who is 60 years old and Non-Hispanic Asian/Pacific Islander, what is the predicted probability for late stage breast cancer diagnosis (with confidence intervals)?\n\n\n\nInterpret results\n\nFor someone who is 60 years old and Non-Hispanic Asian/Pacific Islander, the predicted probability of late stage breast cancer diagnosis is 0.269 (95% CI: 0.238, 0.299)."
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-table",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-table",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "How to present odds ratios: Table",
    "text": "How to present odds ratios: Table\n\ntbl_regression() in the gtsummary package is helpful for presenting the odds ratios in a clean way\n\n\nlibrary(gtsummary)\ntbl_regression(multi_bc, exponentiate = TRUE) %&gt;% \n  as_gt() %&gt;% # allows us to use tab_options()\n  tab_options(table.font.size = 38)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\n\nRace_Ethnicity\n\n\n\n\n\n\n\n\n    NH White\n—\n—\n\n\n\n\n    Hispanic-Latino\n0.98\n0.83, 1.16\n0.9\n\n\n    NH American Indian/Alaskan Native\n0.92\n0.33, 2.25\n0.9\n\n\n    NH Asian/Pacific Islander\n1.14\n0.97, 1.35\n0.11\n\n\n    NH Black\n1.43\n1.24, 1.65\n&lt;0.001\n\n\nAge_c\n1.06\n1.05, 1.07\n&lt;0.001\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-forest-plot-setup",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-forest-plot-setup",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "How to present odds ratios: Forest Plot Setup",
    "text": "How to present odds ratios: Forest Plot Setup\n\nlibrary(broom.helpers)\nMLR_tidy = tidy_and_attach(multi_bc, conf.int=T, exponentiate = T) %&gt;%\n  tidy_remove_intercept() %&gt;%\n  tidy_add_reference_rows() %&gt;%\n  tidy_add_estimate_to_reference_rows() %&gt;%\n  tidy_add_term_labels()\nglimpse(MLR_tidy)\n\nRows: 6\nColumns: 16\n$ term           &lt;chr&gt; \"Race_EthnicityNH White\", \"Race_EthnicityHispanic-Latin…\n$ variable       &lt;chr&gt; \"Race_Ethnicity\", \"Race_Ethnicity\", \"Race_Ethnicity\", \"…\n$ var_label      &lt;chr&gt; \"Race_Ethnicity\", \"Race_Ethnicity\", \"Race_Ethnicity\", \"…\n$ var_class      &lt;chr&gt; \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"nume…\n$ var_type       &lt;chr&gt; \"categorical\", \"categorical\", \"categorical\", \"categoric…\n$ var_nlevels    &lt;int&gt; 5, 5, 5, 5, 5, NA\n$ contrasts      &lt;chr&gt; \"contr.treatment\", \"contr.treatment\", \"contr.treatment\"…\n$ contrasts_type &lt;chr&gt; \"treatment\", \"treatment\", \"treatment\", \"treatment\", \"tr…\n$ reference_row  &lt;lgl&gt; TRUE, FALSE, FALSE, FALSE, FALSE, NA\n$ label          &lt;chr&gt; \"NH White\", \"Hispanic-Latino\", \"NH American Indian/Alas…\n$ estimate       &lt;dbl&gt; 1.0000000, 0.9846940, 0.9178662, 1.1433526, 1.4300256, …\n$ std.error      &lt;dbl&gt; NA, 0.083653090, 0.484110085, 0.083796726, 0.071788616,…\n$ statistic      &lt;dbl&gt; NA, -0.1843845, -0.1770333, 1.5986877, 4.9825778, 17.81…\n$ p.value        &lt;dbl&gt; NA, 8.537118e-01, 8.594822e-01, 1.098900e-01, 6.274274e…\n$ conf.low       &lt;dbl&gt; NA, 0.8344282, 0.3262638, 0.9688184, 1.2414629, 1.05221…\n$ conf.high      &lt;dbl&gt; NA, 1.158411, 2.254643, 1.345732, 1.645053, 1.065538"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-forest-plot-setup-1",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-forest-plot-setup-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "How to present odds ratios: Forest Plot Setup",
    "text": "How to present odds ratios: Forest Plot Setup\n\nMLR_tidy = MLR_tidy %&gt;%\n  mutate(var_label = case_match(var_label, \n                               \"Race_Ethnicity\" ~ \"Race and ethnicity\", \n                               \"Age_c\" ~ \"\"), \n         label = case_match(label, \n                            \"NH White\" ~ \"Non-Hispanic White\", \n                            \"Hispanic-Latino\" ~ \"Hispanic-Latinx\", \n                            \"NH American Indian/Alaskan Native\" ~ \"Non-Hispanic American \\n Indian/Alaskan Native\", \n                            \"NH Asian/Pacific Islander\" ~ \"Non-Hispanic \\n Asian/Pacific Islander\",\n                            \"NH Black\" ~ \"Non-Hispanic Black\", \n                               \"Age_c\" ~ \"Age (yrs)\"))\n  # %&gt;%\n  #                       fct_relevel(\"Age (yrs)\", \"Non-Hispanic \\n Asian/Pacific Islander\", \"Non-Hispanic American \\n Indian/Alaskan Native\", \"Non-Hispanic White\", \"Hispanic-Latinx\", \"Non-Hispanic Black\"))"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-forest-plot",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-forest-plot",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "How to present odds ratios: Forest Plot",
    "text": "How to present odds ratios: Forest Plot\n\nMLR_tidy = MLR_tidy %&gt;% mutate(label = fct_reorder(label, term))\n\nplot_MLR = ggplot(data=MLR_tidy, \n       aes(y=label, x=estimate, xmin=conf.low, xmax=conf.high)) + \n  geom_point(size = 3) +  geom_errorbarh(height=.2) + \n  \n  geom_vline(xintercept=1, color='#C2352F', linetype='dashed', alpha=1) +\n  theme_classic() +\n  \n  facet_grid(rows = vars(var_label), scales = \"free\",\n             space='free_y', switch = \"y\") + \n  \n  labs(x = \"OR (95% CI)\", \n       title = \"Odds ratios of Late Stage Breast Cancer Diagnosis\") +\n  theme(axis.title = element_text(size = 25), \n        axis.text = element_text(size = 25), \n        title = element_text(size = 25), \n        axis.title.y=element_blank(), \n        strip.text = element_text(size = 25), \n        strip.placement = \"outside\", \n        strip.background = element_blank())\n# plot_MLR"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-forest-plot-1",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-forest-plot-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "How to present odds ratios: Forest Plot",
    "text": "How to present odds ratios: Forest Plot"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#adding-odds-ratios",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#adding-odds-ratios",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Adding odds ratios",
    "text": "Adding odds ratios\n\nMLR_tidy = MLR_tidy %&gt;% \n  mutate(estimate_r = round(estimate, 2), \n         conf.low_r = round(conf.low, 2), \n         conf.high_r = round(conf.high, 2), \n         OR_char = paste0(estimate_r, \" (\", conf.low_r, \", \", conf.high_r, \")\"), \n         OR_char = ifelse(reference_row == F | is.na(reference_row), OR_char, NA))\n\n \n\n“Plot” of the text for odds ratios estimates\n\n\nOR_labs = ggplot(data=MLR_tidy, aes(y=label)) +\n  geom_text(aes(x = -1, label = OR_char), hjust = 0, size=8) +   \n  xlim(-1, 1) +\n  facet_grid(rows = vars(var_label), scales = \"free\",\n             space='free_y') +\n  theme_void() + \n    theme(strip.text = element_blank())"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#combine-them",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#combine-them",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Combine them!!",
    "text": "Combine them!!\n\nlibrary(cowplot)\nplot_grid(plot_MLR, OR_labs, ncol=2, align = \"h\", rel_widths = c(4, 1))"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#multivariable-logistic-regression-model",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#multivariable-logistic-regression-model",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Multivariable Logistic Regression Model",
    "text": "Multivariable Logistic Regression Model\n\nThe multivariable model of logistic regression (called multiple logistic regression) is useful in that it statistically adjusts the estimated effect of each variable in the model\n\n \n\nEach estimated coefficient provides an estimate of the log odds adjusting for all other variables included in the model\n \n\nThe adjusted odds ratio can be different from or similar to the unadjusted odds ratio\n\n \n\nComparing adjusted vs. unadjusted odds ratios can be a useful activity"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#interpretation-of-coefficients-in-mlr",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#interpretation-of-coefficients-in-mlr",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Interpretation of Coefficients in MLR",
    "text": "Interpretation of Coefficients in MLR\n\nThe interpretation of coefficients in multiple logistic regression is essentially the same as the interpretation of coefficients in simple logistic regression\n\n \n\nFor interpretation, we need to\n\npoint out that these are adjusted estimates\nprovide a list of other variables in the model"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-race-and-ethnicity-and-age-model-fit-fixed",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-race-and-ethnicity-and-age-model-fit-fixed",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example: Race and Ethnicity and Age model fit (FIXED)",
    "text": "Example: Race and Ethnicity and Age model fit (FIXED)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\n\nRace_Ethnicity\n\n\n\n\n\n\n\n\n    NH White\n—\n—\n\n\n\n\n    Hispanic-Latino\n0.98\n0.83, 1.16\n0.9\n\n\n    NH American Indian/Alaskan Native\n0.92\n0.33, 2.25\n0.9\n\n\n    NH Asian/Pacific Islander\n1.14\n0.97, 1.35\n0.11\n\n\n    NH Black\n1.43\n1.24, 1.65\n&lt;0.001\n\n\nAge_c\n1.06\n1.05, 1.07\n&lt;0.001\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\n\n\nThe estimated odds of late stage breast cancer diagnosis for Hispanic-Latinx individuals is 0.98 times that of Non-Hispanic White individuals, controlling for age (95% CI: 0.83, 1.16).\nThe estimated odds of late stage breast cancer diagnosis for Non-Hispanic American Indian/Alaskan Natives is 0.92 times that of Non-Hispanic White individuals, controlling for age (95% CI: 0.33, 2.25).\nThe estimated odds of late stage breast cancer diagnosis for Non-Hispanic Asian/Pacific Islanders is 1.14 times that of Non-Hispanic White individuals, controlling for age (95% CI: 0.97, 1.35).\nThe estimated odds of late stage breast cancer diagnosis for Non-Hispanic Black individuals is 1.43 times that of Non-Hispanic White individuals, controlling for age (95% CI: 1.24, 1.65).\nFor every one year increase in age, there is an 6% increase in the estimated odds of late stage breast cancer diagnosis, adjusting for race and ethnicity (95% CI: 5%, 7%)."
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#review-of-model-assessment-so-far-12",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#review-of-model-assessment-so-far-12",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Review of model assessment so far (1/2)",
    "text": "Review of model assessment so far (1/2)\n\nOverall measurements of fit\n\nHow well does the fitted logistic regression model predict the outcome?\nDifferent ways to measure the answer to this question\n\n\n\n\n\n\n\n\n\n\n\nMeasure of fit\nHypothesis tested?\nEquation\nR code\n\n\n\n\nPearson residual\nYes\n\\(X^2=\\sum_{j=1}^{J}{r\\left(Y_j,{\\hat{\\pi}}_j\\right)^2}\\)\nNot given\n\n\nHosmer-Lemeshow test\nYes\n\\(\\hat{C}=\\sum_{k=1}^{g}\\frac{\\left(o_k-n_k^\\prime{\\bar{\\pi}}_k\\right)^2}{n_k^\\prime{\\bar{\\pi}}_k(1-{\\bar{\\pi}}_k)}\\)\nhoslem.test()\n\n\nAUC-ROC\nKinda\nNot given\nauc(observed, predicted)\n\n\nAIC\nOnly to compare models\n\\(AIC = -2 \\cdot \\text{log-likelihood} + 2q\\)\nAIC(model_name)\n\n\nBIC\nOnly to compare models\n\\(BIC = -2 \\cdot \\text{log-likelihood} + q\\text{log}(n)\\)\nBIC(model_name)"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#review-of-model-assessment-so-far-22",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#review-of-model-assessment-so-far-22",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Review of model assessment so far (2/2)",
    "text": "Review of model assessment so far (2/2)\n\nNumerical problems\n\nAssess pre and post model fit\nNumerical problems often depend on the final model (which variables and interactions are included)\n\nDifferent numerical problems to look out for\n\nZero cell count\nComplete separation\nMulticollinearity"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#today",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#today",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Today",
    "text": "Today\n\nWe now use model diagnostics to identify any observations that the model does not fit well"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#review-of-number-of-covariate-patterns",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#review-of-number-of-covariate-patterns",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Review of Number of Covariate Patterns",
    "text": "Review of Number of Covariate Patterns\n\nCovariate patterns are the unique covariate combinations that are observed\n\n \n\nFor example: model contains two binary covariates (history of fracture and smoking status), there will be 4 unique combination of these factors\n\nThis model has 4 covariate patterns\nSubjects can be divided into 4 groups based on the covariates’ values\n\n\n \n\nWhen we have continuous covariates, the number of covariate patterns will be close to the number of individuals in the dataset"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#from-overall-measure-to-diagnostics",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#from-overall-measure-to-diagnostics",
    "title": "Lesson 14: Model Diagnostics",
    "section": "From overall measure to diagnostics",
    "text": "From overall measure to diagnostics\n\nNow we need to investigate diagnostics looking at individual data or covariate pattern data\n\nMake sure the overall measure has not been influenced by certain observations\n\n\n \n\nThe key quantities from logistic regression diagnostics are the components of “residual sum-of-squares”\n\nThe same idea as in the linear regression\nAssessed for each covariate pattern \\(j\\), by computing standardized Pearson residuals and Deviance residuals\n\nStandardization using \\(h_j\\), the leverage values"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#hat-matrix-and-leverage-values-linear-regression",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#hat-matrix-and-leverage-values-linear-regression",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Hat Matrix and Leverage Values: Linear regression",
    "text": "Hat Matrix and Leverage Values: Linear regression\n\nWe have learned “hat” matrix and leverage values from linear regression diagnostics\n\n \n\nIn linear regression, the hat matrix projects the outcome variable onto the covariate space:\n \n\n\\(H=X\\left(X^\\prime X\\right)^{-1}X^\\prime\\) and \\(\\hat{y}=Hy\\)\n\n \n\nThe linear regression residuals is thus \\(y - \\widehat{y}\\), or \\((I-H)y\\)\n\n\n \n\nThe leverage is just the diagonal elements of the hat matrix, which is proportional to the distance of \\(x_j\\) to the mean of the data \\(\\overline{x}\\)"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#hat-matrix-and-leverage-values-logistic-regression",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#hat-matrix-and-leverage-values-logistic-regression",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Hat Matrix and Leverage Values: Logistic regression",
    "text": "Hat Matrix and Leverage Values: Logistic regression\n\nIn logistic regression model, the hat matrix is: \\[H=V^\\frac{1}{2}X\\left(X^\\prime V\\ X\\right)^{-1}X^\\prime V^\\frac{1}{2}\\]\nThe leverage is \\[h_j=m_j\\cdot\\hat{\\pi}\\left(\\textbf{x}_j\\right)\\left[1-\\hat{\\pi}\\left(\\textbf{x}_j\\right)\\right]\\textbf{x}_j^\\prime\\left(\\textbf{X}^\\prime\\textbf{VX}\\right)^{-1}\\textbf{x}_j=v_j\\cdot b_j\\]\n\n\\(b\\): weighted distance of \\(x_j\\) from \\(\\overline{x}\\)\n\\(v_j\\): model based estimator of the variance of \\(y_j\\)\n\n\\(v_j=m_j\\cdot\\hat{\\pi}\\left(\\textbf{x}_j\\right)\\left[1-\\hat{\\pi}\\left(\\textbf{x}_j\\right)\\right]\\)\n\n\n\\(h_j\\) reflects the relative influence of each covariate pattern on the model’s fit"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#poll-everywhere-question-1",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#poll-everywhere-question-1",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#poll-everywhere-question-2",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#poll-everywhere-question-2",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#diagnostic-statistics-computation-12",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#diagnostic-statistics-computation-12",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Diagnostic Statistics Computation (1/2)",
    "text": "Diagnostic Statistics Computation (1/2)\n\nTwo diagnostic statistics computation approach\n\nApproach 1: computed by covariate pattern\n\nRecommendation of Hosmer-Lemeshow textbook\nR uses this approach\nIdentify outliers as group that shares the same covariate values (in the same covariate pattern)\n\nApproach 2: individual observation approach\n\nSAS uses this approach\nIdentify outliers as individual\n\n\nWhy prefer covariate patterns approach?\n\nWhen the number of covariate pattern is much smaller than n, there is risk that we may fail to identify influential and/or poorly fit covariate patterns using individual based on residual"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#diagnostic-statistics-computation-22",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#diagnostic-statistics-computation-22",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Diagnostic Statistics Computation (2/2)",
    "text": "Diagnostic Statistics Computation (2/2)\nConsider a covariate pattern with \\(m_j\\) subjects, all did not have event (some \\(y_i = 0\\)). So the estimated logistic probability is \\(\\widehat\\pi_j\\)\n\nPearson residual computed by individual \\[r_i=-\\sqrt{\\frac{{\\hat{\\pi}}_j}{(1-{\\hat{\\pi}}_j)}}\\]\nPearson residual computed by covariate pattern \\[r_i=-\\sqrt{m_j}\\sqrt{\\frac{{\\hat{\\pi}}_j}{(1-{\\hat{\\pi}}_j)}}\\]\nDifference between aboveresiduals will be large if \\(m_j\\) is large: usually a problem if less covariate patterns\n\nResidual from covariate pattern will identify poorly fit covariate patterns"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#diagnostics-of-logistic-regression",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#diagnostics-of-logistic-regression",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Diagnostics of Logistic Regression",
    "text": "Diagnostics of Logistic Regression\n\nModel diagnostics of logistic regression can be assessed by checking how influential a covariate pattern is:\n \n\nLook at change in residuals if a covariate pattern is excluded\n\nStandardized Pearson residual\nStandardized Deviance residual\n\n\n \n\nLook at change in coefficients if a covariate pattern is excluded"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#change-of-standardized-residuals",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#change-of-standardized-residuals",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Change of Standardized Residuals",
    "text": "Change of Standardized Residuals\n\nChange in standardized Pearson Chi-square statistic due to deletion of subjects with covariate pattern \\(x_j\\): \\[\\Delta X_j^2 = r_{sj}^2 = \\dfrac{r_j^2}{1-h_j}\\]\nDon’t need to know this: change in standardized deviance statistic due to deletion of subjects with covariate pattern \\(x_j\\): \\[\\Delta D_j = \\dfrac{d_j^2}{1-h_j}\\]\nRefer to Lesson 12: Assessing Model Fit for expression of Pearson residual"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#change-of-estimated-coefficients",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#change-of-estimated-coefficients",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Change of Estimated Coefficients",
    "text": "Change of Estimated Coefficients\n\nChange in estimated coefficients due to deletion of subjects with covariate pattern \\(x_j\\): \\[\\Delta \\widehat{\\beta}_j = \\dfrac{r_j^2 h_j}{(1-h_j)^2}\\]\n\n \n\nThis is the logistic regression analog of Cook’s influence statistic (in linear regression)"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#visual-assessment-for-diagnostics-of-logistic-regression-i",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#visual-assessment-for-diagnostics-of-logistic-regression-i",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Visual Assessment for Diagnostics of Logistic Regression (I)",
    "text": "Visual Assessment for Diagnostics of Logistic Regression (I)\n\nIn logistic regression, we mainly rely on graphical methods\n\nBecause the distribution of diagnostic measures under null hypothesis (that the model fits) is only known in certain limited settings\n\n\n \n\nFour plots for analysis of diagnostics in logistic regression:\n\n\\(\\Delta X_j^2\\) vs. \\({\\hat{\\pi}}_j\\)\n\\(\\Delta D_j\\) vs. \\({\\hat{\\pi}}_j\\)\n\\(\\Delta\\widehat{\\beta}_j\\) vs. \\({\\hat{\\pi}}_j\\)\n\\(h_j\\) vs. \\({\\hat{\\pi}}_j\\)"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#recall-the-model-we-fit-glow-study-with-interactions",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#recall-the-model-we-fit-glow-study-with-interactions",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Recall the model we fit: GLOW Study with interactions",
    "text": "Recall the model we fit: GLOW Study with interactions\n\nOutcome variable: any fracture in the first year of follow up (FRACTURE: 0 or 1)\nRisk factor/variable of interest: history of prior fracture (PRIORFRAC: 0 or 1)\nPotential confounder or effect modifier: age (AGE, a continuous variable)\nFitted model with interactions: \\[\\begin{aligned} \\text{logit}\\left(\\widehat\\pi(\\mathbf{X})\\right) & = \\widehat\\beta_0 &+ &\\widehat\\beta_1\\cdot I(\\text{PF}) & + &\\widehat\\beta_2\\cdot Age& + &\\widehat\\beta_3 \\cdot I(\\text{PF}) \\cdot Age \\\\  \n\\text{logit}\\left(\\widehat\\pi(\\mathbf{X})\\right) & = -1.376 &+ &1.002\\cdot I(\\text{PF})& + &0.063\\cdot Age& -&0.057 \\cdot I(\\text{PF}) \\cdot Age\n\\end{aligned}\\]\n\n \n\nLesson 12: determined the overall fit of this model\nToday: determine the if any observations/covariate patterns that model does not fit well"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#how-do-we-get-these-values-in-r",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#how-do-we-get-these-values-in-r",
    "title": "Lesson 14: Model Diagnostics",
    "section": "How do we get these values in R?",
    "text": "How do we get these values in R?\n\nNice function in the R script Logistic_Dx_Functions.R\n\nHighly suggest you save this R script for future use!!\n\n\n\nsource(here(\"lessons\", \"14_Model_diagnostics\", \"Logistic_Dx_Functions.R\"))\ndx_glow = dx(glow_m3)\nglimpse(dx_glow)\n\nRows: 71\nColumns: 16\n$ `(Intercept)`        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ priorfracYes         &lt;dbl&gt; 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0…\n$ age_c                &lt;dbl&gt; 1, -7, 7, -2, 10, 20, 1, -2, 2, 8, 18, -8, 11, 10…\n$ `priorfracYes:age_c` &lt;dbl&gt; 1, 0, 7, 0, 10, 0, 0, -2, 2, 0, 0, 0, 0, 0, -3, 0…\n$ y                    &lt;dbl&gt; 2, 2, 3, 2, 2, 1, 3, 3, 1, 5, 1, 3, 2, 1, 1, 4, 1…\n$ P                    &lt;dbl&gt; 0.4088354, 0.1402159, 0.4162991, 0.1822879, 0.420…\n$ n                    &lt;int&gt; 5, 15, 7, 10, 5, 2, 12, 8, 3, 15, 2, 18, 7, 4, 3,…\n$ yhat                 &lt;dbl&gt; 2.0441770, 2.1032389, 2.9140936, 1.8228786, 2.100…\n$ Pr                   &lt;dbl&gt; -0.04018670, -0.07677228, 0.06586860, 0.14507476,…\n$ dr                   &lt;dbl&gt; -0.04023255, -0.07730975, 0.06577949, 0.14332786,…\n$ h                    &lt;dbl&gt; 0.008844090, 0.003811004, 0.008725450, 0.00290085…\n$ sPr                  &lt;dbl&gt; -0.04036559, -0.07691899, 0.06615786, 0.14528564,…\n$ sdr                  &lt;dbl&gt; -0.04041165, -0.07745749, 0.06606836, 0.14353620,…\n$ dChisq               &lt;dbl&gt; 0.001629381, 0.005916530, 0.004376863, 0.02110791…\n$ dDev                 &lt;dbl&gt; 0.001633102, 0.005999662, 0.004365028, 0.02060264…\n$ dBhat                &lt;dbl&gt; 1.453897e-05, 2.263418e-05, 3.852626e-05, 6.14091…"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#key-to-the-values",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#key-to-the-values",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Key to the values",
    "text": "Key to the values\n\n\n\ncolnames(dx_glow)\n\n [1] \"(Intercept)\"        \"priorfracYes\"       \"age_c\"             \n [4] \"priorfracYes:age_c\" \"y\"                  \"P\"                 \n [7] \"n\"                  \"yhat\"               \"Pr\"                \n[10] \"dr\"                 \"h\"                  \"sPr\"               \n[13] \"sdr\"                \"dChisq\"             \"dDev\"              \n[16] \"dBhat\"             \n\n\n\nFor each covariate pattern (which is each row) …\n\ny: Number of events\nP: Estimated probability of events\nn: Number of observations\nyhat: Estimated number of events\nPr: Pearson residual\ndr: Deviance\nh: leverage\nsPr: Standardized Pearson residual\nsdr: Standardized deviance\ndChisq: Change in standardized Pearson residual\ndDev: Change in standardized deviance\ndBhat: Change in coefficient estimates"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#poll-everywhere-question-3",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#poll-everywhere-question-3",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#visual-assessment-for-diagnostics-of-logistic-regression",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#visual-assessment-for-diagnostics-of-logistic-regression",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Visual Assessment for Diagnostics of Logistic Regression",
    "text": "Visual Assessment for Diagnostics of Logistic Regression\n\nThe plots allow us to identify those covariate patterns that are…\n\nPoorly fit\n\nLarge values of \\(\\Delta X_j^2\\) (and/or \\(\\Delta D_j\\) if we looked at those)\n\nInfluential on estimated coefficients\n\nLarge values of \\(\\Delta\\widehat{\\beta}_j\\)\n\n\nIf you are interested to look at the contribution of leverage (ℎ_𝑗) to the values of the diagnostic statistic, you may also look at plots of:\n\n\\(\\Delta X_j^2\\) vs. \\({\\hat{\\pi}}_j\\)\n\\(\\Delta D_j\\) vs. \\({\\hat{\\pi}}_j\\)\n\\(\\Delta\\widehat{\\beta}_j\\) vs. \\({\\hat{\\pi}}_j\\)"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-standardized-pearson-residuals",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-standardized-pearson-residuals",
    "title": "Lesson 14: Model Diagnostics",
    "section": "GLOW study: standardized Pearson residuals",
    "text": "GLOW study: standardized Pearson residuals\n\n\n\nGenerally, the points that curve from top left to bottom right of plot correspond to covariate patterns with \\(y_j = 1\\)\n\nOpposite corresponds to \\(y_j = 0\\)\n\n\n\n\n\nTo make the plot\nggplot(dx_glow) + geom_point(aes(x=P, y=dChisq), size = 3) + \n  xlab(\"Estimated/Predicted Probability of Fracture\") +\n  ylab(\"Change in Std. Pearson Residual\") +\n  theme(text = element_text(size = 26))"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-standardized-pearson-residuals-1",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-standardized-pearson-residuals-1",
    "title": "Lesson 14: Model Diagnostics",
    "section": "GLOW study: standardized Pearson residuals",
    "text": "GLOW study: standardized Pearson residuals\n\n\n\nGenerally, the points that curve from top left to bottom right of plot correspond to covariate patterns with \\(y_j = 1\\)\n\nOpposite corresponds to \\(y_j = 0\\)\n\nPoints in the top left or top right corners identify the covariate patterns that are poorly fit\nWe may use 4 as a crude approximation to the upper 95th percentile for \\(\\Delta X_j^2\\)\n\n95th percentile of chi-squared distribution is 3.84\n\n\n\n\n\nTo make the plot\nggplot(dx_glow) + geom_point(aes(x=P, y=dChisq), size = 3) + \n  xlab(\"Estimated/Predicted Probability of Fracture\") +\n  ylab(\"Change in Std. Pearson Residual\") +\n  theme(text = element_text(size = 26))"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-standardized-pearson-residuals-2",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-standardized-pearson-residuals-2",
    "title": "Lesson 14: Model Diagnostics",
    "section": "GLOW study: standardized Pearson residuals",
    "text": "GLOW study: standardized Pearson residuals\n\n\n\nGenerally, the points that curve from top left to bottom right of plot correspond to covariate patterns with \\(y_j = 1\\)\n\nOpposite corresponds to \\(y_j = 0\\)\n\nPoints in the top left or top right corners identify the covariate patterns that are poorly fit\nWe may use 4 as a crude approximation to the upper 95th percentile for \\(\\Delta X_j^2\\)\n\n95th percentile of chi-squared distribution is 3.84\n\nWhich point is over 4?\n\n\ndx_glow %&gt;% filter(dChisq &gt; 4) %&gt;% select(priorfracYes, age_c, P, dChisq)\n\n   priorfracYes age_c         P   dChisq\n          &lt;num&gt; &lt;num&gt;     &lt;num&gt;    &lt;num&gt;\n1:            0    -4 0.1643855 4.413937\n\n\n\n\n\nTo make the plot\nggplot(dx_glow) + geom_point(aes(x=P, y=dChisq), size = 3) + \n  xlab(\"Estimated/Predicted Probability of Fracture\") +\n  ylab(\"Change in Std. Pearson Residual\") +\n  theme(text = element_text(size = 26))"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-standardized-deviance-residuals",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-standardized-deviance-residuals",
    "title": "Lesson 14: Model Diagnostics",
    "section": "GLOW study: standardized Deviance residuals",
    "text": "GLOW study: standardized Deviance residuals\n\n\n\nSame investigation as Pearson residuals\nPoints in the top left or top right corners identify the covariate patterns that are poorly fit\nUse 4 as a crude approximation to the upper 95th percentile\nWhich point is over 4?\n\n\ndx_glow %&gt;% filter(dDev &gt; 4) %&gt;% \n  select(priorfracYes, age_c, P, dDev)\n\n   priorfracYes age_c         P     dDev\n          &lt;num&gt; &lt;num&gt;     &lt;num&gt;    &lt;num&gt;\n1:            0   -10 0.1190935 4.841217\n2:            0     7 0.2812460 5.313540\n3:            1     6 0.4150524 4.325664\n\n\n\n\n\nTo make the plot\nggplot(dx_glow) + geom_point(aes(x=P, y=dDev), size = 3) + \n  xlab(\"Estimated/Predicted Probability of Fracture\") +\n  ylab(\"Change in Std. Deviance Residual\") +\n  theme(text = element_text(size = 26))"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-change-in-coefficient-estimates",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-change-in-coefficient-estimates",
    "title": "Lesson 14: Model Diagnostics",
    "section": "GLOW Study: Change in coefficient estimates",
    "text": "GLOW Study: Change in coefficient estimates\n\n\n\nBook recommends flagging certain covariate patterns if change in coefficient estimates are greater than 1\nAll values of \\(\\Delta\\widehat{\\beta}_j\\) are below 0.09\n\n\ndx_glow %&gt;% filter(dBhat &gt; 0.075) %&gt;% \n  select(priorfracYes, age_c, P, dBhat)\n\n   priorfracYes age_c         P      dBhat\n          &lt;num&gt; &lt;num&gt;     &lt;num&gt;      &lt;num&gt;\n1:            1    20 0.4325984 0.08926472\n\n\n\n\n\nTo make the plot\nggplot(dx_glow) + geom_point(aes(x=P, y=dBhat), size = 3) + \n  xlab(\"Estimated/Predicted Probability of Fracture\") +\n  ylab(\"Change in Coefficient Estimates\") +\n  theme(text = element_text(size = 26))"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-leverage",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-leverage",
    "title": "Lesson 14: Model Diagnostics",
    "section": "GLOW Study: Leverage",
    "text": "GLOW Study: Leverage\n\n\n\nWe can use the same rule as linear regression: \\(h_j &gt; 3p/n\\)\n\nFlag these points as high leverage\n\nPoints with high leverage\n\n\\(p=4\\): four regression coefficients\n\\(n=500\\): 500 total observations\nLook for \\(h_j &gt; 3p/n = 3\\cdot4 /500 = 0.024\\)\n\n\n\ndx_glow %&gt;% filter(h &gt; 3*4/500) %&gt;% \n  select(priorfracYes, age_c, P, h) %&gt;% \n  head()\n\n   priorfracYes age_c         P          h\n          &lt;num&gt; &lt;num&gt;     &lt;num&gt;      &lt;num&gt;\n1:            0    20 0.4686423 0.02688958\n2:            1   -12 0.3928116 0.03186122\n3:            0    19 0.4531105 0.02451738\n4:            1   -11 0.3940365 0.02900675\n5:            1    19 0.4313389 0.02895824\n6:            1    18 0.4300804 0.02621708\n\n\n\n\n\nTo make the plot\nggplot(dx_glow) + geom_point(aes(x=P, y=h), size=3) + \n  xlab(\"Estimated/Predicted Probability of Fracture\") +\n  ylab(\"Leverage\") +\n  theme(text = element_text(size = 26))"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#find-out-the-influential-observation-from-the-data-set",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#find-out-the-influential-observation-from-the-data-set",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Find Out the “Influential” Observation From the Data Set",
    "text": "Find Out the “Influential” Observation From the Data Set\n\n\n\nWe identified covariate patterns that may be poorly fit or influential\n\n \n\nLet’s identify the covariate patterns that were not fit well\n\n\n\ndx_glow %&gt;% mutate(Cov_patt = 1:nrow(.)) %&gt;%\n  filter(dChisq &gt; 4 | dDev &gt; 4 | dBhat &gt; 1 | \n          h &gt; 3*4/500) %&gt;%\n  select(Cov_patt, y, P, h, dChisq, dDev, dBhat, h) %&gt;%\n  round(., 3)\n\n    Cov_patt     y     P     h dChisq  dDev dBhat\n       &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;  &lt;num&gt; &lt;num&gt; &lt;num&gt;\n 1:        6     1 0.469 0.027  0.008 0.008 0.000\n 2:       22     1 0.393 0.032  0.046 0.047 0.002\n 3:       36     1 0.453 0.025  0.178 0.183 0.004\n 4:       43     0 0.119 0.005  2.581 4.841 0.012\n 5:       45     6 0.164 0.003  4.414 3.554 0.014\n 6:       47     0 0.281 0.006  3.148 5.314 0.018\n 7:       48     0 0.394 0.029  0.670 1.032 0.020\n 8:       49     2 0.431 0.029  0.698 0.693 0.021\n 9:       50     0 0.430 0.026  0.775 1.155 0.021\n10:       53     0 0.415 0.008  2.862 4.326 0.024\n11:       57     2 0.395 0.026  0.949 0.924 0.026\n12:       63     0 0.484 0.029  0.967 1.364 0.029\n13:       69     0 0.434 0.035  1.588 2.358 0.058\n14:       70     1 0.392 0.035  1.610 1.943 0.058\n15:       71     2 0.433 0.032  2.710 3.462 0.089"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#after-identifying-points",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#after-identifying-points",
    "title": "Lesson 14: Model Diagnostics",
    "section": "After identifying points",
    "text": "After identifying points\n\nDo a data quality check\n\nUnless you have a very good reason to believe the data are not measured correctly, then we leave it in\nCommon to do nothing\n\n\n \n\nIf only a few covariate pattern does not fit well (\\(y_j\\) differs from \\(m_j\\widehat\\pi_j\\) ), we are not too worried\n\nWe had 15 out of 71 covariate patterns\n\n\n \n\nIf quite a few covariate patterns do not fit well, potential reasons can be considered:\n\nThe link used in logistic regression model is not appropriate for outcome\n\nThis is usually unlikely, since logistic regression model is very flexible (think back to why we transformed our outcome from binary form)\n\nOne or more important covariates missing in the model\n\nAt least one of the covariates in the model has been entered in the wrong scale (think age-squared vs. age)"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#how-would-i-report-this-combining-all-model-assessment",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#how-would-i-report-this-combining-all-model-assessment",
    "title": "Lesson 14: Model Diagnostics",
    "section": "How would I report this? (Combining all model assessment)",
    "text": "How would I report this? (Combining all model assessment)\n\nAssuming I have not checked other final models (no other models to compare AIC/BIC or AUC with)\n\nMethods: To assess the overall model fit, we calculated the AUC-ROC. We also calculated several model diagnostics including standardized Pearson residual, standardized deviance, change in coefficient estimates, and leverage. We identified covariate patterns with high standardized Pearson residual (greater than 4), standardized deviance (greater than 4), change in coefficient estimates (greater than 1), and leverage (greater than 0.024).\n \nResults: Our final logistic regression model consisted of the outcome, fracture, and predictors including prior fracture, age, and their interaction. The AUC-ROC was 0.68. We identified 11 covariate patterns with high leverage and 4 with high standardized Pearson residual, standardized deviance, or change in coefficient estimates. No identified observations were omitted.\n \nDiscussion:\n\nAUC-ROC low: Included covariates were pre-determined\nInfluential points were kept in because all observations were within feasible range of the predictors and outcome. (we could try age-sqaured and see if that helps AUC and/or diagnostics)"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#last-class-glow-study-with-interactions",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#last-class-glow-study-with-interactions",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Last Class: GLOW Study with interactions",
    "text": "Last Class: GLOW Study with interactions\n\nOutcome variable: any fracture in the first year of follow up (FRACTURE: 0 or 1)\nRisk factor/variable of interest: history of prior fracture (PRIORFRAC: 0 or 1)\nPotential confounder or effect modifier: age (AGE, a continuous variable)\nFitted model with interactions: \\[\\begin{aligned} \\text{logit}\\left(\\widehat\\pi(\\mathbf{X})\\right) & = \\widehat\\beta_0 &+ &\\widehat\\beta_1\\cdot I(\\text{PF}) & + &\\widehat\\beta_2\\cdot Age& + &\\widehat\\beta_3 \\cdot I(\\text{PF}) \\cdot Age \\\\  \n\\text{logit}\\left(\\widehat\\pi(\\mathbf{X})\\right) & = -1.376 &+ &1.002\\cdot I(\\text{PF})& + &0.063\\cdot Age& -&0.057 \\cdot I(\\text{PF}) \\cdot Age\n\\end{aligned}\\]\n\n \n\nToday: determine the overall fit of this model"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#last-class-reporting-results-of-glow-study-with-interactions",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#last-class-reporting-results-of-glow-study-with-interactions",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Last Class: Reporting results of GLOW Study with interactions",
    "text": "Last Class: Reporting results of GLOW Study with interactions\n\nRemember our main covariate is prior fracture, so we want to focuse on how age changes the relationship between prior fracture and a new fracture!\n\n\n\nFor individuals 69 years old, the estimated odds of a new fracture for individuals with prior fracture is 2.72 times the estimated odds of a new fracture for individuals with no prior fracture (95% CI: 1.70, 4.35). As seen in Figure 1 (a), the odds ratio of a new fracture when comparing prior fracture status decreases with age, indicating that the effect of prior fractures on new fractures decreases as individuals get older. In Figure 1 (b), it is evident that for both prior fracture statuses, the predict probability of a new fracture increases as age increases. However, the predicted probability of new fracture for those without a prior fracture increases at a higher rate than that of individuals with a prior fracture. Thus, the predicted probabilities of a new fracture converge at age [insert age here].\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Odds ratio of fracture outcome comparing prior fracture to no prior fracture\n\n\n\n\n\n\n\n\n\n\n\n(b) Predicted probability of fracture\n\n\n\n\n\n\n\nFigure 1: Plots of odds ratio and predicted probability from fitted interaction model"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#overview-12",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#overview-12",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Overview (1/2)",
    "text": "Overview (1/2)\n\nOnce a potential final model has been determined, we need to assess the fit of the model\n\n \n\nVariable selection is no longer our focus at this stage\n\nWe want to find answer to whether the model fits the data adequately\n\n\n \n\nAssessing the Goodness of Fit or Assessing model fit\n\nAssess how well our fitted logistic regression model predicts/estimates the observed outcomes\nComparison: fitted/estimated outcome vs. observed outcome"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#some-good-measurements-for-our-final-models",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#some-good-measurements-for-our-final-models",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Some good measurements for our final model(s)",
    "text": "Some good measurements for our final model(s)\n\nPearson residual statistic\n\n \n\nHosmer-Lemeshaw goodness-of-fit statistic\n\n \n\nAUC-ROC (area under the curve of the receiver operating characteristic)\n\n \n\nAIC/BIC"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#overview-22",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#overview-22",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Overview (2/2)",
    "text": "Overview (2/2)\n\nTo assess the fit of the model, it is good to have a mixture of measurements\n\n \n\nWe want to measure the absolute fit: not comparing to any models, but determining if the model fits the data well\n\nPearson residual statistic\nHosmer-Lemeshaw goodness-of-fit statistic\nAUC-ROC (kind of, often do not use a hypothesis test but you can!)\n\n\n \n\nWe want comparable measures of fit: if we have candidate models that are not nested\n\nAUC-ROC\nAIC/BIC"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#poll-everywhere-question-1",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#poll-everywhere-question-1",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#components-to-assess-model-fit",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#components-to-assess-model-fit",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Components to Assess Model Fit",
    "text": "Components to Assess Model Fit\n\nThe model fits the data well if\n\nSummary measures of the distance between the predicted/estimated/fitted and observed Y are small\n\nToday’s lecture!!\n\nThe contribution of each pair (predicted and observed) to these summary measures is unsystematic and is small relative to the error structure of the model\n\nModel Diagnostics that will be covered in another lecture!\n\n\n\n \n\nNeed both components\n\nIt is possible to see a “good” summary measure of the distance between predicted and observed Y with some substantial deviation from fit for a few subjects"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#summary-measures-of-goodness-of-fit",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#summary-measures-of-goodness-of-fit",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Summary Measures of Goodness of Fit",
    "text": "Summary Measures of Goodness of Fit\n\nAka overall measure of fit\n\n \n\nWhat do we need to calculate them?\n\nNeed to define what the fitted outcome is\nNeed to calculate how close the fitted outcome is to the observed outcome\nSummarize across all observations (or individuals’ data)\n\n\n \n\nTwo tests of goodness-of-fit\n\nPearson residual statistic\nHosmer-Lemeshaw goodness-of-fit statistic"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#comparing-fitted-outcome-to-observed-outcome",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#comparing-fitted-outcome-to-observed-outcome",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Comparing fitted outcome to observed outcome",
    "text": "Comparing fitted outcome to observed outcome\n\nIn logistic regression model, we estimate \\(\\pi(\\mathbf{X}) = P(Y=1|\\mathbf{X})\\)\n\nPredicted value, \\(\\widehat\\pi(\\mathbf{X})\\), is between 0 and 1 for each subject\n\nHowever, we always observe \\(Y=1\\) or \\(Y=0\\)\n\nNot an observed \\(\\pi(\\mathbf{X})\\)\n\n\n \n\nWe can deterimine the fitted outcome by sampling Y’s from a Bernoulli distribution with the fitted probability\n\n\\(\\widehat{Y} \\sim \\text{Bernoulli}(\\widehat\\pi(\\mathbf{X}))\\)\n\nIf there are groups of individuals that share the same covariate observations, then we can use the same \\(\\widehat\\pi(\\mathbf{X})\\)\n\n\\(\\sum_j \\widehat{Y} \\sim \\text{Binomial}(\\sum_j, \\widehat\\pi(\\mathbf{X}))\\)\n\n\n \n\nInstead of comparing the expected vs. observed at individual level, we can compare them at “group” level"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#number-of-covariate-patterns",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#number-of-covariate-patterns",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Number of Covariate Patterns",
    "text": "Number of Covariate Patterns\n\nWhen the logistic regression model contains only categorical covariates, we can think of the number of covariate patterns\nFor example: model contains two binary covariates (history of fracture and smoking status), there will be 4 unique combination of these factors\n\nThis model has 4 covariate patterns\nSubjects can be divided into 4 groups based on the covariates’ values\n\nWe can then compute the predicted number of individuals with Y=1 in each group, and compare that with the actual observed number of individuals with Y=1 in that group\n\nWe don’t need to sample this\nWe use the expected value (mean) of the Binomial to determine the \\(\\widehat{Y}\\) for each covariate pattern\nFor covariate pattern \\(j\\) with \\(m_j\\) observations: \\[\\widehat{Y}_j = m_j \\widehat\\pi(\\mathbf{X_j}) = m_j{\\hat{\\pi}}_j\\]"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#pearson-residual",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#pearson-residual",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Pearson Residual",
    "text": "Pearson Residual\n\nIn logistic regression model, can use Pearson residual for summary measure of goodness-of-fit Uses the \\(\\widehat{Y}_j\\) fitted value from previous slide\nPearson residual for jth covariate pattern is: \\[r\\left(Y_j,{\\hat{\\pi}}_j\\right)=\\frac{(Y_j-m_j{\\hat{\\pi}}_j)}{\\sqrt{m_j{\\hat{\\pi}}_j(1-{\\hat{\\pi}}_j)}}=\\frac{(Y_j-{\\hat{Y}}_i)}{\\sqrt{{\\hat{Y}}_i(1-{\\hat{\\pi}}_j)}}\\]\nThe summary statistics of Pearson residual is thus: \\[X^2=\\sum_{j=1}^{J}{r\\left(Y_j,{\\hat{\\pi}}_j\\right)^2}\\]"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#pearson-residual-procedure",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#pearson-residual-procedure",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Pearson Residual procedure",
    "text": "Pearson Residual procedure\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses: same for all data\n\n\\(H_0\\): model fits well\n\\(H_1\\): model does not fits well\n\nCalculate the test statistic and p-value\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#not-going-to-bother-going-through-an-example",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#not-going-to-bother-going-through-an-example",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Not going to bother going through an example",
    "text": "Not going to bother going through an example\n\nWe can calculate this by hand and test against a chi-squared distribution\n\n \n\nNo set R code to do this\n\n \n\nI do not see this as the main way to determine goodness of fit… for a binary outcome!\n\nOften because of the bigger issues with it…"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#issues-with-pearson-residuals",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#issues-with-pearson-residuals",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Issues with Pearson Residuals",
    "text": "Issues with Pearson Residuals\n\nAssume current model has p covariates…\n\nthen \\(X^2\\) (Pearson residual) follows a chi-squared distribution\n\nunder the null hypothesis based on large sample theory\n\nOnly appropriate if the number of covariate patterns is less than the number of observations\n\n\n \n\nWhen the logistic regression model contains one or more continuous covariates, it is likely that the number of covariate patterns equals to the sample size n\n\n \n\nWe should not use Pearson Residuals to evaluate goodness-of-fit test when the fitted model contains one or more continuous variables"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#hosmer-lemeshow-test",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#hosmer-lemeshow-test",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Hosmer-Lemeshow test",
    "text": "Hosmer-Lemeshow test\n\nIf number of covariate patterns is roughly same as the number of observations\n\nWhenever you include a continuous variable in your model\nHosmer-Lemeshow (HL) goodness-of-fit test should be used instead\n\n\n \n\nHowever, HL test does not work well if the number of covariate patterns is small\n\nHL test should not be used if the number of covariate patterns ≤ 6\n\nFor reference: 3 binary predictors makes 8 covariate patterns\n\nPearson residuals \\(X^2\\) should be used when the number of covariate patterns is small\n\n\n \n\nA large p-value from HL test suggests the model fits well"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#poll-everwhere-question-2",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#poll-everwhere-question-2",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Poll Everwhere question 2",
    "text": "Poll Everwhere question 2"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#hosmer-lemeshow-test-1",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#hosmer-lemeshow-test-1",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Hosmer-Lemeshow test",
    "text": "Hosmer-Lemeshow test\n\nHL test uses groupings from percentiles to basically measure what Pearson residual measures\n\n \n\nSteps to compute HL test statistic:\n\nCompute estimated probability \\(\\widehat\\pi(\\mathbf{X}))\\) for all n subjects (\\(n=1, 2, ..., n\\))\nOrder \\(\\widehat\\pi(\\mathbf{X}))\\) from largest to smallest values\nDivide ordered values into g percentile grouping (usually \\(g = 10\\) based on H-L’s suggestion)\nForm table of observed and expected counts\nCalculate HL test statistic from table\nCompare HL test statistic to chi-squared distribution (\\(\\chi^2_{g-2}\\))"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#hosmer-lemeshow-test-statistic",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#hosmer-lemeshow-test-statistic",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Hosmer-Lemeshow test statistic",
    "text": "Hosmer-Lemeshow test statistic\n\nThe test statistic of Hosmer-Lemeshow goodness-of-fit test is denoted by \\(\\widehat{C}\\), which is obtained by calculating the Pearson chi-squared statistic from the \\(g \\times 2\\) table of observed and estimated expected frequencies \\[\\hat{C}=\\sum_{k=1}^{g}\\frac{\\left(o_k-n_k^\\prime{\\bar{\\pi}}_k\\right)^2}{n_k^\\prime{\\bar{\\pi}}_k(1-{\\bar{\\pi}}_k)}\\]\n\nwhere \\(n'_k\\) is the total number of subjects in the \\(k\\)th group\n\nLet \\(c_k\\) be the number of covariate patterns in the \\(k\\)th decile: \\[o_k=\\sum_{j=1}^{c_k}y_j\\] and \\[{\\bar{\\pi}}_k=\\sum_{j=1}^{c_k}\\frac{m_j{\\hat{\\pi}}_j}{n_k^\\prime}\\]"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#hosmer-lemeshow-test-procedure",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#hosmer-lemeshow-test-procedure",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Hosmer-Lemeshow test procedure",
    "text": "Hosmer-Lemeshow test procedure\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses: same for all data\n\n\\(H_0\\): model fits well\n\\(H_1\\): model does not fits well\n\nCalculate the test statistic and p-value\n\nNote: \\(\\widehat{C} \\sim \\chi^2_{df=g-2}\\)\n\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#glow-study-hosmer-lemeshow-test",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#glow-study-hosmer-lemeshow-test",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "GLOW Study: Hosmer-Lemeshow test",
    "text": "GLOW Study: Hosmer-Lemeshow test\n\nOkay, so let’s look at the interaction model from last class \\[\\text{logit}\\left(\\pi(\\mathbf{X})\\right) = \\beta_0 + \\beta_1\\cdot I(\\text{PF}) +\\beta_2\\cdot Age + \\beta_3 \\cdot I(\\text{PF}) \\cdot Age\\]\nWe need to fit the model and use a new command:\n\n\nglow_m3 = glm(fracture ~ priorfrac + age_c + priorfrac*age_c, \n              data = glow, family = binomial)\nlibrary(ResourceSelection)\nobs_vals = as.numeric(glow$fracture) -1\nfit_vals = fitted(glow_m3)\nhoslem.test(obs_vals, fit_vals, g = 10)\n\n\n    Hosmer and Lemeshow goodness of fit (GOF) test\n\ndata:  obs_vals, fit_vals\nX-squared = 6.778, df = 8, p-value = 0.5608\n\n\nNote to Nicky: do NOT make conclusion yet! In the poll everywhere!"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#poll-everywhere-question-3",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#poll-everywhere-question-3",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Poll Everywhere question 3",
    "text": "Poll Everywhere question 3"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#glow-study-hosmer-lemeshow-test-1",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#glow-study-hosmer-lemeshow-test-1",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "GLOW Study: Hosmer-Lemeshow test",
    "text": "GLOW Study: Hosmer-Lemeshow test\n\nConclusion: The p-value is 0.5608, so we fail to reject the null hypothesis that the model fits the data well. Thus, the preliminary final model for the GLOW dataset fits the data well\n\n \n\nDon’t forget that we still need to check individual observations (Model Diagnostics!)\n\n \n\nR may give results for the HL test even if it is not appropriate to use it!\n\nIf number of covariate patterns ≤ 6, do not use HL test"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#big-data-issue-in-goodness-of-fit-test",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#big-data-issue-in-goodness-of-fit-test",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Big Data Issue in Goodness-of-fit Test",
    "text": "Big Data Issue in Goodness-of-fit Test\n\nWhen the sample size is really big (&gt; 1000), it is much more likely to find the H-L reject the model fit (even when the expected vs. observed in each decile categories looks pretty similar)\n\n \n\nThis is due to “too much” power in hypothesis testing.\n\nPaul et al. (2012) for samples sizes from 1000 to 25,000, the number of groups g should be equal to \\[g=\\max{\\left(10,\\min{\\left\\{\\frac{n_1}{2},\\ \\frac{n-n_1}{2},\\ 2+8\\left(\\frac{n}{1000}\\right)^2\\right\\}}\\right)}\\]\n\n\n \n\nFor example, if one has a sample with \\(n=10, 000\\) (sample size) and \\(n_1=1,000\\) (number of events) then \\(g=500\\) groups are suggested\nFor n &gt; 25000, other methods, such as partitioning data into a developmental data set (with smaller n) and a validation set"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#final-notes-on-goodness-of-fit-test",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#final-notes-on-goodness-of-fit-test",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Final Notes on Goodness-of-fit Test",
    "text": "Final Notes on Goodness-of-fit Test\n\nThey should not be used for variable selection\n\nThe likelihood ratio tests for significance of coefficients are much more powerful and appropriate (when nested)\n\n\n \n\nThey are not for model comparison\n\nOne should not use the p-value from goodness of fit tests of different models to decide which model is better than the other\nSomething like AUC-ROC, AIC, or BIC can be used"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#roc-curve-and-auc-12",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#roc-curve-and-auc-12",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "ROC Curve and AUC (1/2)",
    "text": "ROC Curve and AUC (1/2)\n\n\n\nReceiver Operating Characteristics (ROC) curve is useful tool to quantify how good is our model predicting binary outcome\n\n \n\nIt is a plot of sensitivity (true positive rate) versus (1-specificity) or false positive rate of fitted binary values\n\nTrue Positive Rate \\(= \\dfrac{TP}{TP + FN}\\)\nFalse Positive Rate \\(=  \\dfrac{FP}{FP + TN}\\)\n\n\n \n\nThe ROC curve shows the tradeoff between sensitivity and specificity"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#roc-curve-and-auc-22",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#roc-curve-and-auc-22",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "ROC Curve and AUC (2/2)",
    "text": "ROC Curve and AUC (2/2)\n\n\n\nArea under the ROC curve (AUC ROC) is a reasonable summary of the overall predictive accuracy of the test\n\nAccuracy means how well the predicted value matches the observed value\n\n\n \n\nThe closer the curve follows the left-hand border and top border of the ROC space, the more accurate the test\n\nAn AUC =1 represents 100% accuracy\n\n\n \n\nThe closer the curve comes to the 45-degree diagonal line, the less accurate the test\n\nAn AUC = 0.5 represents an unhelpful model\n\nRandom predictions"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#poll-everywhere-question-4",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#poll-everywhere-question-4",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#roc-curve-and-auc-33",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#roc-curve-and-auc-33",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "ROC Curve and AUC (3/3)",
    "text": "ROC Curve and AUC (3/3)\n\nOften only report the AUC\n\n \n\nSuggestions of how to interpret model fit through AUC values:"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#glow-study-roc-of-interaction-model",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#glow-study-roc-of-interaction-model",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "GLOW Study: ROC of interaction model",
    "text": "GLOW Study: ROC of interaction model\n\n\n\nlibrary(pROC)\npredicted &lt;- predict(glow_m3, glow, type=\"response\")\n\n# define object to plot and calculate AUC\nrocobj &lt;- roc(glow$fracture, predicted)\nauc &lt;- round(auc(glow$fracture, predicted),4)\n\n#create ROC plot\nggroc(rocobj, colour = 'steelblue', \n      size = 2, legacy.axes = TRUE) +\n  ggtitle(paste0('ROC Curve ','(AUC = ',auc,')')) +\n  theme(text = element_text(size = 23)) +\n  xlab(\"False Positive Rate (1 - Specificity)\") +\n  ylab(\"True Positive Rate (Sensitivity)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe have a poorly fitting model\nWe can take auc and compare it to other models: good way to pick a model based on predictive power"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#another-way-to-think-about-auc",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#another-way-to-think-about-auc",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Another way to think about AUC",
    "text": "Another way to think about AUC\n\nGLOW Study: Consider the situation in which the fracture status of each individual is known\n\n \n\nRandomly pick one individual from fractured group and one from non-fractured outcome group\n\nBased on their age, height, prior fracture, and all other covariates, we will correctly predict which is from fractured group\n\n\n \n\nThe AUC is the percentage of randomly drawn pairs for which we predict the pair correctly\n\n \n\nTherefore, AUC represents the ability of our covariates to discriminate between individuals with the outcome (fracture) and those without the outcome"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#aic-and-bic",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#aic-and-bic",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "AIC and BIC",
    "text": "AIC and BIC\n\nTwo widely used non-hypothesis testing based measurements that helps select a good model\n\nAkaike Information Criterion (AIC)\nBayesian Information Criterion (BIC)\n\n\n \n\nUnlike likelihood ratio test which is only suitable for nested model, AIC and BIC are suitable for both nested and non-nested model\n\n \n\nThere is no hypothesis/conclusion testing for the comparison between two models\n\nSo not the best for selecting covariates to include in model\nBUT helpful if you have a few preliminary final models that you want to compare"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#poll-everywhere-question-5",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#poll-everywhere-question-5",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Poll Everywhere Question 5",
    "text": "Poll Everywhere Question 5"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#aic-and-bic-1",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#aic-and-bic-1",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "AIC and BIC",
    "text": "AIC and BIC\n\nBoth AIC and BIC penalize a model for having many parameters\n\n \n\n\n\n\n\n\n\n\nMeasure of fit\nEquation\nR code\n\n\n\n\nAkaike information criterion (AIC)\n\\(AIC = -2 \\cdot \\text{log-likelihood} + 2q\\)\nAIC(model_name)\n\n\nBayesian information criterion (BIC)\n\\(AIC = -2 \\cdot \\text{log-likelihood} + q\\text{log}(n)\\)\nBIC(model_name)\n\n\n\n \n\nWhere q is the number of parameters in the model and n is the sample size\nBoth AIC and BIC can only be used to compare models fitting the same data set\nIn comparing two models, the model with smaller AIC and/or BIC is preferred\n\nWhen the difference in AIC between two models exceeds 3, the difference is viewed as “meaningful”"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#aic-and-bic-in-r",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#aic-and-bic-in-r",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "AIC and BIC in R",
    "text": "AIC and BIC in R\n\nAfter fitting the logistic regression model, can calculate AIC and BIC\nLet’s look at the AIC and BIC of our interaction model:\n\n\nAIC(glow_m3)\n\n[1] 531.2716\n\nBIC(glow_m3)\n\n[1] 548.13"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#summary-12",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#summary-12",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Summary (1/2)",
    "text": "Summary (1/2)\n\n\n\n\n\n\n\n\n\nMeasure of fit\nHypothesis tested?\nEquation\nR code\n\n\n\n\nPearson residual\nYes\n\\(X^2=\\sum_{j=1}^{J}{r\\left(Y_j,{\\hat{\\pi}}_j\\right)^2}\\)\nNot given\n\n\nHosmer-Lemeshow test\nYes\n\\(\\hat{C}=\\sum_{k=1}^{g}\\frac{\\left(o_k-n_k^\\prime{\\bar{\\pi}}_k\\right)^2}{n_k^\\prime{\\bar{\\pi}}_k(1-{\\bar{\\pi}}_k)}\\)\nhoslem.test()\n\n\nAUC-ROC\nKinda\nNot given\nauc(observed, predicted)\n\n\nAIC\nOnly to compare models\n\\(AIC = -2 \\cdot \\text{log-likelihood} + 2q\\)\nAIC(model_name)\n\n\nBIC\nOnly to compare models\n\\(AIC = -2 \\cdot \\text{log-likelihood} + q\\text{log}(n)\\)\nBIC(model_name)\n\n\n\nSpecial notes:\n\nUse Hosmer-Lemshow test over Pearson residual unless number of covariate patterns is less than 6\nCannot use Pearson residual when there is a continuous variable in the model"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#summary-22",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#summary-22",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Summary (2/2)",
    "text": "Summary (2/2)\n\nFor our interaction model: \\[\\begin{aligned} \\text{logit}\\left(\\widehat\\pi(\\mathbf{X})\\right) & = \\widehat\\beta_0 &+ &\\widehat\\beta_1\\cdot I(\\text{PF}) & + &\\widehat\\beta_2\\cdot Age& + &\\widehat\\beta_3 \\cdot I(\\text{PF}) \\cdot Age \\\\  \n\\text{logit}\\left(\\widehat\\pi(\\mathbf{X})\\right) & = -1.376 &+ &1.002\\cdot I(\\text{PF})& + &0.063\\cdot Age& -&0.057 \\cdot I(\\text{PF}) \\cdot Age\n\\end{aligned}\\]\nWe can examine the overall model fit using:\n\nNot comparing to any other models:\n\nPearson residual: Not appropriate for this model\nHosmer-Lemeshow: \\(\\hat{C}=6.778\\), p-value = 0.56\nAUC-ROC: 0.6819\n\nCan be used to compare to other models:\n\nAUC-ROC: 0.6819\nAIC: 531.27\nBIC: 548.13"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#three-numerical-problems",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#three-numerical-problems",
    "title": "Lesson 13: Numerical Problems",
    "section": "Three Numerical Problems",
    "text": "Three Numerical Problems\n\nIssues that may cause numerical problems:\n \n\nZero cell count\n\n \n\nComplete separation\n\n \n\nMulticollinearity\n\n\n \n\nAll may cause large estimated coefficients and/or large estimated standard errors"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-count-in-a-contingency-table",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-count-in-a-contingency-table",
    "title": "Lesson 13: Numerical Problems",
    "section": "Zero cell count in a contingency table",
    "text": "Zero cell count in a contingency table\n\nIf no observations at any intersection of the covariate and outcome\n\n \n\nZero cell in a contingency table should be detected in descriptive statistical analysis stage\n\n \n\nExample of one covariate with outcome:"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-count-example-13",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-count-example-13",
    "title": "Lesson 13: Numerical Problems",
    "section": "Zero cell count: example (1/3)",
    "text": "Zero cell count: example (1/3)\n\n\n\nExample of logistic regression with one covariate:\n\n\nex1_m = glm(outcome ~ x, data = ex1, \n              family = binomial)"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-count-example-23",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-count-example-23",
    "title": "Lesson 13: Numerical Problems",
    "section": "Zero cell count: example (2/3)",
    "text": "Zero cell count: example (2/3)\n\n\n\nExample of logistic regression with one covariate:\n\n\nex1_m = glm(outcome ~ x, data = ex1, \n              family = binomial())\n\n\n\n\n\n\n\n\nCoefficient estimates\ntidy(ex1_m, conf.int=T) %&gt;% gt() %&gt;% \n  tab_options(table.font.size = 32) %&gt;%\n  fmt_number(decimals = 2)\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n−0.62\n0.47\n−1.32\n0.19\n−1.60\n0.27\n\n\nxTwo\n1.02\n0.65\n1.57\n0.12\n−0.23\n2.35\n\n\nxThree\n20.19\n2,404.67\n0.01\n0.99\n−119.00\nNA\n\n\n\n\n\n\n\n\n\n\nOdds ratio\ntbl_regression(ex1_m, exponentiate=T) %&gt;%\n  as_gt() %&gt;% # allows us to use tab_options()\n  tab_options(table.font.size = 38)\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\n\nx\n\n\n\n\n\n\n\n\n    One\n—\n—\n\n\n\n\n    Two\n2.79\n0.79, 10.5\n0.12\n\n\n    Three\n583,822,601\n0.00,\n\n&gt;0.9\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-count-example-33",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-count-example-33",
    "title": "Lesson 13: Numerical Problems",
    "section": "Zero cell count: example (3/3)",
    "text": "Zero cell count: example (3/3)\n\n\n\nExample of logistic regression with one covariate:\n\n\nex1_m = glm(outcome ~ x, data = ex1, \n              family = binomial())\n\n\n\n\n\n\n\n\nCoefficient estimates\ntidy(ex1_m, conf.int=T) %&gt;% gt() %&gt;% \n  tab_options(table.font.size = 32) %&gt;%\n  fmt_number(decimals = 2)\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n−0.62\n0.47\n−1.32\n0.19\n−1.60\n0.27\n\n\nxTwo\n1.02\n0.65\n1.57\n0.12\n−0.23\n2.35\n\n\nxThree\n20.19\n2,404.67\n0.01\n0.99\n−119.00\nNA\n\n\n\n\n\n\n\n \n\nCoefficient estimate is large and standard error is large! Estimated odds ratio is very large and confidence interval cannot be computed.\n\n\n\n\nOdds ratio\ntbl_regression(ex1_m, exponentiate=T) %&gt;%\n  as_gt() %&gt;% # allows us to use tab_options()\n  tab_options(table.font.size = 38)\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\n\nx\n\n\n\n\n\n\n\n\n    One\n—\n—\n\n\n\n\n    Two\n2.79\n0.79, 10.5\n0.12\n\n\n    Three\n583,822,601\n0.00,\n\n&gt;0.9\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#ways-to-address-zero-cell",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#ways-to-address-zero-cell",
    "title": "Lesson 13: Numerical Problems",
    "section": "Ways to address zero cell",
    "text": "Ways to address zero cell\n\nAdd one-half to each of the cell counts\n\nTechnically works, but not the best option\nRarely useful with a more complex analysis: may work for simple logistic regression\nNicky would say worst option because manipulating the data that does not work on individual level\n\nCollapse the categories to remove the 0 cells\n\nWe could collapse groups 2 and 3 together if it makes clinical sense\nGood idea if this makes clinical sense OR there is no difference between groups\n\nRemove the category with 0 cells\n\nThis would mean we reduce the total sample size as well\nNot a good idea: we would remove people from our dataset. Why would we do that?\n\nIf the variable is in ordinal scale, treat it as continuous\n\nGood idea if you have seen evidence that there is a linear trend on log-odds scale"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#decide-on-how-to-address-zero-cell-12",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#decide-on-how-to-address-zero-cell-12",
    "title": "Lesson 13: Numerical Problems",
    "section": "Decide on how to address zero cell (1/2)",
    "text": "Decide on how to address zero cell (1/2)\n\nLook at the proportions across the predictor, X:\n\n\nggplot(data = ex1, aes(x = x, fill = outcome)) + \n      geom_bar(stat = \"count\", position = \"fill\") +\n      labs(y = \"Proportion of Outcome\")"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#decide-on-how-to-address-zero-cell-22",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#decide-on-how-to-address-zero-cell-22",
    "title": "Lesson 13: Numerical Problems",
    "section": "Decide on how to address zero cell (2/2)",
    "text": "Decide on how to address zero cell (2/2)\n\nLook at the proportions across the predictor, X:\n\n\nggplot(data = ex1, aes(x = x, fill = outcome)) + \n      geom_bar(stat = \"count\", position = \"fill\") +\n      labs(y = \"Proportion of Outcome\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\nCombining groups 2 and 3 together may not be a good idea.\nTheir proportions of the outcome do not look similar.\nThe predictor has an ordinal quality, so this is making me think a continuous approach might be good."
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#collapse-the-categories-of-predictor",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#collapse-the-categories-of-predictor",
    "title": "Lesson 13: Numerical Problems",
    "section": "Collapse the categories of predictor",
    "text": "Collapse the categories of predictor\nCombine groups 2 and 3:\n\nex1_23 = ex1 %&gt;% \n            mutate(x = factor(x, levels = c(\"One\", \"Two\", \"Three\"), \n                                 labels = c(\"One\", \"Two-Three\", \"Two-Three\")))\nex1_23_glm = glm(outcome ~ x, data = ex1_23, family = binomial)\ntbl_regression(ex1_23_glm, exponentiate=T) %&gt;% as_gt() %&gt;% \n  tab_options(table.font.size = 38)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\n\nx\n\n\n\n\n\n\n\n\n    One\n—\n—\n\n\n\n\n    Two-Three\n7.43\n2.32, 26.3\n0.001\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\n\nBased on our previous visual, I don’t think this is a good idea\nLook at the estimated OR comparing group 2 to group 1 from our original model: 2.79 (95% CI: 0.79, 10.5)\n\nLooks different than the estimated OR in the above table"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#remove-the-category-with-0-cells",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#remove-the-category-with-0-cells",
    "title": "Lesson 13: Numerical Problems",
    "section": "Remove the category with 0 cells",
    "text": "Remove the category with 0 cells\nRemove group 3 from the data:\n\nex1_two = ex1 %&gt;% filter(x != \"Three\")\nex1_two_glm = glm(outcome ~ x, data = ex1_two, family = binomial())\ntbl_regression(ex1_two_glm, exponentiate=T) %&gt;% as_gt() %&gt;% \n  tab_options(table.font.size = 38)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\n\nx\n\n\n\n\n\n\n\n\n    One\n—\n—\n\n\n\n\n    Two\n2.79\n0.79, 10.5\n0.12\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\n \n\nNot a good idea because we lose information (sample size goes down!)\nAnd really bad when we have other predictors!!"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#treat-predictor-as-continuous",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#treat-predictor-as-continuous",
    "title": "Lesson 13: Numerical Problems",
    "section": "Treat predictor as continuous",
    "text": "Treat predictor as continuous\n\nWhen we treat a predictor as continuous, we need to make sure we have linearty between continuous predictor and log-odds\nCannot test this before fitting the logistic regression with the continuous predictor\n\nTry taking the logit of a probability of 1… it’s infinity!\n\n\n\nex1_cont = ex1 %&gt;% mutate(x = as.numeric(x))\nex1_cont_glm = glm(outcome ~ x, data = ex1_cont, family = binomial())\ntbl_regression(ex1_cont_glm, exponentiate=T) %&gt;% as_gt() %&gt;% \n  tab_options(table.font.size = 38)\n\n\n\n\n\n\n\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\n\nx\n6.22\n2.63, 18.0\n&lt;0.001\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#treat-predictor-as-continuous-check-linearity-assumption",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#treat-predictor-as-continuous-check-linearity-assumption",
    "title": "Lesson 13: Numerical Problems",
    "section": "Treat predictor as continuous: check linearity assumption",
    "text": "Treat predictor as continuous: check linearity assumption\n\nnewdata = data.frame(x = c(1, 2, 3)) \npred = predict(ex1_cont_glm, newdata, se.fit=T, type = \"link\")\nLL_CI1 = pred$fit - qnorm(1-0.05/2) * pred$se.fit\nUL_CI1 = pred$fit + qnorm(1-0.05/2) * pred$se.fit\n\npred_link = cbind(Pred = pred$fit, LL_CI1, UL_CI1) %&gt;% inv.logit()\npred_prob = as.data.frame(pred_link) %&gt;% mutate(x = c(\"One\", \"Two\", \"Three\"))\n\n\n\n\n\nPlotting sample and predicted probabilities\nggplot() + \n      geom_bar(data = ex1, aes(x = x, fill = outcome), stat = \"count\", position = \"fill\") +\n      labs(y = \"Proportion of Outcome\") +\n      scale_fill_manual(values=c(\"#D6295E\", \"#ED7D31\")) +\n      geom_point(data = pred_prob, aes(x = x, y=Pred), size=3) +\n      geom_errorbar(data = pred_prob, aes(x = x, y=Pred, ymin = LL_CI1, ymax = UL_CI1), width = 0.25)\n\n\n\n\n\n\n\n\n\n\n \n\nThis looks pretty good. We’ve mostly captured the trend of the outcome proportion!"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-count-when-we-have-multiple-predictors",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-count-when-we-have-multiple-predictors",
    "title": "Lesson 13: Numerical Problems",
    "section": "Zero cell count when we have multiple predictors",
    "text": "Zero cell count when we have multiple predictors\n\nNote that we may not see the zero count cells in a single predictor\n \n\nBut we may have issues if there is an interaction!\n\n \n\nThis is why I suggested we keep an eye out for cell counts below 10 in our lab!\n\n\n \n\nIf you see a big coefficient estimate with a big standard deviation for a specific category or interaction…\n \n\n…this may mean that a low cell count for that category is causing you issues!"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-count-summary",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-count-summary",
    "title": "Lesson 13: Numerical Problems",
    "section": "Zero cell count: summary",
    "text": "Zero cell count: summary\nMy suggestion is to try possible solutions in this order\n\nFor group with zero cell count, see if there is an adjacent group that makes sense to combine it with\n\n \n\nIf that does not make sense (or obscures your data) AND your data has an inherent order, then you can try treating it as continuous.\n\n \n\nRemove the zero count group and all the observations in it (not a very good solution)\n\n \n\nAdd a half count to each cell (only works for a single predictor)"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#poll-everywhere-question-1",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#poll-everywhere-question-1",
    "title": "Lesson 13: Numerical Problems",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#complete-separation",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#complete-separation",
    "title": "Lesson 13: Numerical Problems",
    "section": "Complete Separation",
    "text": "Complete Separation\n\nComplete separation: occurs when a collection of the covariates completely separates the outcome groups\n\nExample: Outcome is “gets senior discount at iHop” and the only covariate you measure is age\nAge will completely separate the outcome\nNo overlap in distribution of covariates between two outcome groups\n\n\n \n\nProblem: the maximum likelihood estimates do not exist\n\nLikelihood function is monotone\nIn order to have finite maximum likelihood estimates we must have some overlap in the distribution of the covariates in the model"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#poll-everywhere-question-2",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#poll-everywhere-question-2",
    "title": "Lesson 13: Numerical Problems",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-example-13",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-example-13",
    "title": "Lesson 13: Numerical Problems",
    "section": "Complete Separation: example (1/3)",
    "text": "Complete Separation: example (1/3)\n\n\n\nWe get a warning when we have complete separation\n\n\ny = c(0,0,0,0,1,1,1,1)\nx1 = c(1,2,3,3,5,6,10,11)\nx2 = c(3,2,-1,-1,2,4,1,0)\nex3 = data.frame(outcome = y, x1 = x1, x2= x2)\nex3\n\n\n\n\n  outcome x1 x2\n1       0  1  3\n2       0  2  2\n3       0  3 -1\n4       0  3 -1\n5       1  5  2\n6       1  6  4\n7       1 10  1\n8       1 11  0\n\n\n\n \n\nm1 = glm(outcome ~ x1 + x2, data = ex3, family=binomial)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\n\nOutcomes of 0 and 1 are completely separated by x2\n\nIf x2 &gt; 4 then outcome is 1\nIf x2 &lt; 4 then outcome is 0"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-example-23",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-example-23",
    "title": "Lesson 13: Numerical Problems",
    "section": "Complete Separation: example (2/3)",
    "text": "Complete Separation: example (2/3)\n\n\n \nCoefficient estimates:\n\ntidy(m1, conf.int=T) %&gt;% gt() %&gt;% \n  tab_options(table.font.size = 35) %&gt;%\n  fmt_number(decimals = 2)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n−66.10\n183,471.72\n0.00\n1.00\n−10,644.72\n10,512.52\n\n\nx1\n15.29\n27,362.84\n0.00\n1.00\n−3,122.69\nNA\n\n\nx2\n6.24\n81,543.72\n0.00\n1.00\n−12,797.28\nNA"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-example-33",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-example-33",
    "title": "Lesson 13: Numerical Problems",
    "section": "Complete Separation: example (3/3)",
    "text": "Complete Separation: example (3/3)\n\n\n \nCoefficient estimates:\n\ntidy(m1, conf.int=T) %&gt;% gt() %&gt;% \n  tab_options(table.font.size = 35) %&gt;%\n  fmt_number(decimals = 2)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n−66.10\n183,471.72\n0.00\n1.00\n−10,644.72\n10,512.52\n\n\nx1\n15.29\n27,362.84\n0.00\n1.00\n−3,122.69\nNA\n\n\nx2\n6.24\n81,543.72\n0.00\n1.00\n−12,797.28\nNA\n\n\n\n\n\n\n\n\n \n\n\nCoefficient estimate of x1 is large\nStandard error of x1’s coefficient is large\nBut also the coefficients and standard errors for the intercept and x2 are large!"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-1",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-1",
    "title": "Lesson 13: Numerical Problems",
    "section": "Complete Separation",
    "text": "Complete Separation\n\nThe occurrence of complete separation in practice depends on\n\nSample size\nNumber of subjects with the outcome present\nNumber of variables included in the model\n\n\n \n\nExample: 25 observations and only 5 have “success” outcome\n\n1 variable in model may not lead to complete separation\nMore variables = more dimensions that can completely separate the observations\n\n\n \n\nIn most cases, the occurrence of complete separation is not bad for clinical importance\n\nBut rather a numerical coincidence that causing problem for model fitting"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#poll-everywhere-question-3",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#poll-everywhere-question-3",
    "title": "Lesson 13: Numerical Problems",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-ways-to-address-issue",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-ways-to-address-issue",
    "title": "Lesson 13: Numerical Problems",
    "section": "Complete Separation: Ways to address issue",
    "text": "Complete Separation: Ways to address issue\n\nCollapse categorical variables in a meaningful way\n\nEasiest and best if stat methods are restricted (common for collaborations)\n\n\n \n\nExclude x1 from the model\n\nNot ideal because this could lead to biased estimates for the other predicted variables in the model\n\n\n \n\nFirth logistic regression\n\nUses penalized likelihood estimation method\nBasically takes the likelihood (that has no maximum) and adds a penalty that makes the MLE estimatable"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-firth-logistic-regression",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-firth-logistic-regression",
    "title": "Lesson 13: Numerical Problems",
    "section": "Complete Separation: Firth logistic regression",
    "text": "Complete Separation: Firth logistic regression\n\nlibrary(logistf)\nm1_f = logistf(outcome ~ x1 + x2, data = ex3, family=binomial)\nsummary(m1_f) # Cannot use tidy on this :(\n\nlogistf(formula = outcome ~ x1 + x2, data = ex3, family = binomial)\n\nModel fitted by Penalized ML\nCoefficients:\n                  coef  se(coef)   lower 0.95 upper 0.95     Chisq          p\n(Intercept) -2.9748898 1.7244237 -15.47721665 -0.1208883 4.2179522 0.03999841\nx1           0.4908484 0.2745754   0.05268216  2.1275832 5.0225056 0.02501994\nx2           0.4313732 0.4988396  -0.65793078  4.4758930 0.7807099 0.37692411\n            method\n(Intercept)      2\nx1               2\nx2               2\n\nMethod: 1-Wald, 2-Profile penalized log-likelihood, 3-None\n\nLikelihood ratio test=5.505687 on 2 df, p=0.06374636, n=8\nWald test = 3.624899 on 2 df, p = 0.1632538"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity",
    "title": "Lesson 13: Numerical Problems",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nMulticollinearity happens when one or more of the covariates in a model can be predicted from other covariates in the same model\n\n \n\nThis will cause unreliable coefficient estimates for some covariates in logistic regression, as in an ordinary linear regression\n\n \n\nLooking at correlations among pairs of variables is helpful but not enough to identify multicollinearity problem\n\nBecause multicollinearity problems may involve relationships among more than two covariates"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-example-14",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-example-14",
    "title": "Lesson 13: Numerical Problems",
    "section": "Multicollinearity: example (1/4)",
    "text": "Multicollinearity: example (1/4)\n\nTable below is a simulated data with\n\n\\(x_1 \\sim \\text{Normal}(0,1)\\)\n\\(x_2 = x_1 + \\text{Uniform}(0,0.1)\\)\n\\(x_3 = 1 + \\text{Uniform}(0, 0.01)\\)\n\nTherefore, \\(x_1\\) and \\(x_2\\) are highly correlated, and \\(x_3\\) is nearly collinear with the constant term"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-example-24",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-example-24",
    "title": "Lesson 13: Numerical Problems",
    "section": "Multicollinearity: example (2/4)",
    "text": "Multicollinearity: example (2/4)\n\nFour logistic regression models using data in the previous slide\nConsequence of multicollinearity: large coefficient estimates and/or standard errors"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-example-34",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-example-34",
    "title": "Lesson 13: Numerical Problems",
    "section": "Multicollinearity: example (3/4)",
    "text": "Multicollinearity: example (3/4)\n\nFour logistic regression models using data in the previous slide\nConsequence of multicollinearity: large coefficient estimates and/or standard errors"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-example-44",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-example-44",
    "title": "Lesson 13: Numerical Problems",
    "section": "Multicollinearity: example (4/4)",
    "text": "Multicollinearity: example (4/4)\n\nFour logistic regression models using data in the previous slide\nConsequence of multicollinearity: large coefficient estimates and/or standard errors"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-how-to-detect",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-how-to-detect",
    "title": "Lesson 13: Numerical Problems",
    "section": "Multicollinearity: how to detect",
    "text": "Multicollinearity: how to detect\n\nMulticollinearity only involves the covariates\n\nNo specific issues to logistic regression (vs. linear regression)\nTechniques from 512/612 work well for logistic regression model\n\n\n \n\nIn more complicated dataset/analysis, we may not be able to detect multicollinearity using the coefficient estimates/SE\n\n \n\nVariance inflation factor (VIF) approach: well-known approach to detect multicollinearity"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#variance-inflation-factor-vif-approach",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#variance-inflation-factor-vif-approach",
    "title": "Lesson 13: Numerical Problems",
    "section": "Variance Inflation Factor (VIF) Approach",
    "text": "Variance Inflation Factor (VIF) Approach\n\nComputed by regressing each variable on all the other explanatory variables\n\nFor example: \\(E(x_1│x_2,x_3,…)=\\alpha_0+\\alpha_1 x_2+\\alpha_2 x_3\\)\n\nCalculate the coefficient of determination, \\(R^2\\)\n\nProportion of the variation in \\(x_1\\) that is predicted from \\(x_2\\), \\(x_3\\),… \\[VIF = \\dfrac{1}{1=R^2}\\]\n\nEach covariate has its own VIF computed\nGet worried for multicollinearity if VIF &gt; 10\nSometimes VIF approach may miss serious multicollinearity\n\nSame multicollinearity we wish to detect using VIF can cause numerical problems in reliably estimating \\(R^2\\)"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-ways-to-address-the-issue",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-ways-to-address-the-issue",
    "title": "Lesson 13: Numerical Problems",
    "section": "Multicollinearity: Ways to address the issue",
    "text": "Multicollinearity: Ways to address the issue\n\nExclude the redundant variable from the model\nScaling and centering variables\n\nWhen you have transformed a continuous variable\n\nOther modeling approach (outside scope of this class)\n\nRidge regression\nPrinciple component analysis\n\n\n \n\nPlease take a look at the BSTA 512/612 lesson that included multicollinearity"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#poll-everywhere-question-4",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#poll-everywhere-question-4",
    "title": "Lesson 13: Numerical Problems",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lessons/01_Intro/01_Intro.html#course-learning-objectives",
    "href": "lessons/01_Intro/01_Intro.html#course-learning-objectives",
    "title": "Lesson 1: Welcome!",
    "section": "Course Learning Objectives",
    "text": "Course Learning Objectives\nAt the end of this course, students should be able to…\n\nApply and interpret some hypothesis-testing procedures for two-way and three-way contingency tables\nCompute and interpret measures of association for binary and ordinal data.\nCalculate and correctly interpret odds ratios using logistic regression, make comparison across groups and examine relationship between binary outcome and predictor variables.\nApply appropriate model-building strategies for logistic regression. Effectively use statistical computing packages for contingency table and logistic regression procedures.\nPerform Poisson regression analysis using count data and interpret model estimates, make comparison across groups and examine relationship between outcome and predictor variables.\nCoherently summarize methods and results of data analyses, and discuss in context of original health-related research questions to audiences with varied statistical background."
  },
  {
    "objectID": "lessons/01_Intro/01_Intro.html#new-way-to-recordview-lectures",
    "href": "lessons/01_Intro/01_Intro.html#new-way-to-recordview-lectures",
    "title": "Lesson 1: Welcome!",
    "section": "New way to record/view lectures",
    "text": "New way to record/view lectures\n\nI will be using Echo360 to automatically record time from 1-3pm in this room!\nIt will be LIVE\n\nSo you can watch in real time, but won’t be able to interact with us\nMight have a 10 second lag\nNo need to tell me if you cannot make class\n\nHere is the link to the Echo site! I’ll keep posting it on the weekly pages\n\nCurrently a public link, but working on getting everyone an account so we can make it private\n\nI don’t have to post anything after class, so no issues with me forgetting to post it"
  },
  {
    "objectID": "lessons/01_Intro/01_Intro.html#new-exit-ticket-style",
    "href": "lessons/01_Intro/01_Intro.html#new-exit-ticket-style",
    "title": "Lesson 1: Welcome!",
    "section": "New Exit ticket style",
    "text": "New Exit ticket style\n\nAll the questions are optional\nBut still open the link and submit!\n4 of the exit tickets are dropped!"
  },
  {
    "objectID": "lessons/01_Intro/01_Intro.html#some-important-tasks",
    "href": "lessons/01_Intro/01_Intro.html#some-important-tasks",
    "title": "Lesson 1: Welcome!",
    "section": "Some important tasks",
    "text": "Some important tasks\n\nStar the class website: https://nwakim.github.io/S25_BSTA_513/\nComplete the when2meet for office hours\n\nIf your calendar feels set, take 5 minutes to fill this out now!\nComplete by Thursday at 11pm!!!\n\nHighly suggest that you make an appointment with a learning specialist through Student Academic Support Services!\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "lessons/01_Intro/01_Intro.html#syllabus",
    "href": "lessons/01_Intro/01_Intro.html#syllabus",
    "title": "Lesson 1: Welcome!",
    "section": "Syllabus",
    "text": "Syllabus\n\nNot many changes from last quarters syllabus\nLet’s look at the schedule!"
  },
  {
    "objectID": "lessons/01_Intro/01_Intro.html#quizzes",
    "href": "lessons/01_Intro/01_Intro.html#quizzes",
    "title": "Lesson 1: Welcome!",
    "section": "Quizzes",
    "text": "Quizzes\n\nLast quarter, I removed the quizzes that I implemented in W24\nI noticed there were some key learning objectives that many of us were not meeting in the project\n\nNot your fault\n\nLeading me to implement some new assessment practices\nI think quizzes are a helpful way to embed a checkpoint for understanding\nQuizzes will NOT be in class\n\nQuizzes will open at 3pm on Wednesday and close at 11pm on Sunday\nI’m open to changing the exact timing of this"
  },
  {
    "objectID": "lessons/01_Intro/01_Intro.html#homework-grading",
    "href": "lessons/01_Intro/01_Intro.html#homework-grading",
    "title": "Lesson 1: Welcome!",
    "section": "Homework grading",
    "text": "Homework grading\n\nSlightly new grading\nNow, need to turn in 50% of the homework completed to get check mark\nNoticed that demonstrating understanding in the project was correlated with completing the homework*\n\nNo formal analysis was done on this\nAnd there may be confounders like time available to commit to this class in general\n\nEither way, I think practice is the most important tool for learning\n\nSo I want to us to practice the work, but I’m trying to balance this with added stress"
  },
  {
    "objectID": "lessons/01_Intro/01_Intro.html#homework-grading-in-syllabus",
    "href": "lessons/01_Intro/01_Intro.html#homework-grading-in-syllabus",
    "title": "Lesson 1: Welcome!",
    "section": "Homework grading in syllabus",
    "text": "Homework grading in syllabus"
  },
  {
    "objectID": "lessons/01_Intro/01_Intro.html#how-to-print-slides",
    "href": "lessons/01_Intro/01_Intro.html#how-to-print-slides",
    "title": "Lesson 1: Welcome!",
    "section": "How to print slides",
    "text": "How to print slides\n\nAnyone have issues with this?\n\nI can show how to do it in Chrome and Safari\n\nInstructions on Quarto page"
  },
  {
    "objectID": "lessons/01_Intro/01_Intro.html#lets-take-10-minutes-for-student-survey",
    "href": "lessons/01_Intro/01_Intro.html#lets-take-10-minutes-for-student-survey",
    "title": "Lesson 1: Welcome!",
    "section": "Let’s take 10 minutes for Student Survey",
    "text": "Let’s take 10 minutes for Student Survey\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n−+\n11:00"
  },
  {
    "objectID": "lessons/01_Intro/01_Intro.html#now-we-take-a-10-minute-break",
    "href": "lessons/01_Intro/01_Intro.html#now-we-take-a-10-minute-break",
    "title": "Lesson 1: Welcome!",
    "section": "Now we take a 10 minute break!",
    "text": "Now we take a 10 minute break!\n\n\n\n−+\n10:00"
  },
  {
    "objectID": "lessons/01_Intro/01_Intro.html#assessment-breakdown",
    "href": "lessons/01_Intro/01_Intro.html#assessment-breakdown",
    "title": "Lesson 1: Welcome!",
    "section": "Assessment Breakdown",
    "text": "Assessment Breakdown\n\nWe will have quizzes and required in-person lab discussions (on schedule)\nGetting rid of midterm feedback - felt exhaustive\n\n\n\n\n\n\n\n\n\n\n\nCourse activity\nType of Assessment\nDue Date\nPercentage of final grade (BSTA 513)\nPercentage of final grade (BSTA 613)\n\n\nHomework\nFormative\nEvery 1-2 weeks\n30%\n25%\n\n\nQuizzes\nSummative\n4/27, 5/18, 6/1\n25%\n25%\n\n\nProject Labs\nFormative/Summative\nEvery 2-3 weeks\n25%\n25%\n\n\nLab Discussion + Poster Day Participation\nN/A\nEvery 2-3 weeks\n5%\n5%\n\n\nProject Poster\nSummative\n6/13\n10%\n10%\n\n\nExit tickets (Attendance)\nN/A\nTwice Weekly\n5%\n5%\n\n\n613 Readings\nFormative\nApprox. every other week\n0%\n5%"
  },
  {
    "objectID": "lessons/01_Intro/01_Intro.html#now-we-take-a-5-minute-break",
    "href": "lessons/01_Intro/01_Intro.html#now-we-take-a-5-minute-break",
    "title": "Lesson 1: Welcome!",
    "section": "Now we take a 5 minute break!",
    "text": "Now we take a 5 minute break!\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "lessons/01_Intro/01_Intro.html#lab-discussion-poster-day-participation",
    "href": "lessons/01_Intro/01_Intro.html#lab-discussion-poster-day-participation",
    "title": "Lesson 1: Welcome!",
    "section": "Lab Discussion + Poster Day Participation",
    "text": "Lab Discussion + Poster Day Participation\n\nWe will have 5 classes with required in-person attendance\n\n4 lab discussion days\n1 poster day\n\nLab discussions will either take the full or half class time\n\nThey will always follow the due date for a lab\n\nYou will spend the time in a pair or group to discuss what you did in your lab"
  },
  {
    "objectID": "lessons/01_Intro/01_Intro_key_info.html",
    "href": "lessons/01_Intro/01_Intro_key_info.html",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "For National Public Health Week, join us for an exclusive tour of the School of Public Health’s only clinical laboratory, the Center for Infectious Disease (CIDS) on Tuesday, April 8 from 3:00 - 3:45pm. Getting a peek into the work of CIDS, you’ll witness first hand how applied public health research shapes real-world solutions and directly supports community well-being. Their expert-led research delves into the depths of pharmacoepidemiology and rheumatologic diseases, enhancing drug safety and efficacy for the broader population. Join us for a tour to expand your academic horizons, ask questions of the research team on what their day-to-day work and career path looks like, and to see what options you have as a pivotal researcher who is advancing health equity and enhancing community health outcomes."
  },
  {
    "objectID": "lessons/01_Intro/01_Intro_key_info.html#announcements",
    "href": "lessons/01_Intro/01_Intro_key_info.html#announcements",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "For National Public Health Week, join us for an exclusive tour of the School of Public Health’s only clinical laboratory, the Center for Infectious Disease (CIDS) on Tuesday, April 8 from 3:00 - 3:45pm. Getting a peek into the work of CIDS, you’ll witness first hand how applied public health research shapes real-world solutions and directly supports community well-being. Their expert-led research delves into the depths of pharmacoepidemiology and rheumatologic diseases, enhancing drug safety and efficacy for the broader population. Join us for a tour to expand your academic horizons, ask questions of the research team on what their day-to-day work and career path looks like, and to see what options you have as a pivotal researcher who is advancing health equity and enhancing community health outcomes."
  },
  {
    "objectID": "lessons/01_Intro/01_Intro_key_info.html#key-dates",
    "href": "lessons/01_Intro/01_Intro_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/01_Intro/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "href": "lessons/01_Intro/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "title": "Muddy Points",
    "section": "Muddy Points from Spring 2024",
    "text": "Muddy Points from Spring 2024"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA_muddy_points.html#muddy-points-from-spring-2024",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA_muddy_points.html#muddy-points-from-spring-2024",
    "title": "Muddy Points",
    "section": "Muddy Points from Spring 2024",
    "text": "Muddy Points from Spring 2024\n\n1. Why does the test of trend treat ordinal variables as quantitative rather than qualitative?\nWhen we treat something as qualitative, we can only look at differences between groups. This means we cannot rank the groups and look at the change across groups. By treating the ordinal variables as quantitative, we can look at the change as we move from one group to another (and over all the ranked categories).\n\n\n2. Organizing the tests in a tree\nHere’s a organizational tree that I took from Meike and expanded:"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/02_Intro_CDA_key_info.html#key-dates",
    "href": "lessons/14_Model_diagnostics/02_Intro_CDA_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/02_Intro_CDA_key_info.html#last-class",
    "href": "lessons/14_Model_diagnostics/02_Intro_CDA_key_info.html#last-class",
    "title": "Key Info and Announcements",
    "section": "Last class",
    "text": "Last class"
  },
  {
    "objectID": "lessons/09_Missing_data/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "href": "lessons/09_Missing_data/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "title": "Muddy Points",
    "section": "Muddy Points from Spring 2024",
    "text": "Muddy Points from Spring 2024"
  },
  {
    "objectID": "lessons/05_Simple_logistic_reg/week_03_sched.html#resources",
    "href": "lessons/05_Simple_logistic_reg/week_03_sched.html#resources",
    "title": "Week 3",
    "section": "Resources",
    "text": "Resources\n\n\n\nLesson\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n5\nSimple Logistic Regression\n\n\n\n\n\n6\nTests for GLMs using Likelihood function\n\n\n\n\n\n\nIf you ever have trouble with the above links to the videos, this Echo360 link should take you to our class page."
  },
  {
    "objectID": "lessons/05_Simple_logistic_reg/week_03_sched.html#on-the-horizon",
    "href": "lessons/05_Simple_logistic_reg/week_03_sched.html#on-the-horizon",
    "title": "Week 3",
    "section": "On the Horizon",
    "text": "On the Horizon\n\nLab 1 due this Thursday (4/18)\nQuiz 1 opens on Monday, 4/22, at 2pm and will close on Wednesday, 4/24, at 1pm"
  },
  {
    "objectID": "lessons/05_Simple_logistic_reg/week_03_sched.html#class-exit-tickets",
    "href": "lessons/05_Simple_logistic_reg/week_03_sched.html#class-exit-tickets",
    "title": "Week 3",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (4/15)\n Wednesday (4/17)"
  },
  {
    "objectID": "lessons/05_Simple_logistic_reg/week_03_sched.html#announcements",
    "href": "lessons/05_Simple_logistic_reg/week_03_sched.html#announcements",
    "title": "Week 3",
    "section": "Announcements",
    "text": "Announcements\n\nMonday 4/15\n\nI am trying to stay on track of the Exit tickets this quarter\n\nThat may mean you have a 0 in your gradebook\nAs long as you complete the exit ticket within the 7 days, I will change the 0 to a 1\nI plan to have a scheduled block on Fridays to check them\n\n\n\n\nWednesday 4/17\n\nQuiz 1 info"
  },
  {
    "objectID": "lessons/05_Simple_logistic_reg/week_03_sched.html#muddiest-points",
    "href": "lessons/05_Simple_logistic_reg/week_03_sched.html#muddiest-points",
    "title": "Week 3",
    "section": "Muddiest Points",
    "text": "Muddiest Points\n\n1. Not entirely sure I understand what IRLS is about\nFair enough. It’s a little confusing. IWLS is an iterative solving technique that let’s us solve the coefficient estimates ( \\(\\beta_0\\) , \\(\\beta_1\\)) without solving the equations theoretically.\nWe start with an educated guess of the estimates, put them into the likelihood, and calculate the likelihood. Then we update the estimates using some complicated math, put them into the likelihood, and calculate the likelihood again. We compare the two likelihoods, and if the likelihood increases, then we keep going. We stop when the increase in likelihood between iterations is small. This means we are at or very close to the maximum likelihood.\n\n\n2. Link functions\nYes! Link functions are the important transformations we need to make to our outcome in order to connect them to our perdictors/covariates. Specifically, it’s the transformation we make to our mean/expected value.\nThe same link function can be used different types of outcomes. And here’s a few examples:\n\nContinuous data: identity\nBinary: logit, log\nCount/Poisson: log\n\nOur goal with link functions is to put our outcome on a flexible range so that any range of covariates can be mapped to it with coefficients. So think about trying to map age onto a 0 or 1… We can’t come up with an equation like \\(\\beta_0 + \\beta_1 Age\\) that perfectly maps to only 0’s and 1’s.\n\n\n3. Is GLM the umbrella over the other functions? The 4 functions all use different distributions, yes?\nGLM is the umbrella term for different types of regression! Not all types of regression have different outcome distributions. For example, a binary outcome can be used in logistic regression with the logit link or log-binomial regression with the log link.\n\n\n4. What would you need to change in your model to reduce a high IRLS number? As I understand it from the lecture, a high number suggests convergence but it appeared like something unfavorable even though a model that converges might be closer to maximum likelihood or maybe the distance to maximum likelihood\nA high number suggests that the model did NOT converge! Thus, we did not land on an estimate close to our maximum likelihood. You can think of the IRLS number as the number of iterations it is taking to find the maximum likelihood estimate (MLE). If it takes too many iterations, then it just stops without finding the MLE.\n\n\n5. We’re using linear vs logistic, but which are we focusing on? Regarding linear, how does linear used in categorical differ from continuous?\nWe are focusing on logistic! We cannot use linear regression on our binary outcomes anymore. When I say “linear” mapping I mean the mapping between our covariates and the transformed mean outcome using the link function.\n\n\n6. By the end of class (Lesson 6) my understanding is that the saturated model likelihood is the same between the two models being compared, right?\nYep!!\n\n\n7. The differences between each test and when to use them.\nIn terms of what each test is measuring:\n\nThe Wald test measures the distance between two potential values of \\(\\beta\\). One under the null and one under the alternative. The further they are from each other, the more evidence we have that they are different.\n\nThe Wald test approximates the differences in the likelihood function, but we do not actually compare the likelihoods under the null vs. alternative. We are only comparing the difference in the \\(\\beta\\) value, that is a reasonable approximation of the difference in the likelihood.\n\nThe Score test measures how close the tangent line of the likelihood function is to 0 (under the null). If it is close to 0 under the null, this indicates that our MLE of \\(\\beta\\) is not far from 0. Again, this is no a direct comparison of the likelihoods, but only an approximation of the difference.\nThe likelihood ratio test measures the difference in the log-likelihoods. This is a direct comparison of likelihoods, and is not an approximation!\n\nThus, we compare the likelihoods (horizontally, as someone asked) because we are making direct comparisons between the likelihood under the null and under the alternative."
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA_key_info.html#key-dates",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA_key_info.html#last-class",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA_key_info.html#last-class",
    "title": "Key Info and Announcements",
    "section": "Last class",
    "text": "Last class"
  },
  {
    "objectID": "lessons/06_Tests_GLMs/02_Intro_CDA_key_info.html#key-dates",
    "href": "lessons/06_Tests_GLMs/02_Intro_CDA_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/06_Tests_GLMs/02_Intro_CDA_key_info.html#last-class",
    "href": "lessons/06_Tests_GLMs/02_Intro_CDA_key_info.html#last-class",
    "title": "Key Info and Announcements",
    "section": "Last class",
    "text": "Last class"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/02_Intro_CDA_key_info.html#key-dates",
    "href": "lessons/13_Numeric_Problems/02_Intro_CDA_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/02_Intro_CDA_key_info.html#last-class",
    "href": "lessons/13_Numeric_Problems/02_Intro_CDA_key_info.html#last-class",
    "title": "Key Info and Announcements",
    "section": "Last class",
    "text": "Last class"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "href": "lessons/13_Numeric_Problems/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "title": "Muddy Points",
    "section": "Muddy Points from Spring 2024",
    "text": "Muddy Points from Spring 2024"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/02_Intro_CDA_key_info.html#key-dates",
    "href": "lessons/10_Multiple_logistic_regression/02_Intro_CDA_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/02_Intro_CDA_key_info.html#last-class",
    "href": "lessons/10_Multiple_logistic_regression/02_Intro_CDA_key_info.html#last-class",
    "title": "Key Info and Announcements",
    "section": "Last class",
    "text": "Last class"
  },
  {
    "objectID": "lessons/08_Interpretations_SLR/week_04_sched.html#resources",
    "href": "lessons/08_Interpretations_SLR/week_04_sched.html#resources",
    "title": "Week 4",
    "section": "Resources",
    "text": "Resources\n\n\n\nLesson\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n7\nPredictions and Visualizations in Simple Logistic Regression\n\n\n\n\n\n8\nInterpretations and Visualizations of Odds Ratios"
  },
  {
    "objectID": "lessons/08_Interpretations_SLR/week_04_sched.html#on-the-horizon",
    "href": "lessons/08_Interpretations_SLR/week_04_sched.html#on-the-horizon",
    "title": "Week 4",
    "section": "On the Horizon",
    "text": "On the Horizon\n\nHomework 2 due this Thursday"
  },
  {
    "objectID": "lessons/08_Interpretations_SLR/week_04_sched.html#class-exit-tickets",
    "href": "lessons/08_Interpretations_SLR/week_04_sched.html#class-exit-tickets",
    "title": "Week 4",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (4/22)\n\nMonday Exit ticket will not be graded bc of quiz\n\n Wednesday (4/24)"
  },
  {
    "objectID": "lessons/08_Interpretations_SLR/week_04_sched.html#announcements",
    "href": "lessons/08_Interpretations_SLR/week_04_sched.html#announcements",
    "title": "Week 4",
    "section": "Announcements",
    "text": "Announcements\nWednesday 4/24\n\nHave you all seen this??? Page on Basic Needs for students\n\nSPH Emergency funds\nCARE program\nCommittee for Improving Student Food Security\n\nLab 2 is up!\n\nFrom Lab 1:\n\nDO NOT USE ANY_HARDSHIP or MULT_HARDSHIP as your main variable\n\nThese are constructed from food insecurity variable\nSee the User Guide in the downloaded ICPSR folder\n\n\n\nQuiz 1 should be in!\nLab 1 feedback still in progress\nReview last quarter’s project\n\nOn Monday we will take 15 minutes to discuss changes to the project report instructions\nI will bring the learning objectives that I want to assess\nWe can rework or scrap parts of the report that do not assess these learning objectives\nAs part of the exit ticket, I will ask about your preferences as well"
  },
  {
    "objectID": "lessons/08_Interpretations_SLR/week_04_sched.html#muddiest-points",
    "href": "lessons/08_Interpretations_SLR/week_04_sched.html#muddiest-points",
    "title": "Week 4",
    "section": "Muddiest Points",
    "text": "Muddiest Points\nNone?? Wowza!"
  },
  {
    "objectID": "lessons/08_Interpretations_SLR/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "href": "lessons/08_Interpretations_SLR/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "title": "Muddy Points",
    "section": "Muddy Points from Spring 2024",
    "text": "Muddy Points from Spring 2024"
  },
  {
    "objectID": "lessons/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree_key_info.html#key-dates",
    "href": "lessons/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree_key_info.html#last-class",
    "href": "lessons/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree_key_info.html#last-class",
    "title": "Key Info and Announcements",
    "section": "Last class",
    "text": "Last class"
  },
  {
    "objectID": "lessons/07_Pred_Viz/week_04_sched.html#resources",
    "href": "lessons/07_Pred_Viz/week_04_sched.html#resources",
    "title": "Week 4",
    "section": "Resources",
    "text": "Resources\n\n\n\nLesson\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n7\nPredictions and Visualizations in Simple Logistic Regression\n\n\n\n\n\n8\nInterpretations and Visualizations of Odds Ratios"
  },
  {
    "objectID": "lessons/07_Pred_Viz/week_04_sched.html#on-the-horizon",
    "href": "lessons/07_Pred_Viz/week_04_sched.html#on-the-horizon",
    "title": "Week 4",
    "section": "On the Horizon",
    "text": "On the Horizon\n\nHomework 2 due this Thursday"
  },
  {
    "objectID": "lessons/07_Pred_Viz/week_04_sched.html#class-exit-tickets",
    "href": "lessons/07_Pred_Viz/week_04_sched.html#class-exit-tickets",
    "title": "Week 4",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (4/22)\n\nMonday Exit ticket will not be graded bc of quiz\n\n Wednesday (4/24)"
  },
  {
    "objectID": "lessons/07_Pred_Viz/week_04_sched.html#announcements",
    "href": "lessons/07_Pred_Viz/week_04_sched.html#announcements",
    "title": "Week 4",
    "section": "Announcements",
    "text": "Announcements\nWednesday 4/24\n\nHave you all seen this??? Page on Basic Needs for students\n\nSPH Emergency funds\nCARE program\nCommittee for Improving Student Food Security\n\nLab 2 is up!\n\nFrom Lab 1:\n\nDO NOT USE ANY_HARDSHIP or MULT_HARDSHIP as your main variable\n\nThese are constructed from food insecurity variable\nSee the User Guide in the downloaded ICPSR folder\n\n\n\nQuiz 1 should be in!\nLab 1 feedback still in progress\nReview last quarter’s project\n\nOn Monday we will take 15 minutes to discuss changes to the project report instructions\nI will bring the learning objectives that I want to assess\nWe can rework or scrap parts of the report that do not assess these learning objectives\nAs part of the exit ticket, I will ask about your preferences as well"
  },
  {
    "objectID": "lessons/07_Pred_Viz/week_04_sched.html#muddiest-points",
    "href": "lessons/07_Pred_Viz/week_04_sched.html#muddiest-points",
    "title": "Week 4",
    "section": "Muddiest Points",
    "text": "Muddiest Points\nNone?? Wowza!"
  },
  {
    "objectID": "lessons/07_Pred_Viz/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "href": "lessons/07_Pred_Viz/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "title": "Muddy Points",
    "section": "Muddy Points from Spring 2024",
    "text": "Muddy Points from Spring 2024"
  },
  {
    "objectID": "lessons/15_Model_building/02_Intro_CDA_key_info.html#key-dates",
    "href": "lessons/15_Model_building/02_Intro_CDA_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/15_Model_building/02_Intro_CDA_key_info.html#last-class",
    "href": "lessons/15_Model_building/02_Intro_CDA_key_info.html#last-class",
    "title": "Key Info and Announcements",
    "section": "Last class",
    "text": "Last class"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/week_02_sched.html#resources",
    "href": "lessons/03_Meas_Assoc_CT/week_02_sched.html#resources",
    "title": "Week 2",
    "section": "Resources",
    "text": "Resources\n\n\n\nLesson\n3\nTopic\nMeasurement of Association for Contingency Tables\nSlides | Annotated Slides | Recording | :==============================================================================================================================:+:=================================================================================================================================================:+:====================================================================================================================================================================================================================================================:+  |  | \n\n\n\n\n\n\n\n4\nMeasurements of Association and Agreement\n |  |  | | | | |  |\n\n\n\nIf you ever have trouble with the above links to the videos, this Echo360 link should take you to our class page.\nFor the slides, once they are opened, if you would like to print or save them as a PDF, the best way to do this is from a computer internet browser:\n\nClick on the icon with three horizontal bars on the bottom left of the browser.\nClick on “Tools” with the gear icon at the top of the sidebar.\nClick on “PDF Export Mode.”\nFrom there, you can print or save the PDF as you would normally from your internet browser.\n\nNote: this process does not work very well on an iPad."
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/week_02_sched.html#on-the-horizon",
    "href": "lessons/03_Meas_Assoc_CT/week_02_sched.html#on-the-horizon",
    "title": "Week 2",
    "section": "On the Horizon",
    "text": "On the Horizon\n\nHomework 1 due this Thursday!"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/week_02_sched.html#announcements",
    "href": "lessons/03_Meas_Assoc_CT/week_02_sched.html#announcements",
    "title": "Week 2",
    "section": "Announcements",
    "text": "Announcements\n\nMonday 4/8\n\nOffice hours!!\n\nTuesdays 5:30-7pm with Antara\nThursdays 3:30 - 5pm with Ariel\nFridays 2 - 3:30pm with Nicky\n\n\n\n\nWednesday 4/10\n\nEcho360: Let’s all double check that we can see the recordings\n\nLink to class site\n\nHomework question 5: no need to do LRT in the table\nLab 1 is up!!\nQuiz 1 decision\n\nOnline in Sakai\nWill open up on Monday at 2pm. You can chose to take it in the classroom or wait\nQuiz will close before class on Wednesday\nOpen book still\nPlease do not cheat\n\nIf I notice any unusual changes to quiz performance compared to last quarter then we will go back to the old way of giving quizzes\n\nMultiple choice with potentially some free response\n\nFor example: interpreting an OR would be divided into a multiple choice for the estimate, CI, and then writing a sentence to interpret the estimate"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/week_02_sched.html#class-exit-tickets",
    "href": "lessons/03_Meas_Assoc_CT/week_02_sched.html#class-exit-tickets",
    "title": "Week 2",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (4/8)\n Wednesday (4/10)"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/week_02_sched.html#muddiest-points",
    "href": "lessons/03_Meas_Assoc_CT/week_02_sched.html#muddiest-points",
    "title": "Week 2",
    "section": "Muddiest Points",
    "text": "Muddiest Points\n\n1. “times greater than” vs just “times” in interpretation\nI’ve seen it both ways. It comes down to more of an English language nuance, with what seems to be a long battle between viewpoints. Or maybe more accurately, I grammatically correct way to contruct the sentence, but with people understanding the meaning the “incorrect” way. I tend to be more lenient when it comes to grammar in this way, but maybe that’s because I have a general distaste when languages are rigid and don’t accommodate how people currently speak and write.\n\n\n2. For the relative risks poll everywhere question #2, how were they derived?\n\nFor #1 with Trt A’s risk as 0.01 (aka \\(risk_A=0.01\\)) and Trt B’s risk as 0.001 (aka \\(risk_B=0.01\\))\n\nThe risk ratio is: \\(\\widehat{RR} = \\dfrac{\\widehat{p}_1}{\\widehat{p}_2} = \\dfrac{risk_A}{risk_B} = \\dfrac{0.01}{0.001} = 10\\)\n\nFor #2 with Trt A’s risk as 0.41 (aka \\(risk_A=0.41\\)) and Trt B’s risk as 0.401 (aka \\(risk_B=0.401\\))\n\nThe risk ratio is: \\(\\widehat{RR} = \\dfrac{\\widehat{p}_1}{\\widehat{p}_2} = \\dfrac{risk_A}{risk_B} = \\dfrac{0.41}{0.401} = 1.02\\)\n\n\n\n\n3. Ranges that odds ratios can take (0, infinity) vs the ranges that risk ratios can take.\nYeah, so both can theoretically take on the range \\([0, \\infty)\\). Both are ratios, so we also have to think about the range of the denominator and numerator For relative risk, the numerator and denominator are probabilities that can only take values from 0 to 1. While for ORs, the denominator and numerator are odds that can be a range of values \\([0, \\infty)\\).\nThe main point I was trying to make was that once we observe one group’s proportion/probability, then RRs and ORs will differ in their potential range. Let’s say I observe the proportion fro group 1 and now know the numerator for RR and the odds in the numerator for OR. Because the RR has numerator and denominator that has ranges \\([0, 1]\\), if we know the proportion of group 1 (aka numerator value), then the ratio itself has a smaller range of values because the denominator can only be between 0 and 1. Because the OR has numerator and denominator that has ranges \\([0, \\infty)\\), if we know the proportion of group 1, then we do have a fixed numerator. However, the denominator can still be in \\([0, \\infty)\\).\n\n\n4. For the odds ratio equation that we reviewed today, is it different from ad/bc ? If they are different, when is it appropriate to use the equation we just reviewed over the other? p1/(1-p1) / p2/(1-p2)\nNope! These are the same! If you learned it that way, you can definitely use it when we are working with contingency tables. However, once we move into ORs from regression with multiple covariates, I think it’s better to understand the ORs and odds in terms of the probability/proportion.\n\n\n5. Not directly related to this class, but did we cover LRTs already? They’re mentioned in HW1 but aren’t in my notes.\nOops! Fixed it in the HW. No need to do anything with LRTs!\n\n\n6. In Epi, we were very strictly told that Odds Ratios were only to be used in one type of study. (I.e. we CAN NOT use them in cross-sectional and cohort studies) only case-control. So what is the application of attempting to utilize them, if each respective type of study already has a “pre-assigned” statistical method that suits it best?\nOdds ratios CAN be used in cross-sectional AND cohort studies. It is often an over-estimate of the relative risk in those situations, so it is important to interpret it ONLY as the odds ratio.\nEach respective study does not have a pre-assigned method. The only restriction is that relative risk cannot be used in case-control studies."
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT_key_info.html#key-dates",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT_key_info.html#last-class",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT_key_info.html#last-class",
    "title": "Key Info and Announcements",
    "section": "Last class",
    "text": "Last class"
  },
  {
    "objectID": "lessons/11_Interactions/02_Intro_CDA_key_info.html#key-dates",
    "href": "lessons/11_Interactions/02_Intro_CDA_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/11_Interactions/02_Intro_CDA_key_info.html#last-class",
    "href": "lessons/11_Interactions/02_Intro_CDA_key_info.html#last-class",
    "title": "Key Info and Announcements",
    "section": "Last class",
    "text": "Last class"
  },
  {
    "objectID": "lessons/17_Wrap_up/week_10_sched.html",
    "href": "lessons/17_Wrap_up/week_10_sched.html",
    "title": "Week 10",
    "section": "",
    "text": "Room Locations for the week\n\n\n\nOn Monday, 6/3, we will be in RPV Room A (1217)"
  },
  {
    "objectID": "lessons/17_Wrap_up/week_10_sched.html#resources",
    "href": "lessons/17_Wrap_up/week_10_sched.html#resources",
    "title": "Week 10",
    "section": "Resources",
    "text": "Resources\n\n\n\nLesson\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n16\nPoisson Regression\n\n\n\n\n\n17\nOther types of categorical regressions!"
  },
  {
    "objectID": "lessons/17_Wrap_up/week_10_sched.html#on-the-horizon",
    "href": "lessons/17_Wrap_up/week_10_sched.html#on-the-horizon",
    "title": "Week 10",
    "section": "On the Horizon",
    "text": "On the Horizon\n\nQuiz 3 open 6/3 at 2pm\n\nCloses on 6/5 at 1pm\n\nLab 4 due yesterday!\nHW 5 due 6/6 at 11pm\nProject report due 6/13 at 11pm"
  },
  {
    "objectID": "lessons/17_Wrap_up/week_10_sched.html#announcements",
    "href": "lessons/17_Wrap_up/week_10_sched.html#announcements",
    "title": "Week 10",
    "section": "Announcements",
    "text": "Announcements\n\nMonday 6/3\n\nIn the last stretch of the project\n\nClass time next week dedicated to project report help\nI may have other time slots for project help next week. I just need to take a serious look at my calendar\n\nI’m attending a virtual stat ed conference, so I just need to balance meetings with conference attendance\n\n\nNo office hours this Wednesday 6/5\n#3 “The Last Bounce: Bunnies, Burritos, and DIY Bath Salts”\n\nWednesday, June 5th Noon-2pm\nStudent Success Center, 6th floor Vanport\n\n\n\n\nWednesday 6/5\n\nLast day of lecture!!\nQuiz 2: added 3 points to everyone’s grade for that one confusing question (I forget the number, maybe question 6?)\nLab 4\n\nYour starting variables should be the TEN from lab 2, not the 5 from Lab 3\ng-value for Hosmer-Lemeshow test\n\nPlease look at Lesson 12!! There is a note on how to pick the g-value when we have many samples!!\nSome of us are getting NA’s when we put the correct g\n\nDouble check that your observed values are in numeric form\n\n\nIf doing LASSO, make sure you describe the process!\n\nWe used LASSO regression with a penalty of 0.001 to identify important predictors. We used a single test and training split of 80% and 20%, respectively. Only main effects were considered in our LASSO regression.\nAlways think: what are the key pieces of information that someone else might need to recreate this method?\n\n\nThe class will end on June 14, 2024. All coursework is expected to be completed by June 16, 2024 at 11pm.\n\nI need to have grades in on June 21st. In order to grade fairly and thoroughly, I need the whole week to grade the project report and any late assignments.\n\nA word on project grading\n\nIn the final lab, I gave you the option to do LASSO regression and focus on prediction. I know this created some confusion since we mostly set up the project as a question of association in Lab 1-3. We can still interpret the odds ratios from LASSO regression. I will be fairly lenient if reports are confused between prediction and association aims. I will try to give feedback on it, but I will not penalize any minor confusions.\nAnother word: My process starts harsh. I want to give you as much feedback as possible, and this will also reflect in lower assigned scores. At this point, I put the report grades into Sakai. I check to see if anyone’s overall course letter grade goes down. If less than ~5 course grades go down, then I revisit their project reports. If their report fails to demonstrate the most important learning objectives from the course, then I will keep the lower grade. If they demonstrate an understanding of the most important learning objectives, then I will adjust their score to increase their course grade. If more than 5 grades go down, then I revisit everyone’s reports. I will make a class wide grade bump in all reports.\nThe most important learning objectives are: understanding when and what test is appropriate, and interpreting odds ratios (from main effects and interactions)\n\nProject: LASSO\n\nYou can take the finalized formula for LASSO and use it in glm()\n\nIn this case, use the test data to come up with predictive values (like AUC)\nYou can run it on the full dataset to get the coefficient estimates and other diagnostic information\n\n\nGuide on figures"
  },
  {
    "objectID": "lessons/17_Wrap_up/week_10_sched.html#class-exit-tickets",
    "href": "lessons/17_Wrap_up/week_10_sched.html#class-exit-tickets",
    "title": "Week 10",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (6/3)\n Wednesday (6/5)"
  },
  {
    "objectID": "lessons/17_Wrap_up/week_10_sched.html#muddiest-points",
    "href": "lessons/17_Wrap_up/week_10_sched.html#muddiest-points",
    "title": "Week 10",
    "section": "Muddiest Points",
    "text": "Muddiest Points"
  },
  {
    "objectID": "lessons/17_Wrap_up/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "href": "lessons/17_Wrap_up/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "title": "Muddy Points",
    "section": "Muddy Points from Spring 2024",
    "text": "Muddy Points from Spring 2024"
  },
  {
    "objectID": "lessons/16_Poisson_regression/02_Intro_CDA_key_info.html#key-dates",
    "href": "lessons/16_Poisson_regression/02_Intro_CDA_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/16_Poisson_regression/02_Intro_CDA_key_info.html#last-class",
    "href": "lessons/16_Poisson_regression/02_Intro_CDA_key_info.html#last-class",
    "title": "Key Info and Announcements",
    "section": "Last class",
    "text": "Last class"
  },
  {
    "objectID": "lessons/16_Poisson_regression/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "href": "lessons/16_Poisson_regression/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "title": "Muddy Points",
    "section": "Muddy Points from Spring 2024",
    "text": "Muddy Points from Spring 2024"
  },
  {
    "objectID": "lessons/16_Poisson_regression/week_10_sched.html",
    "href": "lessons/16_Poisson_regression/week_10_sched.html",
    "title": "Week 10",
    "section": "",
    "text": "Room Locations for the week\n\n\n\nOn Monday, 6/3, we will be in RPV Room A (1217)"
  },
  {
    "objectID": "lessons/16_Poisson_regression/week_10_sched.html#resources",
    "href": "lessons/16_Poisson_regression/week_10_sched.html#resources",
    "title": "Week 10",
    "section": "Resources",
    "text": "Resources\n\n\n\nLesson\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n16\nPoisson Regression\n\n\n\n\n\n17\nOther types of categorical regressions!"
  },
  {
    "objectID": "lessons/16_Poisson_regression/week_10_sched.html#on-the-horizon",
    "href": "lessons/16_Poisson_regression/week_10_sched.html#on-the-horizon",
    "title": "Week 10",
    "section": "On the Horizon",
    "text": "On the Horizon\n\nQuiz 3 open 6/3 at 2pm\n\nCloses on 6/5 at 1pm\n\nLab 4 due yesterday!\nHW 5 due 6/6 at 11pm\nProject report due 6/13 at 11pm"
  },
  {
    "objectID": "lessons/16_Poisson_regression/week_10_sched.html#announcements",
    "href": "lessons/16_Poisson_regression/week_10_sched.html#announcements",
    "title": "Week 10",
    "section": "Announcements",
    "text": "Announcements\n\nMonday 6/3\n\nIn the last stretch of the project\n\nClass time next week dedicated to project report help\nI may have other time slots for project help next week. I just need to take a serious look at my calendar\n\nI’m attending a virtual stat ed conference, so I just need to balance meetings with conference attendance\n\n\nNo office hours this Wednesday 6/5\n#3 “The Last Bounce: Bunnies, Burritos, and DIY Bath Salts”\n\nWednesday, June 5th Noon-2pm\nStudent Success Center, 6th floor Vanport\n\n\n\n\nWednesday 6/5\n\nLast day of lecture!!\nQuiz 2: added 3 points to everyone’s grade for that one confusing question (I forget the number, maybe question 6?)\nLab 4\n\nYour starting variables should be the TEN from lab 2, not the 5 from Lab 3\ng-value for Hosmer-Lemeshow test\n\nPlease look at Lesson 12!! There is a note on how to pick the g-value when we have many samples!!\nSome of us are getting NA’s when we put the correct g\n\nDouble check that your observed values are in numeric form\n\n\nIf doing LASSO, make sure you describe the process!\n\nWe used LASSO regression with a penalty of 0.001 to identify important predictors. We used a single test and training split of 80% and 20%, respectively. Only main effects were considered in our LASSO regression.\nAlways think: what are the key pieces of information that someone else might need to recreate this method?\n\n\nThe class will end on June 14, 2024. All coursework is expected to be completed by June 16, 2024 at 11pm.\n\nI need to have grades in on June 21st. In order to grade fairly and thoroughly, I need the whole week to grade the project report and any late assignments.\n\nA word on project grading\n\nIn the final lab, I gave you the option to do LASSO regression and focus on prediction. I know this created some confusion since we mostly set up the project as a question of association in Lab 1-3. We can still interpret the odds ratios from LASSO regression. I will be fairly lenient if reports are confused between prediction and association aims. I will try to give feedback on it, but I will not penalize any minor confusions.\nAnother word: My process starts harsh. I want to give you as much feedback as possible, and this will also reflect in lower assigned scores. At this point, I put the report grades into Sakai. I check to see if anyone’s overall course letter grade goes down. If less than ~5 course grades go down, then I revisit their project reports. If their report fails to demonstrate the most important learning objectives from the course, then I will keep the lower grade. If they demonstrate an understanding of the most important learning objectives, then I will adjust their score to increase their course grade. If more than 5 grades go down, then I revisit everyone’s reports. I will make a class wide grade bump in all reports.\nThe most important learning objectives are: understanding when and what test is appropriate, and interpreting odds ratios (from main effects and interactions)\n\nProject: LASSO\n\nYou can take the finalized formula for LASSO and use it in glm()\n\nIn this case, use the test data to come up with predictive values (like AUC)\nYou can run it on the full dataset to get the coefficient estimates and other diagnostic information\n\n\nGuide on figures"
  },
  {
    "objectID": "lessons/16_Poisson_regression/week_10_sched.html#class-exit-tickets",
    "href": "lessons/16_Poisson_regression/week_10_sched.html#class-exit-tickets",
    "title": "Week 10",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (6/3)\n Wednesday (6/5)"
  },
  {
    "objectID": "lessons/16_Poisson_regression/week_10_sched.html#muddiest-points",
    "href": "lessons/16_Poisson_regression/week_10_sched.html#muddiest-points",
    "title": "Week 10",
    "section": "Muddiest Points",
    "text": "Muddiest Points"
  },
  {
    "objectID": "lessons/17_Wrap_up/02_Intro_CDA_key_info.html#key-dates",
    "href": "lessons/17_Wrap_up/02_Intro_CDA_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/17_Wrap_up/02_Intro_CDA_key_info.html#last-class",
    "href": "lessons/17_Wrap_up/02_Intro_CDA_key_info.html#last-class",
    "title": "Key Info and Announcements",
    "section": "Last class",
    "text": "Last class"
  },
  {
    "objectID": "lessons/11_Interactions/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "href": "lessons/11_Interactions/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "title": "Muddy Points",
    "section": "Muddy Points from Spring 2024",
    "text": "Muddy Points from Spring 2024"
  },
  {
    "objectID": "lessons/11_Interactions/week_06_sched.html#resources",
    "href": "lessons/11_Interactions/week_06_sched.html#resources",
    "title": "Week 6",
    "section": "Resources",
    "text": "Resources\n\n\n\nLesson\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n10\nMultiple Logistic Regression\n\n\n\n\n\n11\nInteractions"
  },
  {
    "objectID": "lessons/11_Interactions/week_06_sched.html#on-the-horizon",
    "href": "lessons/11_Interactions/week_06_sched.html#on-the-horizon",
    "title": "Week 6",
    "section": "On the Horizon",
    "text": "On the Horizon\n\nHomework 3 due 5/9 at 11pm\nMid-quarter feedback due 5/9 at 11pm"
  },
  {
    "objectID": "lessons/11_Interactions/week_06_sched.html#class-exit-tickets",
    "href": "lessons/11_Interactions/week_06_sched.html#class-exit-tickets",
    "title": "Week 6",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (5/6)\n Wednesday (5/8)"
  },
  {
    "objectID": "lessons/11_Interactions/week_06_sched.html#announcements",
    "href": "lessons/11_Interactions/week_06_sched.html#announcements",
    "title": "Week 6",
    "section": "Announcements",
    "text": "Announcements\n\nMonday 5/6\n\nNicky’s office hours move to 3-4pm on Wednesdays\n\nI will stay after class and open Webex\n\nNotes on Homework 2\n\nContingency tables\n\nRev=”b”\n\nKnow the appropriate rev option\nriskratio() and oddsratio() reads first row as reference and first column as reference.\n\nQuestion 3 part h\n\nBoth a and b are correct\nMost ppl only marked one or the other\n\n\nWays to change outcome into factor or appropriate form for glm()\n\nicu$STA &lt;- ifelse(icu$STA==\"Died\",1,0)\nOR: icu = icu %&gt;% mutate(STA = as.factor(STA) %\\&gt;% relevel(ref = “Lived”))\n\nQuestion 4, part d\n\nTest for intercept wrong\nUsed coefficient for age\n\n\n\n\n\nWednesday 5/8\n\nLab 3 is up!"
  },
  {
    "objectID": "lessons/11_Interactions/week_06_sched.html#muddiest-points",
    "href": "lessons/11_Interactions/week_06_sched.html#muddiest-points",
    "title": "Week 6",
    "section": "Muddiest Points",
    "text": "Muddiest Points\n\n1. Why use logistic instead of linear regression?\nFrom waaay back in our slides from Lesson 5! We violate several of the assumptions for linear regression when our outcome has only two options. One of the most important reasons is that we cannot map our predictors (that can be continuous of categorical) onto a binary outcome. We need to transform our binary outcome so it is on a continuous and unbounded scale (logit does that!)\n\n\n2. Where can we find more resources for making forest plots?\nYou can either use the code I gave in Lesson 10 or use the reference links from Lab 4 in Linear Models.\n\n\n3. And how do we make a likelihood probability table instead of a plot?\nI’m not sure what this means. Please post on Slack so we can clarify! We have discussed plotting predicted probability in Lesson 7.\n\n\n4. I am confused on why we would do the odds ratio of prior fracture vs no prior fracture (in Lesson 11).\nThe odds ratio is mostly used to compare the two cases using one value. We want to compare the odds of a new fracture. We want to see how those odds of a new fracture change when we have a prior fracture or when we do not have a prior fracture. We can calculate the odds ratio for prior fracture to do that. When we only have main effects in our model, this is easier to calculate. We only have one odds ratio. However, when we have an interaction between prior fracture and age, we need a way to demonstrate how the odds ratio for prior fracture changes with age.\n\n\n5. I’m still confused about the difference between Model 3 and the estimated odd ratio table on slide 39 and what each is telling us.\nAh, yes! I’ll clarify a little more in the slide. The first table includes the coefficient estimates (\\(\\widehat\\beta_1\\),\\(\\widehat\\beta_2\\), \\(\\widehat\\beta_3\\)), and the second table includes the odds ratios (\\(\\exp(\\widehat\\beta_1)\\),\\(\\exp(\\widehat\\beta_2)\\), \\(\\exp(\\widehat\\beta_3)\\))"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT_muddy_points.html#muddy-points-from-spring-2024",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT_muddy_points.html#muddy-points-from-spring-2024",
    "title": "Muddy Points",
    "section": "Muddy Points from Spring 2024",
    "text": "Muddy Points from Spring 2024"
  },
  {
    "objectID": "lessons/15_Model_building/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "href": "lessons/15_Model_building/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "title": "Muddy Points",
    "section": "Muddy Points from Spring 2024",
    "text": "Muddy Points from Spring 2024"
  },
  {
    "objectID": "lessons/15_Model_building/week_08_sched.html",
    "href": "lessons/15_Model_building/week_08_sched.html",
    "title": "Week 8",
    "section": "",
    "text": "Room Locations for the week\n\n\n\nOn Wednesday, 5/22, we will be in RPV Room A (1217)"
  },
  {
    "objectID": "lessons/15_Model_building/week_08_sched.html#resources",
    "href": "lessons/15_Model_building/week_08_sched.html#resources",
    "title": "Week 8",
    "section": "Resources",
    "text": "Resources\n\n\n\nLesson\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n14\nModel Diagnostics\n\n\n\n\n\n15\nModel Building"
  },
  {
    "objectID": "lessons/15_Model_building/week_08_sched.html#announcements",
    "href": "lessons/15_Model_building/week_08_sched.html#announcements",
    "title": "Week 8",
    "section": "Announcements",
    "text": "Announcements\n\nMonday 5/20\n\nHW 4 part d UPDATED!!\n\nHelp from Monday class on Part e\n\nHomework 3\n\nRemember to include the indicator function for different categories of your variables!!\nLOC has three levels: there should be two indicator functions and two coefficients for this variable!!\n\n\n\n\n\nWednesday 5/22\n\nLab 3\n\nWhen interpreting ORs…\n\nYou all are correct by including as much detail about the covariate as possible\n\nFor example: If I was using UNMETCARE_Y and I wrote “The estimated odds of food insecurity for individuals who needed medical care in the last 12 months but could not get it because they could not afford it…”\n\nThis is correct!\nBUT within our longer written report, we should define “unmet care” earlier on. Thus, once we get to interpreting ORs, we can just say “unmet care.”\n\n\nAlso, correct for including a list of the variables that you are adjusting for!\n\nBut again, we hopefully defined our final model and specifically mentioned the variables that we are adjusting for\nThus, once we get to our interpretation, we can say something more like “adjusting for the previously listed variables in our model”\n\n\nFor output of `tbl_regression()` make sure to edit the variable names into more common language\n\nset_variable_labels() from tibbleOne package might help!"
  },
  {
    "objectID": "lessons/15_Model_building/week_08_sched.html#class-exit-tickets",
    "href": "lessons/15_Model_building/week_08_sched.html#class-exit-tickets",
    "title": "Week 8",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (5/20)\n Wednesday (5/22)"
  },
  {
    "objectID": "lessons/15_Model_building/week_08_sched.html#muddiest-points-questions",
    "href": "lessons/15_Model_building/week_08_sched.html#muddiest-points-questions",
    "title": "Week 8",
    "section": "Muddiest Points / Questions",
    "text": "Muddiest Points / Questions\n\n1. How did you determine the ages for the R output on slide 24 (standardized deviance residuals)\nThe centered ages are centered around the mean age. A few classes ago I mentioned that the mean was 69 years old, might have gotten lost in this lesson. So calculating the actual ages is just adding the mean age and centered age. So centered age of 6 is 69+6 = 75. Also, very confusing because apparently I can’t add!\n\n\n2. From comment on shrinkage vs. regularization vs. penalized methods\nAll these terms are used intercahngeably!\nPenalized regression means that penalty is added to our likelihood function! This may feel like a more generic form of shrinkage or regularization. However, within statistics, I do not see penalized regression used for anything other than minimizing the coefficient values towards zero. I often see it defined as: form of regression that uses a penalty to shrink coefficients towards zero.\nDefinitions of regularized regression mirror the above for penalized regression.\nShrinkage is more the action of reducing coefficient values towards zero. Many people will refer to regularization and penalized regression as shrinkage methods.\n\nLASSO, ridge, and Elastic net are all types of penalized/regularization/shrinkage methods\n\n\n\n3. Sign column within vi() output\nThe sign column is in fact the sign of the coefficient within the model.\nSo within our interaction model, the sign for smoking status is negative. Since smoking status had many interactions, we cannot make claims about the association between smoking and fracture without considering all other variables that it interacts with. ALSO, remember that our goal here is prediction, NOT association."
  },
  {
    "objectID": "lessons/07_Pred_Viz/02_Intro_CDA_key_info.html#key-dates",
    "href": "lessons/07_Pred_Viz/02_Intro_CDA_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/07_Pred_Viz/02_Intro_CDA_key_info.html#last-class",
    "href": "lessons/07_Pred_Viz/02_Intro_CDA_key_info.html#last-class",
    "title": "Key Info and Announcements",
    "section": "Last class",
    "text": "Last class"
  },
  {
    "objectID": "lessons/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree_muddy_points.html#muddy-points-from-spring-2024",
    "href": "lessons/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree_muddy_points.html#muddy-points-from-spring-2024",
    "title": "Muddy Points",
    "section": "Muddy Points from Spring 2024",
    "text": "Muddy Points from Spring 2024"
  },
  {
    "objectID": "lessons/04_Meas_Assoc_Agree/week_02_sched.html#resources",
    "href": "lessons/04_Meas_Assoc_Agree/week_02_sched.html#resources",
    "title": "Week 2",
    "section": "Resources",
    "text": "Resources\n\n\n\nLesson\n3\nTopic\nMeasurement of Association for Contingency Tables\nSlides | Annotated Slides | Recording | :==============================================================================================================================:+:=================================================================================================================================================:+:====================================================================================================================================================================================================================================================:+  |  | \n\n\n\n\n\n\n\n4\nMeasurements of Association and Agreement\n |  |  | | | | |  |\n\n\n\nIf you ever have trouble with the above links to the videos, this Echo360 link should take you to our class page.\nFor the slides, once they are opened, if you would like to print or save them as a PDF, the best way to do this is from a computer internet browser:\n\nClick on the icon with three horizontal bars on the bottom left of the browser.\nClick on “Tools” with the gear icon at the top of the sidebar.\nClick on “PDF Export Mode.”\nFrom there, you can print or save the PDF as you would normally from your internet browser.\n\nNote: this process does not work very well on an iPad."
  },
  {
    "objectID": "lessons/04_Meas_Assoc_Agree/week_02_sched.html#on-the-horizon",
    "href": "lessons/04_Meas_Assoc_Agree/week_02_sched.html#on-the-horizon",
    "title": "Week 2",
    "section": "On the Horizon",
    "text": "On the Horizon\n\nHomework 1 due this Thursday!"
  },
  {
    "objectID": "lessons/04_Meas_Assoc_Agree/week_02_sched.html#announcements",
    "href": "lessons/04_Meas_Assoc_Agree/week_02_sched.html#announcements",
    "title": "Week 2",
    "section": "Announcements",
    "text": "Announcements\n\nMonday 4/8\n\nOffice hours!!\n\nTuesdays 5:30-7pm with Antara\nThursdays 3:30 - 5pm with Ariel\nFridays 2 - 3:30pm with Nicky\n\n\n\n\nWednesday 4/10\n\nEcho360: Let’s all double check that we can see the recordings\n\nLink to class site\n\nHomework question 5: no need to do LRT in the table\nLab 1 is up!!\nQuiz 1 decision\n\nOnline in Sakai\nWill open up on Monday at 2pm. You can chose to take it in the classroom or wait\nQuiz will close before class on Wednesday\nOpen book still\nPlease do not cheat\n\nIf I notice any unusual changes to quiz performance compared to last quarter then we will go back to the old way of giving quizzes\n\nMultiple choice with potentially some free response\n\nFor example: interpreting an OR would be divided into a multiple choice for the estimate, CI, and then writing a sentence to interpret the estimate"
  },
  {
    "objectID": "lessons/04_Meas_Assoc_Agree/week_02_sched.html#class-exit-tickets",
    "href": "lessons/04_Meas_Assoc_Agree/week_02_sched.html#class-exit-tickets",
    "title": "Week 2",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (4/8)\n Wednesday (4/10)"
  },
  {
    "objectID": "lessons/04_Meas_Assoc_Agree/week_02_sched.html#muddiest-points",
    "href": "lessons/04_Meas_Assoc_Agree/week_02_sched.html#muddiest-points",
    "title": "Week 2",
    "section": "Muddiest Points",
    "text": "Muddiest Points\n\n1. “times greater than” vs just “times” in interpretation\nI’ve seen it both ways. It comes down to more of an English language nuance, with what seems to be a long battle between viewpoints. Or maybe more accurately, I grammatically correct way to contruct the sentence, but with people understanding the meaning the “incorrect” way. I tend to be more lenient when it comes to grammar in this way, but maybe that’s because I have a general distaste when languages are rigid and don’t accommodate how people currently speak and write.\n\n\n2. For the relative risks poll everywhere question #2, how were they derived?\n\nFor #1 with Trt A’s risk as 0.01 (aka \\(risk_A=0.01\\)) and Trt B’s risk as 0.001 (aka \\(risk_B=0.01\\))\n\nThe risk ratio is: \\(\\widehat{RR} = \\dfrac{\\widehat{p}_1}{\\widehat{p}_2} = \\dfrac{risk_A}{risk_B} = \\dfrac{0.01}{0.001} = 10\\)\n\nFor #2 with Trt A’s risk as 0.41 (aka \\(risk_A=0.41\\)) and Trt B’s risk as 0.401 (aka \\(risk_B=0.401\\))\n\nThe risk ratio is: \\(\\widehat{RR} = \\dfrac{\\widehat{p}_1}{\\widehat{p}_2} = \\dfrac{risk_A}{risk_B} = \\dfrac{0.41}{0.401} = 1.02\\)\n\n\n\n\n3. Ranges that odds ratios can take (0, infinity) vs the ranges that risk ratios can take.\nYeah, so both can theoretically take on the range \\([0, \\infty)\\). Both are ratios, so we also have to think about the range of the denominator and numerator For relative risk, the numerator and denominator are probabilities that can only take values from 0 to 1. While for ORs, the denominator and numerator are odds that can be a range of values \\([0, \\infty)\\).\nThe main point I was trying to make was that once we observe one group’s proportion/probability, then RRs and ORs will differ in their potential range. Let’s say I observe the proportion fro group 1 and now know the numerator for RR and the odds in the numerator for OR. Because the RR has numerator and denominator that has ranges \\([0, 1]\\), if we know the proportion of group 1 (aka numerator value), then the ratio itself has a smaller range of values because the denominator can only be between 0 and 1. Because the OR has numerator and denominator that has ranges \\([0, \\infty)\\), if we know the proportion of group 1, then we do have a fixed numerator. However, the denominator can still be in \\([0, \\infty)\\).\n\n\n4. For the odds ratio equation that we reviewed today, is it different from ad/bc ? If they are different, when is it appropriate to use the equation we just reviewed over the other? p1/(1-p1) / p2/(1-p2)\nNope! These are the same! If you learned it that way, you can definitely use it when we are working with contingency tables. However, once we move into ORs from regression with multiple covariates, I think it’s better to understand the ORs and odds in terms of the probability/proportion.\n\n\n5. Not directly related to this class, but did we cover LRTs already? They’re mentioned in HW1 but aren’t in my notes.\nOops! Fixed it in the HW. No need to do anything with LRTs!\n\n\n6. In Epi, we were very strictly told that Odds Ratios were only to be used in one type of study. (I.e. we CAN NOT use them in cross-sectional and cohort studies) only case-control. So what is the application of attempting to utilize them, if each respective type of study already has a “pre-assigned” statistical method that suits it best?\nOdds ratios CAN be used in cross-sectional AND cohort studies. It is often an over-estimate of the relative risk in those situations, so it is important to interpret it ONLY as the odds ratio.\nEach respective study does not have a pre-assigned method. The only restriction is that relative risk cannot be used in case-control studies."
  },
  {
    "objectID": "lessons/08_Interpretations_SLR/02_Intro_CDA_key_info.html#key-dates",
    "href": "lessons/08_Interpretations_SLR/02_Intro_CDA_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/08_Interpretations_SLR/02_Intro_CDA_key_info.html#last-class",
    "href": "lessons/08_Interpretations_SLR/02_Intro_CDA_key_info.html#last-class",
    "title": "Key Info and Announcements",
    "section": "Last class",
    "text": "Last class"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "href": "lessons/10_Multiple_logistic_regression/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "title": "Muddy Points",
    "section": "Muddy Points from Spring 2024",
    "text": "Muddy Points from Spring 2024"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/week_06_sched.html#resources",
    "href": "lessons/10_Multiple_logistic_regression/week_06_sched.html#resources",
    "title": "Week 6",
    "section": "Resources",
    "text": "Resources\n\n\n\nLesson\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n10\nMultiple Logistic Regression\n\n\n\n\n\n11\nInteractions"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/week_06_sched.html#on-the-horizon",
    "href": "lessons/10_Multiple_logistic_regression/week_06_sched.html#on-the-horizon",
    "title": "Week 6",
    "section": "On the Horizon",
    "text": "On the Horizon\n\nHomework 3 due 5/9 at 11pm\nMid-quarter feedback due 5/9 at 11pm"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/week_06_sched.html#class-exit-tickets",
    "href": "lessons/10_Multiple_logistic_regression/week_06_sched.html#class-exit-tickets",
    "title": "Week 6",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (5/6)\n Wednesday (5/8)"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/week_06_sched.html#announcements",
    "href": "lessons/10_Multiple_logistic_regression/week_06_sched.html#announcements",
    "title": "Week 6",
    "section": "Announcements",
    "text": "Announcements\n\nMonday 5/6\n\nNicky’s office hours move to 3-4pm on Wednesdays\n\nI will stay after class and open Webex\n\nNotes on Homework 2\n\nContingency tables\n\nRev=”b”\n\nKnow the appropriate rev option\nriskratio() and oddsratio() reads first row as reference and first column as reference.\n\nQuestion 3 part h\n\nBoth a and b are correct\nMost ppl only marked one or the other\n\n\nWays to change outcome into factor or appropriate form for glm()\n\nicu$STA &lt;- ifelse(icu$STA==\"Died\",1,0)\nOR: icu = icu %&gt;% mutate(STA = as.factor(STA) %\\&gt;% relevel(ref = “Lived”))\n\nQuestion 4, part d\n\nTest for intercept wrong\nUsed coefficient for age\n\n\n\n\n\nWednesday 5/8\n\nLab 3 is up!"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/week_06_sched.html#muddiest-points",
    "href": "lessons/10_Multiple_logistic_regression/week_06_sched.html#muddiest-points",
    "title": "Week 6",
    "section": "Muddiest Points",
    "text": "Muddiest Points\n\n1. Why use logistic instead of linear regression?\nFrom waaay back in our slides from Lesson 5! We violate several of the assumptions for linear regression when our outcome has only two options. One of the most important reasons is that we cannot map our predictors (that can be continuous of categorical) onto a binary outcome. We need to transform our binary outcome so it is on a continuous and unbounded scale (logit does that!)\n\n\n2. Where can we find more resources for making forest plots?\nYou can either use the code I gave in Lesson 10 or use the reference links from Lab 4 in Linear Models.\n\n\n3. And how do we make a likelihood probability table instead of a plot?\nI’m not sure what this means. Please post on Slack so we can clarify! We have discussed plotting predicted probability in Lesson 7.\n\n\n4. I am confused on why we would do the odds ratio of prior fracture vs no prior fracture (in Lesson 11).\nThe odds ratio is mostly used to compare the two cases using one value. We want to compare the odds of a new fracture. We want to see how those odds of a new fracture change when we have a prior fracture or when we do not have a prior fracture. We can calculate the odds ratio for prior fracture to do that. When we only have main effects in our model, this is easier to calculate. We only have one odds ratio. However, when we have an interaction between prior fracture and age, we need a way to demonstrate how the odds ratio for prior fracture changes with age.\n\n\n5. I’m still confused about the difference between Model 3 and the estimated odd ratio table on slide 39 and what each is telling us.\nAh, yes! I’ll clarify a little more in the slide. The first table includes the coefficient estimates (\\(\\widehat\\beta_1\\),\\(\\widehat\\beta_2\\), \\(\\widehat\\beta_3\\)), and the second table includes the odds ratios (\\(\\exp(\\widehat\\beta_1)\\),\\(\\exp(\\widehat\\beta_2)\\), \\(\\exp(\\widehat\\beta_3)\\))"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/week_07_sched.html#resources",
    "href": "lessons/13_Numeric_Problems/week_07_sched.html#resources",
    "title": "Week 7",
    "section": "Resources",
    "text": "Resources\n\n\n\nLesson\n12\nTopic\nAssessing Model Fit\nSlides | Annotated Slides | Recording | :==============================================================================================================================:+:=================================================================================================================================================:+:======================================================================================================================================:+  |  | \n\n\n\n\n\n\n\n13\nNumerical Problems\n |  |  | | | pdf on github | | |"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/week_07_sched.html#on-the-horizon",
    "href": "lessons/13_Numeric_Problems/week_07_sched.html#on-the-horizon",
    "title": "Week 7",
    "section": "On the Horizon",
    "text": "On the Horizon"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/week_07_sched.html#class-exit-tickets",
    "href": "lessons/13_Numeric_Problems/week_07_sched.html#class-exit-tickets",
    "title": "Week 7",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (5/13)\nQuiz starts on Wednesday (5/15)"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/week_07_sched.html#announcements",
    "href": "lessons/13_Numeric_Problems/week_07_sched.html#announcements",
    "title": "Week 7",
    "section": "Announcements",
    "text": "Announcements\n\nMonday 5/13\n\nI will review our interaction notes from last time\nQuiz 2 opens on Wednesday at 2pm!!\n\nWill cover Lessons 5-9!!\nMostly on Lessons 5-8, with one question about concepts covered in Lesson 9 (like types of missing data…)\n\nThink about how we fit logistic regression and GLMs\nThink about link functions!\nThink about our tests (Wald, Score, LRT)\nThink about the different transformations between Y, probability, and logit!\nThink about what our predictions mean\nThink about interpretations within logistic regression.\n\n\nLab 2\n\nI noticed that a lot of you did not comment on the trends from the bivariate analysis\n\nThis is why I didn’t have us use ggpairs() last quarter. It’s too easy to just blow past this.\nGetting to know your data and the trends you see in the sample is incredibly important!!\nThe best way to identify issues with your model is to have a good understanding of your data and their trends\n\nMany people noticed small cell counts for income levels\n\nWe will address this issues in lecture this week!\n\nUnless you are removing food insecurity from any hardship or multiple hardships, do NOT use these variables!"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/week_07_sched.html#muddiest-points",
    "href": "lessons/13_Numeric_Problems/week_07_sched.html#muddiest-points",
    "title": "Week 7",
    "section": "Muddiest Points",
    "text": "Muddiest Points\n\n1. How do we determine the number of covariate patterns in R?\nTheoretically, all you need to do is count the number of groups in each categorical covariates. To find the total number of covariate patterns, you multiple those numbers by each other.\nIn R, we can take a dataframe with only the predictors in your model. You can use the distinct() function to create unique rows. The number of rows outputted will be the number of covariate patterns."
  },
  {
    "objectID": "lessons/06_Tests_GLMs/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "href": "lessons/06_Tests_GLMs/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "title": "Muddy Points",
    "section": "Muddy Points from Spring 2024",
    "text": "Muddy Points from Spring 2024"
  },
  {
    "objectID": "lessons/06_Tests_GLMs/week_03_sched.html#resources",
    "href": "lessons/06_Tests_GLMs/week_03_sched.html#resources",
    "title": "Week 3",
    "section": "Resources",
    "text": "Resources\n\n\n\nLesson\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n5\nSimple Logistic Regression\n\n\n\n\n\n6\nTests for GLMs using Likelihood function\n\n\n\n\n\n\nIf you ever have trouble with the above links to the videos, this Echo360 link should take you to our class page."
  },
  {
    "objectID": "lessons/06_Tests_GLMs/week_03_sched.html#on-the-horizon",
    "href": "lessons/06_Tests_GLMs/week_03_sched.html#on-the-horizon",
    "title": "Week 3",
    "section": "On the Horizon",
    "text": "On the Horizon\n\nLab 1 due this Thursday (4/18)\nQuiz 1 opens on Monday, 4/22, at 2pm and will close on Wednesday, 4/24, at 1pm"
  },
  {
    "objectID": "lessons/06_Tests_GLMs/week_03_sched.html#class-exit-tickets",
    "href": "lessons/06_Tests_GLMs/week_03_sched.html#class-exit-tickets",
    "title": "Week 3",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (4/15)\n Wednesday (4/17)"
  },
  {
    "objectID": "lessons/06_Tests_GLMs/week_03_sched.html#announcements",
    "href": "lessons/06_Tests_GLMs/week_03_sched.html#announcements",
    "title": "Week 3",
    "section": "Announcements",
    "text": "Announcements\n\nMonday 4/15\n\nI am trying to stay on track of the Exit tickets this quarter\n\nThat may mean you have a 0 in your gradebook\nAs long as you complete the exit ticket within the 7 days, I will change the 0 to a 1\nI plan to have a scheduled block on Fridays to check them\n\n\n\n\nWednesday 4/17\n\nQuiz 1 info"
  },
  {
    "objectID": "lessons/06_Tests_GLMs/week_03_sched.html#muddiest-points",
    "href": "lessons/06_Tests_GLMs/week_03_sched.html#muddiest-points",
    "title": "Week 3",
    "section": "Muddiest Points",
    "text": "Muddiest Points\n\n1. Not entirely sure I understand what IRLS is about\nFair enough. It’s a little confusing. IWLS is an iterative solving technique that let’s us solve the coefficient estimates ( \\(\\beta_0\\) , \\(\\beta_1\\)) without solving the equations theoretically.\nWe start with an educated guess of the estimates, put them into the likelihood, and calculate the likelihood. Then we update the estimates using some complicated math, put them into the likelihood, and calculate the likelihood again. We compare the two likelihoods, and if the likelihood increases, then we keep going. We stop when the increase in likelihood between iterations is small. This means we are at or very close to the maximum likelihood.\n\n\n2. Link functions\nYes! Link functions are the important transformations we need to make to our outcome in order to connect them to our perdictors/covariates. Specifically, it’s the transformation we make to our mean/expected value.\nThe same link function can be used different types of outcomes. And here’s a few examples:\n\nContinuous data: identity\nBinary: logit, log\nCount/Poisson: log\n\nOur goal with link functions is to put our outcome on a flexible range so that any range of covariates can be mapped to it with coefficients. So think about trying to map age onto a 0 or 1… We can’t come up with an equation like \\(\\beta_0 + \\beta_1 Age\\) that perfectly maps to only 0’s and 1’s.\n\n\n3. Is GLM the umbrella over the other functions? The 4 functions all use different distributions, yes?\nGLM is the umbrella term for different types of regression! Not all types of regression have different outcome distributions. For example, a binary outcome can be used in logistic regression with the logit link or log-binomial regression with the log link.\n\n\n4. What would you need to change in your model to reduce a high IRLS number? As I understand it from the lecture, a high number suggests convergence but it appeared like something unfavorable even though a model that converges might be closer to maximum likelihood or maybe the distance to maximum likelihood\nA high number suggests that the model did NOT converge! Thus, we did not land on an estimate close to our maximum likelihood. You can think of the IRLS number as the number of iterations it is taking to find the maximum likelihood estimate (MLE). If it takes too many iterations, then it just stops without finding the MLE.\n\n\n5. We’re using linear vs logistic, but which are we focusing on? Regarding linear, how does linear used in categorical differ from continuous?\nWe are focusing on logistic! We cannot use linear regression on our binary outcomes anymore. When I say “linear” mapping I mean the mapping between our covariates and the transformed mean outcome using the link function.\n\n\n6. By the end of class (Lesson 6) my understanding is that the saturated model likelihood is the same between the two models being compared, right?\nYep!!\n\n\n7. The differences between each test and when to use them.\nIn terms of what each test is measuring:\n\nThe Wald test measures the distance between two potential values of \\(\\beta\\). One under the null and one under the alternative. The further they are from each other, the more evidence we have that they are different.\n\nThe Wald test approximates the differences in the likelihood function, but we do not actually compare the likelihoods under the null vs. alternative. We are only comparing the difference in the \\(\\beta\\) value, that is a reasonable approximation of the difference in the likelihood.\n\nThe Score test measures how close the tangent line of the likelihood function is to 0 (under the null). If it is close to 0 under the null, this indicates that our MLE of \\(\\beta\\) is not far from 0. Again, this is no a direct comparison of the likelihoods, but only an approximation of the difference.\nThe likelihood ratio test measures the difference in the log-likelihoods. This is a direct comparison of likelihoods, and is not an approximation!\n\nThus, we compare the likelihoods (horizontally, as someone asked) because we are making direct comparisons between the likelihood under the null and under the alternative."
  },
  {
    "objectID": "lessons/05_Simple_logistic_reg/05_Simple_logistic_reg_muddy_points.html#muddy-points-from-spring-2024",
    "href": "lessons/05_Simple_logistic_reg/05_Simple_logistic_reg_muddy_points.html#muddy-points-from-spring-2024",
    "title": "Muddy Points",
    "section": "Muddy Points from Spring 2024",
    "text": "Muddy Points from Spring 2024"
  },
  {
    "objectID": "lessons/12_Assessing_fit/week_07_sched.html#resources",
    "href": "lessons/12_Assessing_fit/week_07_sched.html#resources",
    "title": "Week 7",
    "section": "Resources",
    "text": "Resources\n\n\n\nLesson\n12\nTopic\nAssessing Model Fit\nSlides | Annotated Slides | Recording | :==============================================================================================================================:+:=================================================================================================================================================:+:======================================================================================================================================:+  |  | \n\n\n\n\n\n\n\n13\nNumerical Problems\n |  |  | | | pdf on github | | |"
  },
  {
    "objectID": "lessons/12_Assessing_fit/week_07_sched.html#on-the-horizon",
    "href": "lessons/12_Assessing_fit/week_07_sched.html#on-the-horizon",
    "title": "Week 7",
    "section": "On the Horizon",
    "text": "On the Horizon"
  },
  {
    "objectID": "lessons/12_Assessing_fit/week_07_sched.html#class-exit-tickets",
    "href": "lessons/12_Assessing_fit/week_07_sched.html#class-exit-tickets",
    "title": "Week 7",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (5/13)\nQuiz starts on Wednesday (5/15)"
  },
  {
    "objectID": "lessons/12_Assessing_fit/week_07_sched.html#announcements",
    "href": "lessons/12_Assessing_fit/week_07_sched.html#announcements",
    "title": "Week 7",
    "section": "Announcements",
    "text": "Announcements\n\nMonday 5/13\n\nI will review our interaction notes from last time\nQuiz 2 opens on Wednesday at 2pm!!\n\nWill cover Lessons 5-9!!\nMostly on Lessons 5-8, with one question about concepts covered in Lesson 9 (like types of missing data…)\n\nThink about how we fit logistic regression and GLMs\nThink about link functions!\nThink about our tests (Wald, Score, LRT)\nThink about the different transformations between Y, probability, and logit!\nThink about what our predictions mean\nThink about interpretations within logistic regression.\n\n\nLab 2\n\nI noticed that a lot of you did not comment on the trends from the bivariate analysis\n\nThis is why I didn’t have us use ggpairs() last quarter. It’s too easy to just blow past this.\nGetting to know your data and the trends you see in the sample is incredibly important!!\nThe best way to identify issues with your model is to have a good understanding of your data and their trends\n\nMany people noticed small cell counts for income levels\n\nWe will address this issues in lecture this week!\n\nUnless you are removing food insecurity from any hardship or multiple hardships, do NOT use these variables!"
  },
  {
    "objectID": "lessons/12_Assessing_fit/week_07_sched.html#muddiest-points",
    "href": "lessons/12_Assessing_fit/week_07_sched.html#muddiest-points",
    "title": "Week 7",
    "section": "Muddiest Points",
    "text": "Muddiest Points\n\n1. How do we determine the number of covariate patterns in R?\nTheoretically, all you need to do is count the number of groups in each categorical covariates. To find the total number of covariate patterns, you multiple those numbers by each other.\nIn R, we can take a dataframe with only the predictors in your model. You can use the distinct() function to create unique rows. The number of rows outputted will be the number of covariate patterns."
  },
  {
    "objectID": "lessons/09_Missing_data/week_05_sched.html",
    "href": "lessons/09_Missing_data/week_05_sched.html",
    "title": "Week 5",
    "section": "",
    "text": "MONDAY CLASS CANCELLED\n\n\n\nMonday’s class is cancelled! We will be back on Wednesday for a fun, random lecture on missing data!!"
  },
  {
    "objectID": "lessons/09_Missing_data/week_05_sched.html#resources",
    "href": "lessons/09_Missing_data/week_05_sched.html#resources",
    "title": "Week 5",
    "section": "Resources",
    "text": "Resources\n\n\n\nLesson\n9\nTopic\nMissing Data\nActivity:\nSlides | Annotated Slides | Recording | :==========================================================================================================================:+:=========================================================================================================================================:+:======================================================================================================================================:+  |  |  | | |  | | |"
  },
  {
    "objectID": "lessons/09_Missing_data/week_05_sched.html#on-the-horizon",
    "href": "lessons/09_Missing_data/week_05_sched.html#on-the-horizon",
    "title": "Week 5",
    "section": "On the Horizon",
    "text": "On the Horizon\n\nLab 2 due 5/2 at 11pm\nHW 3 is up and due 5/9 at 11pm!"
  },
  {
    "objectID": "lessons/09_Missing_data/week_05_sched.html#class-exit-tickets",
    "href": "lessons/09_Missing_data/week_05_sched.html#class-exit-tickets",
    "title": "Week 5",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Wednesday (5/1)"
  },
  {
    "objectID": "lessons/09_Missing_data/week_05_sched.html#announcements",
    "href": "lessons/09_Missing_data/week_05_sched.html#announcements",
    "title": "Week 5",
    "section": "Announcements",
    "text": "Announcements\n\nWednesday 5/1\n\nQuiz 1 grades are up!\nSPH Student survey still going until 5/3!!\n\nIf you already did it, then yay!\nRaffling 2 iPads!\n\nOur mid quarter survey is up!\n\nDUE 5/9 at 11pm!!"
  },
  {
    "objectID": "lessons/09_Missing_data/week_05_sched.html#muddiest-points",
    "href": "lessons/09_Missing_data/week_05_sched.html#muddiest-points",
    "title": "Week 5",
    "section": "Muddiest Points",
    "text": "Muddiest Points\nI couldn’t resist making a gif of me reacting to the fire alarm when I was editing the Echo360 recording."
  },
  {
    "objectID": "lessons/01_Intro/01_Intro.html#check-the-echo360-link",
    "href": "lessons/01_Intro/01_Intro.html#check-the-echo360-link",
    "title": "Lesson 1: Welcome!",
    "section": "Check the Echo360 link",
    "text": "Check the Echo360 link\n\nLet’s check the link and make sure we can see everything!"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#what-is-categorical-data-analysis",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#what-is-categorical-data-analysis",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "What is Categorical Data Analysis?",
    "text": "What is Categorical Data Analysis?\n\nIn BSTA 512/612 (linear regression), we focused on continuous responses/outcomes\n\nWe included categorical variables only as covariates (aka predictors, independent variables, explanatory variables)\nExamples from 512/612: life expectancy (in years), IAT score (ranging from -2 to 2)\n\n\n   \n\nCategorical data analysis focuses on the statistical methods for categorical responses/outcomes\n\nExplanatory (or ‘independent’) variable can be of any type (continuous or categorical)"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#types-of-variables",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#types-of-variables",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Types of Variables",
    "text": "Types of Variables"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#types-of-variables-outcomes-we-will-cover-in-this-course",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#types-of-variables-outcomes-we-will-cover-in-this-course",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Types of Variables: Outcomes we will cover in this course",
    "text": "Types of Variables: Outcomes we will cover in this course"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#what-does-this-course-cover",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#what-does-this-course-cover",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "What does this course cover?",
    "text": "What does this course cover?\n\nStrategies for assessing association between categorical response variable and a one explanatory variable\n\nHypothesis testing\nMeasure of association\nSimple logistic regression\n\n\n   \n\nStatistical modeling strategies for assessing association between the categorical response variable and a set of explanatory variables\n\nLogistic regression\n\nFor binary, ordinal, and multinomial outcomes\n\nPoisson regression\n\nFor counts outcomes"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#poll-everywhere-question-1",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#poll-everywhere-question-1",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Poll everywhere question 1",
    "text": "Poll everywhere question 1"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#binomial-distribution",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#binomial-distribution",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\nConsider a sample of \\(n\\) independent trials, each of which can have only two possible outcomes (“success” and “failure”)\nFor each trial: \\[\\begin{align} P( \\text{success}) & = p \\\\\n                  P( \\text{failure}) & = 1- p = q \\end{align}\\]\nBinomial distribution: distribution of the number of successes in n independent trials\nThe probability mass function for the binomial distribution is: \\[P(X=k) = {n \\choose k} p^k q^{n-k}, \\text{ for } k = 0, 1, ..., n\\]\n\n\\(E(X) = np\\)\n\\(Var(X) = npq = np(1-p)\\)"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#binomial-distribution-1",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#binomial-distribution-1",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\nConsider a sample of \\(n\\) independent trials, each of which can have only two possible outcomes (“success” and “failure”)\nFor each trial: \\[\\begin{align} P( \\text{success}) & = p \\\\\n                  P( \\text{failure}) & = 1- p = q \\end{align}\\]\nBinomial distribution: distribution of the number of successes in n independent trials\nThe probability mass function for the binomial distribution is: \\[P(X=k) = {n \\choose k} p^k q^{n-k}, \\text{ for } k = 0, 1, ..., n\\]\n\n\\(E(X) = np\\)\n\\(Var(X) = npq = np(1-p)\\)"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#binomial-distribution-r-commands",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#binomial-distribution-r-commands",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Binomial Distribution: R commands",
    "text": "Binomial Distribution: R commands\nR commands with their input and output:\n\n\n\n\n\n\n\nR code\nWhat does it return?\n\n\n\n\nrbinom()\nreturns sample of random variables with specified binomial distribution\n\n\ndbinom()\nreturns probability of getting certain number of successes\n\n\npbinom()\nreturns cumulative probability of getting certain number or less successes\n\n\nqbinom()\nreturns number of successes corresponding to desired quantile"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#binomial-distribution-example",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#binomial-distribution-example",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Binomial Distribution Example",
    "text": "Binomial Distribution Example\n\n\nExample\n\n\nIf the probability that one white blood cell is a lymphocyte is 0.2, compute the probability of 2 lymphocytes out of 10 white blood cells\n\n\n\\[P(X=2) = {10 \\choose 2} 0.2^2 (1-0.2)^{10-2}  = 0.3020\\]\n\ndbinom(2, 10, 0.2) %&gt;% round(4)\n\n[1] 0.302"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#normal-approximation-of-the-binomial-distribution",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#normal-approximation-of-the-binomial-distribution",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Normal Approximation of the Binomial Distribution",
    "text": "Normal Approximation of the Binomial Distribution\n\nAlso known as: Sampling distribution of \\(\\widehat{p}\\)\nIF \\(X\\sim \\text{Binomial}(n,p)\\) and \\(np&gt;10\\) and \\(nq = n(1-p) &gt; 10\\)\n\nEnsures sample size (\\(n\\)) is moderately large and the \\(p\\) is not too close to 0 or 1\nOther resources use other criteria (like \\(npq&gt;5\\) or \\(np&gt;5\\))\nWhen looking at a sample, we use \\(\\widehat{p}\\) instead of \\(p\\) to check this!!\n\n\n \n\nTHEN approximately \\(𝑋\\sim \\text{Normal}\\big(\\mu_X = np, \\sigma_X = \\sqrt{np(1-p)} \\big)\\)\n\nOr we often write this as the sampling distribution of \\(\\widehat{p}\\): \\[\\widehat{p} \\sim \\text{Normal}\\bigg(\\mu_{\\widehat{p}} = p, \\sigma_{\\widehat{p}} = \\sqrt{\\dfrac{p(1-p)}{n}}\\bigg)\\]\n\nPretty good video behind the intuition of this (Watch 00:00 - 05:40)"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#estimate-and-confidence-interval-of-single-proportion",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#estimate-and-confidence-interval-of-single-proportion",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Estimate and Confidence Interval of Single Proportion",
    "text": "Estimate and Confidence Interval of Single Proportion\n\nEstimate of proportion:\n\n\\[\n\\widehat{p} = \\dfrac{\\# \\text{successes}}{\\# \\text{successes} + \\# \\text{failures}}\n\\]\n\nUse the sampling distribution of \\(\\widehat{p}\\) to construct the confidence interval:\n\n\\((1-\\alpha)\\%\\) confidence interval for estimate proportion:\n\n\n\\[\\begin{align} \\widehat{p} &\\pm z^*_{(1-\\alpha/2)} \\cdot SE_{\\hat{p}} \\\\ \\widehat{p} &\\pm z^*_{(1-\\alpha/2)} \\cdot \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\end{align}\\]\n\nUsing \\(SE_{\\hat{p}} = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\) - instead of \\(\\sigma_{p} = \\sqrt{\\frac{p(1-p)}{n}}\\) - because we don’t know exactly what \\(p\\) is"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#section",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#section",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "",
    "text": "Smoking status example\n\n\nA cross-sectional study of 8681 patients was conducted to evaluate the nature of smoking status among people. Of the 8681 people, 4840 were nonsmokers and 3841 were smokers.\n\n\nNeeded steps:\n\nEstimate proportion \\(\\widehat{p}\\)\nCheck that \\(n\\widehat{p}&gt;10\\) and \\(n(1-\\widehat{p})&gt;10\\) in order to make normal approximation\nConstruct 95% confidence interval\nWrite interpretation"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#section-1",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#section-1",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "",
    "text": "Smoking status example\n\n\nA cross-sectional study of 8681 patients was conducted to evaluate the nature of smoking status among people. Of the 8681 people, 4840 were nonsmokers and 3841 were smokers.\n\n\n\n\n\nEstimate proportion \\(\\widehat{p}\\) \\[ \\widehat{p} = \\dfrac{3841}{3841 + 4840} = \\dfrac{3841}{8681} = 0.44246\\]\nCheck that \\(n\\widehat{p}&gt;10\\) and \\(n(1-\\widehat{p})&gt;10\\) in order to make normal approximation \\[ n\\widehat{p} = 8681\\cdot0.4425 = 3841 &gt; 10\\] \\[ n(1-\\widehat{p}) = 8681\\cdot(1-0.4425) = 4840 &gt; 10\\]\n\n\n\nConstruct 95% confidence interval\n\n\\[ \\widehat{p} \\pm z^*_{0.975} \\cdot \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\]\n\nprop.test(x = 3841, n = 8681, correct = T)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  3841 out of 8681, null probability 0.5\nX-squared = 114.73, df = 1, p-value &lt; 2.2e-16\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.4319827 0.4529896\nsample estimates:\n        p \n0.4424605 \n\n\n\nWrite interpretation of estimate\n\nThe estimated proportion of smokers is 0.442 (95% CI: 0.432, 0.453).\nAdditional interpretation of CI: We are 95% confident that the (population) proportion of smokers is between 0.432 and 0.453."
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#poll-everywhere-question-2",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#poll-everywhere-question-2",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Poll everywhere question 2",
    "text": "Poll everywhere question 2"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#estimate-and-confidence-interval-for-difference-in-proportions",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#estimate-and-confidence-interval-for-difference-in-proportions",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Estimate and Confidence Interval for Difference in Proportions",
    "text": "Estimate and Confidence Interval for Difference in Proportions\n\nUse the sampling distribution of \\(\\widehat{p}_1\\) and \\(\\widehat{p}_2\\) to construct the confidence interval:\n\n\\((1-\\alpha)\\%\\) confidence interval for estimate difference in proportions:\n\n\n\\[\\begin{align} \\widehat{p}_1 - \\widehat{p}_2 &\\pm z^*_{(1-\\alpha/2)} \\cdot SE_{\\hat{p}_1 - \\hat{p}_2} \\\\ \\widehat{p}_1 - \\widehat{p}_2 &\\pm z^*_{(1-\\alpha/2)} \\cdot \\sqrt{\n\\frac{\\hat{p}_1\\cdot(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2\\cdot(1-\\hat{p}_2)}{n_2}} \\end{align}\\]\n\nUsing \\(SE_{\\hat{p}_1 - \\hat{p}_2} = \\sqrt{\\frac{\\hat{p}_1\\cdot(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2\\cdot(1-\\hat{p}_2)}{n_2}}\\) because we don’t know exactly what \\(p_1\\) and \\(p_2\\) are"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#poll-everywhere-question-3",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#poll-everywhere-question-3",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#example-strong-heart-study-12",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#example-strong-heart-study-12",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Example: Strong Heart Study (1/2)",
    "text": "Example: Strong Heart Study (1/2)\n\n\nThe Strong Heart Study is an ongoing study of American Indians residing in 13 tribal communities in three geographic areas (AZ, OK, and SD/ND) to study prevalence and incidence of cardiovascular disease and to identify risk factors. We will be examining the 4-year cumulative incidence of diabetes with one risk factor, glucose tolerance.\n \n\nImpaired glucose: normal or impaired glucose tolerance at baseline visit (between 1988 and 1991)\n \nDiabetes: Indicator of diabetes at follow-up visit (roughly four years after baseline) according to two-hour oral glucose tolerance test"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#example-strong-heart-study-22",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#example-strong-heart-study-22",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Example: Strong Heart Study (2/2)",
    "text": "Example: Strong Heart Study (2/2)\nThere is a total of 1664 American Indians in the dataset, with the following distribution of folks with diabetes and glucose tolerance:\n \n\n\n\n#shs_data = read.csv(file = here(\"./data/SHS_data.csv\"))\n\n\nSHS = tibble(Diabetes = c(rep(\"Not diabetic\", \n                   1338), \n                   rep(\"Diabetic\", 326)),\n              Glucose = c(rep(\"Normal\", \n                  1004),#Not diabetic\n          rep(\"Impaired\", 334),\n          rep(\"Normal\", \n              128), #Diabetic\n          rep(\"Impaired\", 198)))\n\n\n\n\nDisplaying the contingency table in R\nSHS %&gt;% tabyl(Glucose, Diabetes) %&gt;% \n  adorn_totals(where = c(\"row\", \"col\")) %&gt;% \n  gt() %&gt;% \n  tab_stubhead(label = \"Glucose Impairment\") %&gt;%\n  tab_spanner(label = \"Diabetes\", \n              columns = c(\"Not diabetic\", \"Diabetic\")) %&gt;%\n  tab_options(table.font.size = 45)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose\n\nDiabetes\n\nTotal\n\n\nNot diabetic\nDiabetic\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#section-2",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#section-2",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "",
    "text": "Strong Heart Study\n\n\nWhat is the difference in proportions for American Indians that have diabetes comparing individuals with normal vs. impaired glucose?\n\n\nNeeded steps:\n\nEstimate the difference in proportions\nCheck that each cell has at least 10 individuals\nConstruct 95% confidence interval\nWrite interpretation of estimate"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#section-3",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#section-3",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "",
    "text": "Strong Heart Study\n\n\nWhat is the difference in proportions for American Indians that have diabetes comparing individuals with normal vs. impaired glucose?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose\n\nDiabetes\n\nTotal\n\n\nNot diabetic\nDiabetic\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\nEstimate the difference in proportions \\[ \\widehat{p}_1 -\\widehat{p}_2 = \\dfrac{198}{532} - \\dfrac{128}{1132} = 0.2591\\]\nCheck that each cell has at least 10 individuals\n\n\n\nConstruct 95% confidence interval\n\n\nprop.test(x = table(SHS$Glucose, SHS$Diabetes), \n          correct = T)\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  table(SHS$Glucose, SHS$Diabetes)\nX-squared = 152.6, df = 1, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n95 percent confidence interval:\n 0.2126963 0.3055162\nsample estimates:\n   prop 1    prop 2 \n0.3721805 0.1130742 \n\n\n\nWrite interpretation of estimate\n\nThe estimated difference in proportion of diabetic American Indians comparing is 0.259 (95% CI: 0.213, 0.306).\nAdditional interpretation of CI: We are 95% confident that the difference in (population) proportions of American Indians who have normal glucose tolerance and impaired glucose tolerance that developed diabetes is between 0.213 and 0.306."
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#mcnemars-test",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#mcnemars-test",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "McNemar’s Test",
    "text": "McNemar’s Test\n\nMcNemar’s test should be used if data is from a matched pairs study\n\n \n\nWhat is a matched-pairs study?\n\nParticipants are paired based on key characteristics\nEach participant within a pair will be assigned to different treatment groups\n\nCategorical test that is parallel to the “paired t-test”\n\n \n\nR packages and functions\n\nNormal approximation: mcnemar.test() in built-in stats package\nExact test: mcnemar.exact() in exact2x2 package\n\n\n \n\nIf you would like more information of McNemar’s test, please see Rosner TB: 10.4 and 10.5: Paired Samples"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#summary-so-far",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#summary-so-far",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Summary so far",
    "text": "Summary so far\n\nIntroduced categorical data as the response in analysis\nReviewed an important distribution (Binomial distribution) for categorical data analysis\nEstimated a single proportion from a sample with its confidence interval\nEstimated a difference in proportions from a sample with its confidence interval\n\n   \n\nCan we expand this to ask a more general question about association between a response and explanatory variable?\n\nWhat if there is more than 2 categories for either variable?"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#contingency-tables-r-x-c",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#contingency-tables-r-x-c",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Contingency Tables (R x C)",
    "text": "Contingency Tables (R x C)\n\nR X C contingency tables\n\nContains information for two discrete variables: one has R categories and the other has C categories.\nRefers to the number of rows (R) and number of columns (C) in the table\n\n\n \n\nFor two proportions: focused on 2 X 2 contingency tables\n\nR = 2, C = 2\n\n\n \n\nExpand our contingency tables to variables with 2 or more categories\n\nCategories can be ordinal or nominal"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#contingency-table-example",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#contingency-table-example",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Contingency Table: Example",
    "text": "Contingency Table: Example\nLet’s say we are interested in learning the association between the development of breast cancer and age at first birth. Our first step is typically to present the observed data:\n\n\nThis is a 2 x 5 contingency table"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#test-associationtrend-of-r-x-c-contingency-table",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#test-associationtrend-of-r-x-c-contingency-table",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Test Association/Trend of R X C Contingency Table",
    "text": "Test Association/Trend of R X C Contingency Table\n \n\n\nIf both variables are nominal, a test of general association will be sufficient\n\nTest of general association is the same regardless of R and C\nTest used for 2x2 contingency table same as 5x3 contingency table\nWe will cover:\n\nChi-squared test\nFisher Exact test\n\n\n\n\n\nIf one or both variables are ordinal, a test of trend may be of interest\n\nTreats ordinal variables as quantitative rather than qualitative (nominal scale)\nTest of trend has greater power than the test of general association\nWe will cover:\n\nCochran-Armitage test\nMantel-Haenszel test"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#poll-everywhere-question-4",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#poll-everywhere-question-4",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#test-of-general-association",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#test-of-general-association",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Test of General Association",
    "text": "Test of General Association\n\nGeneral research question: Are two variables (both categorical, nominal) associated with each other?\n\n \n\nTranslated to a hypothesis test:\n\n\\(H_0\\) : There is no association between the two variables / The variables are independent\n\\(H_1\\) : There is an association between the two variables / The variables are not independent\n\n\n   \n\nWe have two options for testing general association:\n\nChi-squared test\nFisher’s Exact test"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#test-of-general-association-shs-example",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#test-of-general-association-shs-example",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Test of General Association: SHS Example",
    "text": "Test of General Association: SHS Example\n\nMain question: Do American Indians with impaired glucose tolerance have a different incidence of diabetes?\n\nIs glucose tolerance associated with diabetes incidence among American Indians?\n\nWe have two variables, and both variables have two nominal categories\n\nDiabetes outcome: Not diabetic and Diabetic\nGlucose tolerance: Normal or Impaired\n\n\n \n\nAnswer research question with a test of general association\nHypothesis:\n\n\\(H_0\\) : There is no association between glucose tolerance and diabetes / Glucose tolerance and diabetes are independent\n\\(H_1\\) : There is an association between glucose tolerance and diabetes / Glucose tolerance and diabetes are not independent"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Chi-squared test",
    "text": "Chi-squared test\n\nTest to see how likely is it that we observe our data given the null hypothesis (no association)\n\n \n\nWe use the null to calculate the expected cell counts and compare them to the observed cell counts\n\n \n\nRequirements to conduct Chi-squared test (expected cell counts)\n\nFor 2 x 2 contingency table:\n\nNo expected cell counts should be less than 10\n\nFor contingency table with 3x2, 3x3, 4x4, etc.:\n\nNo more than 20% of expected cell counts are less than 5\nNo expected cell counts are less than 1"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test-process",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test-process",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Chi-squared test: Process",
    "text": "Chi-squared test: Process\n\nCheck that the expected cell counts threshold is met\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic and p-value for Chi-squared test in R\n\nWe will not discuss the test statistic’s equation\n\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test-expected-cell-counts",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test-expected-cell-counts",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Chi-squared test: Expected Cell Counts",
    "text": "Chi-squared test: Expected Cell Counts\n\nIs the sample size big enough for the chi-square test to be adequate? What are the expected cell counts?\n\n \n\nIf you want an explanation of how to calculate by hand, please see Vu and Harringtion TB (section 8.3.1, page 405)\n\n \n\nToo time consuming for this class, but R does it quickly using the expected() function in the epitools package"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test-expected-cell-counts-1",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test-expected-cell-counts-1",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Chi-squared test: Expected Cell Counts",
    "text": "Chi-squared test: Expected Cell Counts\n\nIn the Strong Heart Study…\n\n\nSHS_table = table(SHS$Glucose, SHS$Diabetes)\nSHS_table\n\n          \n           Diabetic Not diabetic\n  Impaired      198          334\n  Normal        128         1004\n\nlibrary(epitools)\nexpected(SHS_table)\n\n          \n           Diabetic Not diabetic\n  Impaired  104.226      427.774\n  Normal    221.774      910.226\n\n\n \n\n\n\n\n\nAll expected counts &gt; 5"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test-shs-example",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test-shs-example",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Chi-squared test: SHS Example",
    "text": "Chi-squared test: SHS Example\n\n\n\nCheck expected cell counts threshold\n\n\nexpected(SHS_table)\n\n          \n           Diabetic Not diabetic\n  Impaired  104.226      427.774\n  Normal    221.774      910.226\n\n\nAll expected cells are greater than 5.\n\n\\(\\alpha = 0.05\\)\nHypothesis test:\n\n\\(H_0\\) : There is no association between glucose tolerance and diabetes\n\\(H_1\\) : There is an association between glucose tolerance and diabetes\n\n\n\n\nCalculate the test statistic and p-value for Chi-squared test in R\n\n\nchisq.test(x = SHS_table, correct = T)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  SHS_table\nX-squared = 152.6, df = 1, p-value &lt; 2.2e-16\n\n\n\nConclusion to the hypothesis test\n\nWe reject the null hypothesis that glucose tolerance and diabetes are not associated (\\(p&lt;2.2\\cdot10^{-16}\\)). There is sufficient evidence that glucose tolerance and diabetes incidence are associated among American Indians."
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Fisher’s Exact Test",
    "text": "Fisher’s Exact Test\n\nOnly necessary when expected counts in one or more cells is less than 5\nGiven row and column totals fixed, computes exact probability that we observe our data or more extreme data\nConsider a general 2 x 2 table:\n\n\n\nThe exact probability of observing a table with cells (a, b, c, d) can be computed based on the hypergeometric distribution\n\n\\[P(a, b, c, d) = \\dfrac{(a+b)!\\cdot(c+d)!\\cdot(a+c)!\\cdot(b+d)!}{n!\\cdot a!\\cdot b!\\cdot c!\\cdot d!}\\]\n\nNumerator is fixed and denominator changes"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-process",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-process",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Fisher’s Exact test: Process",
    "text": "Fisher’s Exact test: Process\n\nCheck the expected cell counts\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic and p-value for Fisher Exact test in R\n\nWe will not discuss the test statistic’s equation\n\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-shs-example-not-appropriate-use-of-fishers-exact",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-shs-example-not-appropriate-use-of-fishers-exact",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Fisher’s Exact test: SHS Example (not appropriate use of Fisher’s Exact)",
    "text": "Fisher’s Exact test: SHS Example (not appropriate use of Fisher’s Exact)\n\n\n\nCheck expected cell counts threshold\n\n\nexpected(GAC_table)\n\n     \n             No      Yes\n  No  143.21983 5.780172\n  Yes  79.78017 3.219828\n\n\nWe’re going to pretend they are less than 5.\n\n\\(\\alpha = 0.05\\)\nHypothesis test:\n\n\\(H_0\\) : There is no association between glucose tolerance and diabetes\n\\(H_1\\) : There is an association between glucose tolerance and diabetes\n\n\n\n\nCalculate the test statistic and p-value for Chi-squared test in R\n\n\nfisher.test(x = GAC_table)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  GAC_table\np-value = 0.2877\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  0.4829292 12.0164676\nsample estimates:\nodds ratio \n  2.314727 \n\n\n\nConclusion to the hypothesis test\n\nWe reject the null hypothesis that glucose tolerance and diabetes are not associated (\\(p&lt;2.2\\cdot10^{-16}\\)). There is sufficient evidence that glucose tolerance and diabetes incidence are associated among American Indians."
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#test-of-trend",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#test-of-trend",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Test of Trend",
    "text": "Test of Trend\n\nIf one or both variables are ordinal, a test of trend may be of interest\n\nTreats ordinal variables as quantitative rather than qualitative (nominal scale)\nTest of trend has greater power than the test of general association\nYou can use a test of general association for non-ordinal variables\n\n\n \n\nTwo tests of trend that we we learn:\n\nCochran-Armitage test\n\nTests association between a binary response and an ordinal explanatory variable\n\nMantel-Haenszel test\n\nTest association between an ordinal response and an ordinal explanatory variable"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Cochran-Armitage test",
    "text": "Cochran-Armitage test\n\nCochran-Armitage test for trend will determine if there is association between a binary response variable and an ordinal variable with 3 or more categories\n\n \n\nIt will test the trend of the proportions over the ordinal variable\n\nAnswers the question: Does the proportion of people with a “successful” outcome increase as the ordinal explanatory variable increases?\n\n\n \n\nCochran-Armitage test for trend is only suitable for 2 x C contingency tables"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-hypothesis-test",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-hypothesis-test",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Cochran-Armitage test: Hypothesis Test",
    "text": "Cochran-Armitage test: Hypothesis Test\n\n\n\n\nNull Hypothesis (\\(H_0\\))\n\n\nThe proportions of successes are the same across all C ordinal values of the explanatory variable. \\[p_1 = p_2 = ... = p_C\\]\n\n\n\n\n\nAlternative Hypothesis (\\(H_1\\))\n\n\nThe proportions of successes tend to increase as ordinal value of the explanatory variable increases\n\\[p_1 \\leq p_2 \\leq ... \\leq p_C\\]\nOR\nThe proportions of successes tend to decrease as ordinal value of the explanatory variable increases\n\\[p_1 \\geq p_2 \\geq ... \\geq p_C\\]"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-process",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-process",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Cochran-Armitage test: Process",
    "text": "Cochran-Armitage test: Process\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic and p-value for Cochran-Armitage test in R\n\nWe will not discuss the test statistic’s equation\nJust know it follows a Normal distribution\n\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-example-13",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-example-13",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Cochran-Armitage test: Example (1/3)",
    "text": "Cochran-Armitage test: Example (1/3)\nWe are interested in learning the association between the development of breast cancer and age at first birth among people who have given birth"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-example-23",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-example-23",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Cochran-Armitage test: Example (2/3)",
    "text": "Cochran-Armitage test: Example (2/3)\n\nBefore we go into the hypothesis test procedure, we need to construct the contingency table in R\n\n\nCancer = c(320, 1206, 1011, 463, 220)\nNo_Cancer = c(1422, 4432, 2893, 1092, 406)\nbscancer = matrix (c(Cancer, No_Cancer), nrow = 2, byrow = T)\nrownames(bscancer) = c(\"Cancer\",\"No Cancer\")\ncolnames(bscancer) = c(\"&lt;20\",\"20-24\",\"25-29\",\"30-34\",\"&gt;=35\")\nbscancer\n\n           &lt;20 20-24 25-29 30-34 &gt;=35\nCancer     320  1206  1011   463  220\nNo Cancer 1422  4432  2893  1092  406\n\n\n\nIt does not need to be pretty to use in function"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-example-33",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-example-33",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Cochran-Armitage test: Example (3/3)",
    "text": "Cochran-Armitage test: Example (3/3)\n\n\n\n\\(\\alpha = 0.05\\)\nHypothesis test:\n\n\\(H_0\\): The proportions of breast cancer are the same for all age levels of first birth. \\[p_1 = p_2 = ... = p_5\\]\n\\(H_1\\): The proportions of breast cancer tends to increase as level of age of first birth increases\n\n\n\\[p_1 \\leq p_2 \\leq ... \\leq p_5\\]\n\n\nCalculate the test statistic and p-value for Cochran-Armitage test in R\n\n\nlibrary(DescTools)\nCochranArmitageTest(bscancer)\n\n\n    Cochran-Armitage test for trend\n\ndata:  bscancer\nZ = 11.358, dim = 5, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\nConclusion to the hypothesis test\n\nWe reject the null hypothesis that proportions of breast cancer are the same for all age levels of first birth (\\(p&lt;2.2\\cdot10^{-16}\\)). There is sufficient evidence that the proportion of of breast cancer increase as the the age at first birth increases."
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Mantel-Haenszel test",
    "text": "Mantel-Haenszel test\n\nWhen both variables are ordinal, we can conduct Mantel-Haenszel test of trend for linear association\nMantel-Haenszel test for linear trend is suitable for any R x C contingency tables with two ordinal variables\nHypothesis test:\n\n\n\n\n\nNull Hypothesis (\\(H_0\\))\n\n\nThere is no correlation between the two variables \\[ \\rho = 0\\]\n\n\n\n\n\nAlternative Hypothesis (\\(H_1\\))\n\n\nThere is correlation between the two variables\n\\[ \\rho \\neq 0\\]"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-process",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-process",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Mantel-Haenszel test: Process",
    "text": "Mantel-Haenszel test: Process\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic and p-value for Mantel-Haenszel test in R\n\nWe will not discuss the test statistic’s equation\n\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-example-13",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-example-13",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Mantel-Haenszel test: Example (1/3)",
    "text": "Mantel-Haenszel test: Example (1/3)\nA water treatment company is studying water additives and investigating how they affect clothes washing (through measurements of abrasions, wearing, and color loss).\nThe treatments studies where no treatment (plain water), the standard treatment, and a double dose of the standard treatment, called super. Washability was measured as low, medium and high.\nAre levels of washability associated with treatment?"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-example-23",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-example-23",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Mantel-Haenszel test: Example (2/3)",
    "text": "Mantel-Haenszel test: Example (2/3)\n\nBefore we go into the hypothesis test procedure, we need to construct the contingency table in R\n\n\nwater = matrix (c(27, 14, 5, 10, 17, 26, 5, 12, 50), nrow = 3, byrow = T)\nrownames(water) = c(\"plain\",\"standard\",\"super\")\ncolnames(water) = c(\"low\",\"medium\",\"high\")\nwater\n\n         low medium high\nplain     27     14    5\nstandard  10     17   26\nsuper      5     12   50\n\n\n\nIt does not need to be pretty to use in function"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-example-33",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-example-33",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Mantel-Haenszel test: Example (3/3)",
    "text": "Mantel-Haenszel test: Example (3/3)\n\n\n\n\\(\\alpha = 0.05\\)\nHypothesis test:\n\n\\(H_0\\): Water treatment and washability are not correlated. \\[ \\rho = 0\\]\n\\(H_1\\): Water treatment and washability are correlated.\n\n\n\\[ \\rho \\neq 0\\]\n\n\nCalculate the test statistic and p-value for Mantel-Haenszel test in R\n\n\nlibrary(DescTools)\nMHChisqTest(water)\n\n\n    Mantel-Haenszel Chi-Square\n\ndata:  water\nX-squared = 50.602, df = 1, p-value = 1.132e-12\n\n\n\nConclusion to the hypothesis test\n\nWe reject the null hypothesis that there is no correlation between washability and water treatment (\\(p = 1.13 \\cdot 10^{-12} &lt; 0.05\\)). There is sufficient evidence that level of water treatment is associated with washability."
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#more-resources",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#more-resources",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "More resources",
    "text": "More resources\n\nReview from my EPI 525 class in F25\n\nSchedule from class: Lessons 15, 16, 19\n\nFor a refresher or review of one proportion and differences in proportions\n\nAnd their power calculations\nFrom Meike’s BSTA 511 course (see Day 12!)\n\nFor a refresher or review of Chi-squared test or Fisher’s Exact test\n\nFrom Meike’s BSTA 511 course (see Day 13!)"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-1",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-1",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1\nMake sure to remember your answer!! We’ll use this on Wednesday!"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#review-of-test-of-association-12",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#review-of-test-of-association-12",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Review of Test of Association (1/2)",
    "text": "Review of Test of Association (1/2)\n\nLast week: learned some tests of association for contingency tables\n\n \n\nFor studies with two independent samples\n\nGeneral association\n\nChi-squared test\nFisher’s Exact test\n\nTest of trends\n\nCochran-Armitage test\nMantel-Haenszel test"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#review-of-test-of-association-22",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#review-of-test-of-association-22",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Review of Test of Association (2/2)",
    "text": "Review of Test of Association (2/2)"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#test-of-association-does-not-measure-association",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#test-of-association-does-not-measure-association",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Test of association does not measure association",
    "text": "Test of association does not measure association\n\nTest of association does not provide an effective measure of association. The p-value alone is not enough\n\n\\(\\text{p-value} &lt; 0.05\\) suggests there is a statistically significant association between the group and outcome\n\\(\\text{p-value} = 0.00001\\) vs. \\(\\text{p-value} = 0.01\\) does not mean the magnistude of association is different\n\n\n \n\nBut it does not tell how different the risks are between the two groups\n\n \n\nWe want to find out one or more measurements for quantifying the risks across the groups."
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#measures-of-association",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#measures-of-association",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Measures of Association",
    "text": "Measures of Association\n\nWhen we have a 2x2 contingency table and independent samples, we have an option of three measures of association:\n \n\nRisk difference (RD)\n\n \n\nRelative risk (RR)\n\n \n\nOdds ratio (OR)\n\n\n \nEach measures association by comparing the proportion of successes/failures from each categorical group of our explanatory variable."
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#before-we-discuss-each-further",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#before-we-discuss-each-further",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Before we discuss each further…",
    "text": "Before we discuss each further…\n \nLet’s define the cells within a 2x2 contingency table:\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nThen we can define risk: the proportion of “successes”\n\nWith \\(\\text{Risk}_1 = \\dfrac{n_{11}}{n_1}\\)"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#risk-difference-rd",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#risk-difference-rd",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Risk Difference (RD)",
    "text": "Risk Difference (RD)\n\nRisk difference computes the absolute difference in risk for the two groups (from the explanatory variable)\nPoint estimate: \\[\\widehat{RD} = \\widehat{p}_1 - \\widehat{p}_1 = \\dfrac{n_{11}}{n_1} - \\dfrac{n_{21}}{n_2}\\]\n\nWith range of point estimate from \\([-1, 1]\\)\n\nApproximate standard error:\n\n\\[ SE_{\\widehat{RD}} = \\sqrt{\\frac{\\hat{p}_1\\cdot(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2\\cdot(1-\\hat{p}_2)}{n_2}}\\]\n\n95% Wald confidence interval for \\(\\widehat{RD}\\):\n\n\\[\\widehat{RD} \\pm 1.96 \\cdot SE_{\\widehat{RD}}\\]"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#recall-the-strong-heart-study",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#recall-the-strong-heart-study",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Recall the Strong Heart Study",
    "text": "Recall the Strong Heart Study\n\n\nThe Strong Heart Study is an ongoing study of American Indians residing in 13 tribal communities in three geographic areas (AZ, OK, and SD/ND). We will look at data from this study examining the incidence of diabetes at a follow-up visit and impaired glucose tolerance (ITG) at baseline (4 years apart).\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Risk Difference",
    "text": "SHS Example: Risk Difference\n\n\n\n\nRisk difference\n\n\nCompute the point estimate and 95% confidence interval for the diabetes risk difference between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\nNeeded steps:\n\nCompute the risk difference\nCompute 95% confidence interval\nInterpret the estimate"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-14",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-14",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Risk Difference (1/4)",
    "text": "SHS Example: Risk Difference (1/4)\n\n\n\n\nRisk difference\n\n\nCompute the point estimate and 95% confidence interval for the diabetes risk difference between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nCompute the risk difference \\[\\widehat{RD}={\\hat{p}}_1 - {\\hat{p}}_2=\\frac{n_{11}}{n_1}-\\frac{n_{21}}{n_2}=\\ \\frac{198}{532}\\ - \\frac{128}{1132}=0.3722−0.1131=0.2591\\]"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-24",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-24",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Risk Difference (2/4)",
    "text": "SHS Example: Risk Difference (2/4)\n\n\n\n\nRisk difference\n\n\nCompute the point estimate and 95% confidence interval for the diabetes risk difference between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nCompute 95% confidence interval\n\n\\[\\begin{aligned} &\\widehat{RD}\\pm z_{\\left(1-\\frac{\\alpha}{2}\\right)}^\\ast \\times SE_{\\widehat{RD}} \\\\\n= &\\widehat{RD}\\pm z_{\\left(1-\\frac{\\alpha}{2}\\right)}^\\ast \\times \\sqrt{\\frac{{\\hat{p}}_1\\ (1-{\\hat{p}}_1)}{n_1}+\\frac{{\\hat{p}}_2(1-{\\hat{p}}_2)}{n_2}\\ }\\\\\n=  & 0.2591 \\pm 1.96\\times \\sqrt{\\frac{0.3722(1-0.3722)}{532}+\\frac{0.1131(1-0.1131)}{1132}\\ }\\\\\n=  & (0.2141,\\ 0.3041 )\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-34",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-34",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Risk Difference (3/4)",
    "text": "SHS Example: Risk Difference (3/4)\n\n\n\n\nRisk difference\n\n\nCompute the point estimate and 95% confidence interval for the diabetes risk difference between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n1/2. Compute risk difference and 95% confidence interval\n\nfmsb::riskdifference(198, 128, 532, 1132)\n\n                 Cases People at risk         Risk\nExposed    198.0000000    532.0000000    0.3721805\nUnexposed  128.0000000   1132.0000000    0.1130742\nTotal      326.0000000   1664.0000000    0.1959135\n\n\n\n    Risk difference and its significance probability (H0: The difference\n    equals to zero)\n\ndata:  198 128 532 1132\np-value &lt; 2.2e-16\n95 percent confidence interval:\n 0.2140779 0.3041346\nsample estimates:\n[1] 0.2591062"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-44",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-44",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Risk Difference (4/4)",
    "text": "SHS Example: Risk Difference (4/4)\n\n\n\n\nRisk difference\n\n\nCompute the point estimate and 95% confidence interval for the diabetes risk difference between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nInterpret the estimate\n\nThe diabetes diagnosis risk difference between impaired and normal glucose tolerance is 0.2591 (95% CI: 0.2141, 0.3041). Since the 95% confidence interval contains 0, we do not have sufficient evidence that the risk of diabetes diagnosis between impaired and normal glucose tolerance is different."
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#when-is-the-risk-difference-misleading",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#when-is-the-risk-difference-misleading",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "When is the risk difference misleading?",
    "text": "When is the risk difference misleading?\n\nThe same risk differences can have very different clinical meanings depending on the risk for each group\n\n \n \n\nExample: for two treatments A and B, we know the risk difference (RD) is 0.009. Is it a meaningful difference?\n\nIf the risk is 0.01 for Trt A and 0.001 for Trt B?\nIf the risk is 0.41 for Trt A and 0.401 for Trt B?\n\n\n \n\nUsing the RD alone to summarize the difference in risks for comparing the two groups can be misleading\n\nThe ratio of risk can provide an informative descriptive measure of the “relative risk”"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#relative-risk-rr",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#relative-risk-rr",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Relative Risk (RR)",
    "text": "Relative Risk (RR)\n\nRelative risk computes the ratio of each group’s proportions of “success”\n\nAlso called risk ratio    \n\nPoint estimate: \\[\\widehat{RR}=\\dfrac{\\hat{p}_1}{\\hat{p}_2} = \\dfrac{n_{11}/n_1}{n_{21}/n_2}\\]\n\nRange: \\([0, \\infty]\\)"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-2",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-2",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#log-transformation-of-rr",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#log-transformation-of-rr",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Log-transformation of RR",
    "text": "Log-transformation of RR\n\nSampling distribution of the relative risk is highly skewed unless sample sizes are quite large\n\nLog transformation results in approximately normal distribution\nThus, compute confidence interval using normally distributed, log-transformed RR\nThen we convert back to the RR\n\nWe take the log (natural log) of RR: \\(\\ln(\\widehat{RR})\\) or \\(log(\\widehat{RR})\\)\n\nWhenever I say “log” I mean natural log (very common in statistics)\n\n\n\n\n\nThen we need to find approximate standard error for \\(\\ln(\\widehat{RR})\\) \\[SE_{\\ln(\\widehat{RR})}=\\sqrt{\\frac{1}{n_{11}}\\ -\\frac{1}{n_1}+\\frac{1}{n_{21}}-\\frac{1}{n_2}}\\]\n95% confidence interval for \\(\\ln(\\widehat{RR})\\): \\[\\ln(\\widehat{RR}) \\pm 1.96 \\times SE_{\\ln(\\widehat{RR})}\\]"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#how-do-we-get-back-to-the-rr-scale",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#how-do-we-get-back-to-the-rr-scale",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "How do we get back to the RR scale?",
    "text": "How do we get back to the RR scale?\n\nWe computed confidence interval using normally distributed, log-transformed RR (\\(\\ln(\\widehat{RR})\\)):\n\n\\[ \\bigg(\\ln(\\widehat{RR}) - 1.96 \\times SE_{\\ln(\\widehat{RR})}, \\ \\ln(\\widehat{RR}) + 1.96 \\times SE_{\\ln(\\widehat{RR})}\\bigg)\\]\n\nNow we need to exponentiate the CI to get back to interpretable values\n\nTake exponential of lower and upper bounds\n\n95% confidence interval for RR: two ways to display equation\n\n\\[ \\bigg(e^{\\ln(\\widehat{RR}) - 1.96 \\times SE_{\\ln(\\widehat{RR})}}, \\ e^{\\ln(\\widehat{RR}) + 1.96 \\times SE_{\\ln(\\widehat{RR})}}\\bigg)\\] \\[ \\bigg(\\exp\\big(\\ln(\\widehat{RR}) - 1.96 \\times SE_{\\ln(\\widehat{RR})}\\big), \\ \\exp\\big(\\ln(\\widehat{RR}) + 1.96 \\times SE_{\\ln(\\widehat{RR})}\\big)\\bigg)\\]"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#relative-risk-rr-1",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#relative-risk-rr-1",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Relative Risk (RR)",
    "text": "Relative Risk (RR)\n\nCan you compute the estimated RRs for the previous example?\n\nIf the risk for Trt A is 0.01 and Trt B is 0.001? \\(\\widehat{RR}= 10\\)\nIf the risk for Trt A is 0.41 and Trt B is 0.401? \\(\\widehat{RR}= 1.02\\)\n\nWhen \\(\\widehat{RR}= 1\\) …\n\nRisk is the same for the two groups\nIn other words, the group and the outcome are independent\n\nWhen computing \\(\\widehat{RR}\\) it is important to identify which variable is the response variable and which is explanatory variable\n\nWe may say “risk for Trt A” but this translates to the risk (or probability) of outcome success for those receiving Trt A"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-16",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-16",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Relative Risk (1/6)",
    "text": "SHS Example: Relative Risk (1/6)\n\n\n\n\nRelative risk\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Relative risk between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\nNeeded steps:\n\nCompute the relative risk\nFind confidence interval of log RR\nConvert back to RR\nInterpret the estimate"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-26",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-26",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Relative Risk (2/6)",
    "text": "SHS Example: Relative Risk (2/6)\n\n\n\n\nRelative risk\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Relative risk between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nCompute the relative risk \\[\\widehat{RR}=\\dfrac{{\\hat{p}}_1}{{\\hat{p}}_2}=\\dfrac{n_{11}/{n_1}}{n_{21}/{n_2}}=\\ \\frac{ 198/532}{128/1132}=\\dfrac{0.3722}{0.1131}=3.2915\\]"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-36",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-36",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Relative Risk (3/6)",
    "text": "SHS Example: Relative Risk (3/6)\n\n\n\n\nRelative risk\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Relative risk between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nFind confidence interval of log RR\n\n\\[\\begin{aligned} & \\ln(\\widehat{RR}) \\pm 1.96 \\times SE_{\\ln(\\widehat{RR})} \\\\\n= &\\ln(\\widehat{RR}) \\pm z_{\\left(1-\\frac{\\alpha}{2}\\right)}^\\ast \\times \\sqrt{\\frac{1}{n_{11}}\\ -\\frac{1}{n_1}+\\frac{1}{n_{21}}-\\frac{1}{n_2}}\\\\\n=  & 1.1913 \\pm 1.96\\times \\sqrt{\\frac{1}{198}\\ -\\frac{1}{532}+\\frac{1}{128}-\\frac{1}{1132}}\\\\\n=  & (0.9944,\\ 1.3883 )\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-46",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-46",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Relative Risk (4/6)",
    "text": "SHS Example: Relative Risk (4/6)\n\n\n\n\nRelative risk\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Relative risk between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nConvert back to RR\n\n\\[\\begin{aligned} & (\\exp(0.9944),\\ \\exp(1.3883 )) \\\\\n= & (2.703,\\ 4.0081 )\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-56",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-56",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Relative Risk (5/6)",
    "text": "SHS Example: Relative Risk (5/6)\n\n\n\n\nRelative risk\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Relative risk between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n1/2/3. Compute risk ratio and 95% confidence interval\n\nlibrary(epitools)\nSHS_ct = table(SHS$glucimp, SHS$case)\nriskratio(x = SHS_ct, rev = \"rows\")$measure\n\n          risk ratio with 95% C.I.\n           estimate    lower    upper\n  Normal   1.000000       NA       NA\n  Impaired 3.291471 2.702998 4.008061"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#pause-other-option-in-pubh-package",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#pause-other-option-in-pubh-package",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Pause: other option in pubh package",
    "text": "Pause: other option in pubh package\n\nSHS = SHS %&gt;% mutate(glucimp = as.factor(glucimp) %&gt;% relevel(ref = \"Normal\"))\ncontingency(case ~ glucimp, data = SHS)\n\n          Outcome\nPredictor     1    0\n  Impaired  198  334\n  Normal    128 1004\n\n             Outcome +    Outcome -      Total                 Inc risk *\nExposed +          198          334        532     37.22 (33.10 to 41.48)\nExposed -          128         1004       1132      11.31 (9.52 to 13.30)\nTotal              326         1338       1664     19.59 (17.71 to 21.58)\n\nPoint estimates and 95% CIs:\n-------------------------------------------------------------------\nInc risk ratio                                 3.29 (2.70, 4.01)\nInc odds ratio                                 4.65 (3.61, 6.00)\nAttrib risk in the exposed *                   25.91 (21.41, 30.41)\nAttrib fraction in the exposed (%)            69.62 (63.00, 75.05)\nAttrib risk in the population *                8.28 (5.63, 10.94)\nAttrib fraction in the population (%)         42.28 (34.71, 48.98)\n-------------------------------------------------------------------\nUncorrected chi2 test that OR = 1: chi2(1) = 154.239 Pr&gt;chi2 = &lt;0.001\nFisher exact test that OR = 1: Pr&gt;chi2 = &lt;0.001\n Wald confidence limits\n CI: confidence interval\n * Outcomes per 100 population units \n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  dat\nX-squared = 152.6, df = 1, p-value &lt; 2.2e-16"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-66",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-66",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Relative Risk (6/6)",
    "text": "SHS Example: Relative Risk (6/6)\n\n\n\n\nRelative risk\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Relative risk between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nInterpret the estimate\n\nThe estimated risk of diabetes is 3.29 times greater for American Indians who had impaired glucose tolerance at baseline compared to those who had normal glucose tolerance (95% CI: 2.70, 4.01).\n \nAdditional interpretation of 95% CI (not needed): We are 95% confident that the (population) relative risk is between 2.70 and 4.01.\n \nSince the 95% confidence interval does not include 1, there is sufficient evidence that the risk of diabetes differs significantly between impaired and normal glucose tolerance at baseline."
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#odds-building-up-to-odds-ratio",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#odds-building-up-to-odds-ratio",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Odds (building up to Odds Ratio)",
    "text": "Odds (building up to Odds Ratio)\n\nFor a probability of success \\(p\\) (or sometimes referred to as \\(\\pi\\)), the odds of success is: \\[\\text{odds}=\\frac{p}{1-p}=\\frac{\\pi}{1-\\pi}\\]\n\nExample: if \\(\\pi=0.75\\), then odds of success \\(= \\dfrac{0.75}{0.25}=3\\)\n\nIf odds &gt; 1, it implies a success is more likely than a failure\n\nExample: for \\(odds = 3\\), we expect to observe three times as many successes as failures\n\nIf odds is known, the probability of success can be computed \\[\\pi = \\dfrac{\\text{odds}}{\\text{odds}+1}\\]"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#odds-ratio-or",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#odds-ratio-or",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Odds Ratio (OR)",
    "text": "Odds Ratio (OR)\n\nOdds ratio is the ratio of two odds:\\[\\widehat{OR}=\\frac{odds_1}{odds_2}=\\frac{{\\hat{p}}_1/(1-{\\hat{p}}_1)}{{\\hat{p}}_2/(1-{\\hat{p}}_2)}\\]\n\nRange: \\([0, \\infty]\\)\nInterpretation: The odds of success for “group 1” is “\\(\\widehat{OR}\\)” times the odds of success for “group 2”\n\n\n \n\nWhat do values of odds ratios mean?\n\n\n\n\n\n\n\nOdds Ratio\nClinical Meaning\n\n\n\n\n\\(\\widehat{OR} &lt; 1\\)\nOdds of success is smaller in group 1 than in group 2\n\n\n\\(\\widehat{OR} = 1\\)\nExplanatory and response variables are independent\n\n\n\\(\\widehat{OR} &gt; 1\\)\nOdds of success is greater in group 1 than in group 2"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-3",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-3",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#odds-ratio-or-1",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#odds-ratio-or-1",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Odds Ratio (OR)",
    "text": "Odds Ratio (OR)\n\nValues of OR farther from 1.0 in a given direction represent stronger association\n\nAn OR = 4 is farther from independence than an OR = 2\nAn OR = 0.25 is farther from independence than an OR = 0.5\nFor OR = 4 and OR = 0.25, they are equally away from independence (because ¼ = 0.25)\n\n\n \n\nWe take the inverse of the OR for success of group 1 compared to group 2 to get…\n\nOR for failure of group 1 compared to group 2\nOR for success of group 2 compared to group 1"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#log-transformation-of-or",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#log-transformation-of-or",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Log-transformation of OR",
    "text": "Log-transformation of OR\n\nLike RR, sampling distribution of the odds ratio is highly skewed\n\nLog transformation results in approximately normal distribution\nThus, compute confidence interval using normally distributed, log-transformed OR\n\n\n\n\n\nApproximate standard error for \\(\\ln (\\widehat{OR})\\): \\[SE_{\\ln(\\widehat{OR})}=\\sqrt{\\frac{1}{n_{11}}\\ +\\frac{1}{n_{12}}+\\frac{1}{n_{21}}+\\frac{1}{n_{22}}}\\]\n95% confidence interval for \\(\\ln(\\widehat{OR})\\): \\[\\ln(\\widehat{OR}) \\pm 1.96 \\times SE_{\\ln(\\widehat{OR})}\\]"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#how-do-we-get-back-to-the-or-scale",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#how-do-we-get-back-to-the-or-scale",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "How do we get back to the OR scale?",
    "text": "How do we get back to the OR scale?\n\nWe computed confidence interval using normally distributed, log-transformed RR (\\(\\ln(\\widehat{OR})\\)):\n\n\\[ \\bigg(\\ln(\\widehat{OR}) - 1.96 \\times SE_{\\ln(\\widehat{OR})}, \\ \\ln(\\widehat{OR}) + 1.96 \\times SE_{\\ln(\\widehat{OR})}\\bigg)\\]\n\nNow we need to exponentiate the CI to get back to interpretable values\n\nTake exponential of lower and upper bounds\n\n95% confidence interval for RR: two ways to display equation\n\n\\[ \\bigg(e^{\\ln(\\widehat{OR}) - 1.96 \\times SE_{\\ln(\\widehat{OR})}}, \\ e^{\\ln(\\widehat{OR}) + 1.96 \\times SE_{\\ln(\\widehat{OR})}}\\bigg)\\] \\[ \\bigg(\\exp\\big(\\ln(\\widehat{OR}) - 1.96 \\times SE_{\\ln(\\widehat{OR})}\\big), \\ \\exp\\big(\\ln(\\widehat{OR}) + 1.96 \\times SE_{\\ln(\\widehat{OR})}\\big)\\bigg)\\]"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-16",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-16",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Odds Ratio (1/6)",
    "text": "SHS Example: Odds Ratio (1/6)\n\n\n\n\nOdds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes odds ratio between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\nNeeded steps:\n\nCompute the odds ratio\nFind confidence interval of log OR\nConvert back to OR\nInterpret the estimate"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-26",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-26",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Odds Ratio (2/6)",
    "text": "SHS Example: Odds Ratio (2/6)\n\n\n\n\nOdds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Odds ratio between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nCompute the odds ratio\n\n\\(\\widehat{p}_1 = 198/532 = 0.3722\\), \\(\\widehat{p}_2 = 128/1132 = 0.1131\\) \\[\\widehat{OR}=\\frac{\\widehat{p_1}/(1-\\widehat{p_1})}{\\widehat{p_2}/(1-\\widehat{p_2})}= \\dfrac{0.3722/(1-0.3722)}{0.1131/(1-0.1131)}= 4.6499\\]"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-36",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-36",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Odds Ratio (3/6)",
    "text": "SHS Example: Odds Ratio (3/6)\n\n\n\n\nOdds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Odds ratio between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nFind confidence interval of log OR\n\n\\[\\begin{aligned} & \\ln(\\widehat{OR}) \\pm 1.96 \\times SE_{\\ln(\\widehat{OR})} \\\\\n= &\\ln(\\widehat{OR}) \\pm z_{\\left(1-\\frac{\\alpha}{2}\\right)}^\\ast \\times \\sqrt{\\frac{1}{n_{11}}\\ +\\frac{1}{n_{12}}+\\frac{1}{n_{21}}+\\frac{1}{n_{22}}}\\\\\n=  & 1.5368 \\pm 1.96\\times \\sqrt{\\frac{1}{198}\\ +\\frac{1}{334}+\\frac{1}{128}+\\frac{1}{1004}}\\\\\n=  & (1.2824,\\ 1.7913 )\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-46",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-46",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Odds Ratio (4/6)",
    "text": "SHS Example: Odds Ratio (4/6)\n\n\n\n\nOdds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Odds ratio between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nConvert back to OR\n\n\\[\\begin{aligned} & (\\exp(1.2824),\\ \\exp(1.7913 )) \\\\\n= & (3.6053,\\ 5.9971 )\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-56",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-56",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Odds Ratio (5/6)",
    "text": "SHS Example: Odds Ratio (5/6)\n\n\n\n\nOdds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Odds ratio between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n1/2/3. Compute OR and 95% confidence interval\n\nlibrary(epitools)\nSHS_ct = table(SHS$glucimp, SHS$case)\n# no `rev` needed below bc we set the reference level in slide 32\noddsratio(x = SHS_ct, method = \"wald\")$measure \n\n          odds ratio with 95% C.I.\n           estimate    lower    upper\n  Normal   1.000000       NA       NA\n  Impaired 4.649888 3.605289 5.997148"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#pause-other-option-in-pubh-package-1",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#pause-other-option-in-pubh-package-1",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Pause: other option in pubh package",
    "text": "Pause: other option in pubh package\n\ncontingency(case ~ glucimp, data = SHS, digits = 3)\n\n          Outcome\nPredictor     1    0\n  Impaired  198  334\n  Normal    128 1004\n\n             Outcome +    Outcome -      Total                 Inc risk *\nExposed +          198          334        532  37.218 (33.097 to 41.482)\nExposed -          128         1004       1132   11.307 (9.521 to 13.298)\nTotal              326         1338       1664  19.591 (17.709 to 21.581)\n\nPoint estimates and 95% CIs:\n-------------------------------------------------------------------\nInc risk ratio                                 3.291 (2.703, 4.008)\nInc odds ratio                                 4.650 (3.605, 5.997)\nAttrib risk in the exposed *                   25.911 (21.408, 30.413)\nAttrib fraction in the exposed (%)            69.618 (63.004, 75.050)\nAttrib risk in the population *                8.284 (5.631, 10.937)\nAttrib fraction in the population (%)         42.284 (34.713, 48.976)\n-------------------------------------------------------------------\nUncorrected chi2 test that OR = 1: chi2(1) = 154.239 Pr&gt;chi2 = &lt;0.001\nFisher exact test that OR = 1: Pr&gt;chi2 = &lt;0.001\n Wald confidence limits\n CI: confidence interval\n * Outcomes per 100 population units \n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  dat\nX-squared = 152.6, df = 1, p-value &lt; 2.2e-16"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-66",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-66",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Odds Ratio (6/6)",
    "text": "SHS Example: Odds Ratio (6/6)\n\n\n\n\nOdds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Odds ratio between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nInterpret the estimate\n\nThe estimated odds of diabetes for American Indians with impaired glucose tolerance at baseline is 4.65 times the odds for American Indians with normal glucose tolerance at baseline.\n \nAdditional interpretation of 95% CI (not needed): We are 95% confident that the odds ratio is between 3.61 and 6.00.\n \nSince the 95% confidence interval does not include 1, there is sufficient evidence that the odds of diabetes differs significantly between impaired and normal glucose tolerance at baseline."
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#inversing-an-odds-ratio",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#inversing-an-odds-ratio",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Inversing an Odds Ratio",
    "text": "Inversing an Odds Ratio\n\nSome clinicians may prefer interpretations of OR &gt; 1 instead of an OR &lt; 1\nThe transformation can easily be done by inverse\n\nRemember we discussed that OR = 4 is an equivalent a strong association as OR = 0.25 (1/4)\n\nOR comparing group 1 to group 2 = inverse of OR comparing group 2 to group 1\n\n\\[ OR_{1v2}=\\frac{{\\hat{p}}_1/(1-{\\hat{p}}_1)}{{\\hat{p}}_2/(1-{\\hat{p}}_2)}=\\frac{1}{\\frac{{\\hat{p}}_2/(1-{\\hat{p}}_2)}{{\\hat{p}}_1/(1-{\\hat{p}}_1)}}=\\frac{1}{OR_{2v1}}\\]"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-4",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-4",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-inversing-odds-ratio",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-inversing-odds-ratio",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Inversing Odds Ratio",
    "text": "SHS Example: Inversing Odds Ratio\n\n\n\n\nInversing odds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes odds ratio between normal and impaired glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\nNeeded steps:\n\nInverse point estimate and confidence interval\n\n\\[\\widehat{OR}=\\frac{1}{4.6499}=0.2151\\] The 95% Confidence interval is then\n\\[ \\left(\\frac{1}{5.9971}, \\frac{1}{3.6053}\\right)\\ =\\ (0.1667, 0.2774)\\]"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-inversing-odds-ratio-1",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-inversing-odds-ratio-1",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Inversing Odds Ratio",
    "text": "SHS Example: Inversing Odds Ratio\n\n\n\n\nInversing odds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes odds ratio between normal and impaired glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\nNeeded steps:\n\nInverse point estimate and confidence interval\n\n\nlibrary(epitools)\noddsratio(x = SHS_ct, method = \"wald\", rev = \"rows\")$measure \n\n          odds ratio with 95% C.I.\n           estimate     lower     upper\n  Impaired 1.000000        NA        NA\n  Normal   0.215059 0.1667459 0.2773702"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-inversing-odds-ratio-2",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-inversing-odds-ratio-2",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Inversing Odds Ratio",
    "text": "SHS Example: Inversing Odds Ratio\n\n\n\n\nInversing odds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes odds ratio between normal and impaired glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\nNeeded steps:\n\nInterpret the estimate\n\nThe estimated odds of diabetes for American Indians with normal glucose tolerance at baseline is 0.22 times the odds for American Indians with impaired glucose tolerance at baseline.\n \nAdditional interpretation of 95% CI (not needed): We are 95% confident that the odds ratio is between 0.17 and 0.28.\n \nSince the 95% confidence interval does not include 1, there is sufficient evidence that the odds of diabetes differs significantly between impaired and normal glucose tolerance at baseline."
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#pubh-vs.-epitools",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#pubh-vs.-epitools",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "pubh vs. epitools",
    "text": "pubh vs. epitools\n\nIn pubh with contingency()\n\nGet all the info at once\nReally nice to double check how the code is interpreting your input\n\nIn epitools with riskratio() or oddsratio()\n\nMuch easier to grab the numbers!\nIn Quarto you can take R code and directly put it in your text\n\ng = oddsratio(x = SHS_ct, method = \"wald\", rev = \"rows\")\ng$measure[2,1]\n\n[1] 0.215059\n\n\n\nI can write {r eval=\"false\" echo=\"true\"} round(g$measure[2,1], 3) to print the number 0.215"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-1",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-1",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Fisher’s Exact test:",
    "text": "Fisher’s Exact test:\n\n\n\nCheck expected cell counts threshold\n\n\nexpected(GAC_table)\n\n     \n             No      Yes\n  No  143.21983 5.780172\n  Yes  79.78017 3.219828\n\n\nOne cell has an expected count less than 5\n\n\\(\\alpha = 0.05\\)\nHypothesis test:\n\n\\(H_0\\) : There is no association between glucose tolerance and diabetes\n\\(H_1\\) : There is an association between glucose tolerance and diabetes\n\n\n\n\nCalculate the test statistic and p-value for Fisher Exact test\n\n\nfisher.test(x = GAC_table)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  GAC_table\np-value = 0.2877\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  0.4829292 12.0164676\nsample estimates:\nodds ratio \n  2.314727 \n\n\n\nConclusion to the hypothesis test\n\nWe fail to reject the null hypothesis that estimated blood loss and postop neuropathy are not associated (\\(p=0.29\\)). There is insufficient evidence that estimated blood loss and postop neuropathy are associated."
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-2",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-2",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Fisher’s Exact test:",
    "text": "Fisher’s Exact test:\n\n\n\nCheck expected cell counts threshold\n\n\nexpected(GAC_table)\n\n     \n             No      Yes\n  No  143.21983 5.780172\n  Yes  79.78017 3.219828\n\n\nOne cell has an expected cell count less than 5\n\n\\(\\alpha = 0.05\\)\nHypothesis test:\n\n\\(H_0\\) : There is no association between glucose tolerance and diabetes\n\\(H_1\\) : There is an association between glucose tolerance and diabetes\n\n\n\n\nCalculate the test statistic and p-value for Chi-squared test in R\n\n\nfisher.test(x = GAC_table)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  GAC_table\np-value = 0.2877\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  0.4829292 12.0164676\nsample estimates:\nodds ratio \n  2.314727 \n\n\n\nConclusion to the hypothesis test\n\nWe reject the null hypothesis that glucose tolerance and diabetes are not associated (\\(p&lt;2.2\\cdot10^{-16}\\)). There is sufficient evidence that glucose tolerance and diabetes incidence are associated among American Indians."
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-postoperative-neuropathy-risk-in-gender-affirming-care",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-postoperative-neuropathy-risk-in-gender-affirming-care",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Fisher’s Exact test: Postoperative neuropathy risk in gender affirming care",
    "text": "Fisher’s Exact test: Postoperative neuropathy risk in gender affirming care\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEBL &gt; 250ml\n\nPostoperative Neuropathy\n\nTotal\n\n\nNo\nYes\n\n\n\n\nNo\n145\n4\n149\n\n\nYes\n78\n5\n83\n\n\nTotal\n223\n9\n232"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-postoperative-neuropathy-in-gender-affirming-care",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-postoperative-neuropathy-in-gender-affirming-care",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Fisher’s Exact test: Postoperative neuropathy in gender affirming care",
    "text": "Fisher’s Exact test: Postoperative neuropathy in gender affirming care\n\nStudy on new postoperative neuropathy after receiving gender affirming care (Study link)\n\nPostop neuropathy = nerve damage following surgery\n\nStudy comprised of 232 trans men and trans women receiving their respective hormone therapies\nThey measured different aspects of the surgery to see if there was an association with development of postop neuropathy\nGoal in this example: see if estimated blood loss, greater than or less than 250 ml, is associated with postop neuropathy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEBL &gt; 250ml\n\nPostoperative Neuropathy\n\nTotal\n\n\nNo\nYes\n\n\n\n\nNo\n145\n4\n149\n\n\nYes\n78\n5\n83\n\n\nTotal\n223\n9\n232"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-postop-neuropathy-in-gender-affirming-care-12",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-postop-neuropathy-in-gender-affirming-care-12",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Fisher’s Exact test: Postop neuropathy in gender affirming care (1/2)",
    "text": "Fisher’s Exact test: Postop neuropathy in gender affirming care (1/2)\n\nStudy on new postoperative neuropathy after receiving gender affirming care (Study link)\n\nPostop neuropathy = nerve damage following surgery\n\nStudy comprised of 232 trans men and trans women receiving their respective hormone therapies\nThey measured different aspects of the surgery to see if there was an association with development of postop neuropathy\nGoal in this example: see if estimated blood loss, greater than or less than 250 ml, is associated with postop neuropathy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEBL &gt; 250ml\n\nPostoperative Neuropathy\n\nTotal\n\n\nNo\nYes\n\n\n\n\nNo\n145\n4\n149\n\n\nYes\n78\n5\n83\n\n\nTotal\n223\n9\n232"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-postop-neuropathy-in-gender-affirming-care-22",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-postop-neuropathy-in-gender-affirming-care-22",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Fisher’s Exact test: Postop neuropathy in gender affirming care (2/2)",
    "text": "Fisher’s Exact test: Postop neuropathy in gender affirming care (2/2)\n\n\n\nCheck expected cell counts threshold\n\n\nexpected(GAC_table)\n\n     \n             No      Yes\n  No  143.21983 5.780172\n  Yes  79.78017 3.219828\n\n\nOne cell has an expected count less than 5\n\n\\(\\alpha = 0.05\\)\nHypothesis test:\n\n\\(H_0\\) : There is no association between glucose tolerance and diabetes\n\\(H_1\\) : There is an association between glucose tolerance and diabetes\n\n\n\n\nCalculate the test statistic and p-value for Fisher Exact test\n\n\nfisher.test(x = GAC_table)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  GAC_table\np-value = 0.2877\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  0.4829292 12.0164676\nsample estimates:\nodds ratio \n  2.314727 \n\n\n\nConclusion to the hypothesis test\n\nWe fail to reject the null hypothesis that estimated blood loss and postop neuropathy are not associated (\\(p=0.29\\)). There is insufficient evidence that estimated blood loss and postop neuropathy are associated."
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#different-tests-of-association",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#different-tests-of-association",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Different tests of association",
    "text": "Different tests of association"
  }
]