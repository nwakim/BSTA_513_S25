[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BSTA 513/613: Categorical Data Analysis",
    "section": "",
    "text": "BSTA 513/613: Categorical Data Analysis\n\nSpring 2025\n \nWelcome to BSTA 513/613! In this course, we will continue to learn about regression analysis, but not with categorical outcomes. We will build some theoretical understanding in order to interpret and apply logistic regression models appropriately. We will learn how to build a logistic regression model, interpret the model and coefficients, and diagnose potential issues with our model.  \n\n\n\n\n\n\n\n \n\n\n\n\n\n OneDrive Folder\n\n\n Echo360\n\n\n\n\n\n\n\nInstructor\n Dr. Nicky Wakim\n Vanport 622A\n wakim@ohsu.edu\n\n\nOffice Hours\nTBD\n\n\nCourse details\n Mondays, Wednesdays\n March 31 - June 9\n 1:00 PM - 2:50 PM\n In-person, RLSB 3A001\n\n\nContacting me\nE-mail or Slack is the best way to get in contact with me. I will try to respond to all course-related e-mails within 24 hours Monday-Friday.\n\n\n\n\n\n\n\n\n View the source on GitHub"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-1",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-1",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1\nMake sure to remember your answer!! We’ll use this on Wednesday!"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#review-of-test-of-association-12",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#review-of-test-of-association-12",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Review of Test of Association (1/2)",
    "text": "Review of Test of Association (1/2)\n\nLast week: learned some tests of association for contingency tables\n\n \n\nFor studies with two independent samples\n\nGeneral association\n\nChi-squared test\nFisher’s Exact test\n\nTest of trends\n\nCochran-Armitage test\nMantel-Haenszel test"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#review-of-test-of-association-22",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#review-of-test-of-association-22",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Review of Test of Association (2/2)",
    "text": "Review of Test of Association (2/2)"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#test-of-association-does-not-measure-association",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#test-of-association-does-not-measure-association",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Test of association does not measure association",
    "text": "Test of association does not measure association\n\nTest of association does not provide an effective measure of association. The p-value alone is not enough\n\n\\(\\text{p-value} &lt; 0.05\\) suggests there is a statistically significant association between the group and outcome\n\\(\\text{p-value} = 0.00001\\) vs. \\(\\text{p-value} = 0.01\\) does not mean the magnistude of association is different\n\n\n \n\nBut it does not tell how different the risks are between the two groups\n\n \n\nWe want to find out one or more measurements for quantifying the risks across the groups."
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#measures-of-association",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#measures-of-association",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Measures of Association",
    "text": "Measures of Association\n\nWhen we have a 2x2 contingency table and independent samples, we have an option of three measures of association:\n \n\nRisk difference (RD)\n\n \n\nRelative risk (RR)\n\n \n\nOdds ratio (OR)\n\n\n \nEach measures association by comparing the proportion of successes/failures from each categorical group of our explanatory variable."
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#before-we-discuss-each-further",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#before-we-discuss-each-further",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Before we discuss each further…",
    "text": "Before we discuss each further…\n \nLet’s define the cells within a 2x2 contingency table:\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nThen we can define risk: the proportion of “successes”\n\nWith \\(\\text{Risk}_1 = \\dfrac{n_{11}}{n_1}\\)"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#risk-difference-rd",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#risk-difference-rd",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Risk Difference (RD)",
    "text": "Risk Difference (RD)\n\nRisk difference computes the absolute difference in risk for the two groups (from the explanatory variable)\nPoint estimate: \\[\\widehat{RD} = \\widehat{p}_1 - \\widehat{p}_1 = \\dfrac{n_{11}}{n_1} - \\dfrac{n_{21}}{n_2}\\]\n\nWith range of point estimate from \\([-1, 1]\\)\n\nApproximate standard error:\n\n\\[ SE_{\\widehat{RD}} = \\sqrt{\\frac{\\hat{p}_1\\cdot(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2\\cdot(1-\\hat{p}_2)}{n_2}}\\]\n\n95% Wald confidence interval for \\(\\widehat{RD}\\):\n\n\\[\\widehat{RD} \\pm 1.96 \\cdot SE_{\\widehat{RD}}\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#recall-the-strong-heart-study",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#recall-the-strong-heart-study",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Recall the Strong Heart Study",
    "text": "Recall the Strong Heart Study\n\n\nThe Strong Heart Study is an ongoing study of American Indians residing in 13 tribal communities in three geographic areas (AZ, OK, and SD/ND). We will look at data from this study examining the incidence of diabetes at a follow-up visit and impaired glucose tolerance (ITG) at baseline (4 years apart).\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Risk Difference",
    "text": "SHS Example: Risk Difference\n\n\n\n\nRisk difference\n\n\nCompute the point estimate and 95% confidence interval for the diabetes risk difference between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\nNeeded steps:\n\nCompute the risk difference\nCompute 95% confidence interval\nInterpret the estimate"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-14",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-14",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Risk Difference (1/4)",
    "text": "SHS Example: Risk Difference (1/4)\n\n\n\n\nRisk difference\n\n\nCompute the point estimate and 95% confidence interval for the diabetes risk difference between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nCompute the risk difference \\[\\widehat{RD}={\\hat{p}}_1 - {\\hat{p}}_2=\\frac{n_{11}}{n_1}-\\frac{n_{21}}{n_2}=\\ \\frac{198}{532}\\ - \\frac{128}{1132}=0.3722−0.1131=0.2591\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-24",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-24",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Risk Difference (2/4)",
    "text": "SHS Example: Risk Difference (2/4)\n\n\n\n\nRisk difference\n\n\nCompute the point estimate and 95% confidence interval for the diabetes risk difference between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nCompute 95% confidence interval\n\n\\[\\begin{aligned} &\\widehat{RD}\\pm z_{\\left(1-\\frac{\\alpha}{2}\\right)}^\\ast \\times SE_{\\widehat{RD}} \\\\\n= &\\widehat{RD}\\pm z_{\\left(1-\\frac{\\alpha}{2}\\right)}^\\ast \\times \\sqrt{\\frac{{\\hat{p}}_1\\ (1-{\\hat{p}}_1)}{n_1}+\\frac{{\\hat{p}}_2(1-{\\hat{p}}_2)}{n_2}\\ }\\\\\n=  & 0.2591 \\pm 1.96\\times \\sqrt{\\frac{0.3722(1-0.3722)}{532}+\\frac{0.1131(1-0.1131)}{1132}\\ }\\\\\n=  & (0.2141,\\ 0.3041 )\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-34",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-34",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Risk Difference (3/4)",
    "text": "SHS Example: Risk Difference (3/4)\n\n\n\n\nRisk difference\n\n\nCompute the point estimate and 95% confidence interval for the diabetes risk difference between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n1/2. Compute risk difference and 95% confidence interval\n\nfmsb::riskdifference(198, 128, 532, 1132)\n\n                 Cases People at risk         Risk\nExposed    198.0000000    532.0000000    0.3721805\nUnexposed  128.0000000   1132.0000000    0.1130742\nTotal      326.0000000   1664.0000000    0.1959135\n\n\n\n    Risk difference and its significance probability (H0: The difference\n    equals to zero)\n\ndata:  198 128 532 1132\np-value &lt; 2.2e-16\n95 percent confidence interval:\n 0.2140779 0.3041346\nsample estimates:\n[1] 0.2591062"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-44",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-44",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Risk Difference (4/4)",
    "text": "SHS Example: Risk Difference (4/4)\n\n\n\n\nRisk difference\n\n\nCompute the point estimate and 95% confidence interval for the diabetes risk difference between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nInterpret the estimate\n\nThe diabetes diagnosis risk difference between impaired and normal glucose tolerance is 0.2591 (95% CI: 0.2141, 0.3041). Since the 95% confidence interval contains 0, we do not have sufficient evidence that the risk of diabetes diagnosis between impaired and normal glucose tolerance is different."
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#when-is-the-risk-difference-misleading",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#when-is-the-risk-difference-misleading",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "When is the risk difference misleading?",
    "text": "When is the risk difference misleading?\n\nThe same risk differences can have very different clinical meanings depending on the risk for each group\n\n \n \n\nExample: for two treatments A and B, we know the risk difference (RD) is 0.009. Is it a meaningful difference?\n\nIf the risk is 0.01 for Trt A and 0.001 for Trt B?\nIf the risk is 0.41 for Trt A and 0.401 for Trt B?\n\n\n \n\nUsing the RD alone to summarize the difference in risks for comparing the two groups can be misleading\n\nThe ratio of risk can provide an informative descriptive measure of the “relative risk”"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#relative-risk-rr",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#relative-risk-rr",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Relative Risk (RR)",
    "text": "Relative Risk (RR)\n\nRelative risk computes the ratio of each group’s proportions of “success”\n\nAlso called risk ratio    \n\nPoint estimate: \\[\\widehat{RR}=\\dfrac{\\hat{p}_1}{\\hat{p}_2} = \\dfrac{n_{11}/n_1}{n_{21}/n_2}\\]\n\nRange: \\([0, \\infty]\\)"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-2",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-2",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#log-transformation-of-rr",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#log-transformation-of-rr",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Log-transformation of RR",
    "text": "Log-transformation of RR\n\nSampling distribution of the relative risk is highly skewed unless sample sizes are quite large\n\nLog transformation results in approximately normal distribution\nThus, compute confidence interval using normally distributed, log-transformed RR\nThen we convert back to the RR\n\nWe take the log (natural log) of RR: \\(\\ln(\\widehat{RR})\\) or \\(log(\\widehat{RR})\\)\n\nWhenever I say “log” I mean natural log (very common in statistics)\n\n\n\n\n\nThen we need to find approximate standard error for \\(\\ln(\\widehat{RR})\\) \\[SE_{\\ln(\\widehat{RR})}=\\sqrt{\\frac{1}{n_{11}}\\ -\\frac{1}{n_1}+\\frac{1}{n_{21}}-\\frac{1}{n_2}}\\]\n95% confidence interval for \\(\\ln(\\widehat{RR})\\): \\[\\ln(\\widehat{RR}) \\pm 1.96 \\times SE_{\\ln(\\widehat{RR})}\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#how-do-we-get-back-to-the-rr-scale",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#how-do-we-get-back-to-the-rr-scale",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "How do we get back to the RR scale?",
    "text": "How do we get back to the RR scale?\n\nWe computed confidence interval using normally distributed, log-transformed RR (\\(\\ln(\\widehat{RR})\\)):\n\n\\[ \\bigg(\\ln(\\widehat{RR}) - 1.96 \\times SE_{\\ln(\\widehat{RR})}, \\ \\ln(\\widehat{RR}) + 1.96 \\times SE_{\\ln(\\widehat{RR})}\\bigg)\\]\n\nNow we need to exponentiate the CI to get back to interpretable values\n\nTake exponential of lower and upper bounds\n\n95% confidence interval for RR: two ways to display equation\n\n\\[ \\bigg(e^{\\ln(\\widehat{RR}) - 1.96 \\times SE_{\\ln(\\widehat{RR})}}, \\ e^{\\ln(\\widehat{RR}) + 1.96 \\times SE_{\\ln(\\widehat{RR})}}\\bigg)\\] \\[ \\bigg(\\exp\\big(\\ln(\\widehat{RR}) - 1.96 \\times SE_{\\ln(\\widehat{RR})}\\big), \\ \\exp\\big(\\ln(\\widehat{RR}) + 1.96 \\times SE_{\\ln(\\widehat{RR})}\\big)\\bigg)\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#relative-risk-rr-1",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#relative-risk-rr-1",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Relative Risk (RR)",
    "text": "Relative Risk (RR)\n\nCan you compute the estimated RRs for the previous example?\n\nIf the risk for Trt A is 0.01 and Trt B is 0.001? \\(\\widehat{RR}= 10\\)\nIf the risk for Trt A is 0.41 and Trt B is 0.401? \\(\\widehat{RR}= 1.02\\)\n\nWhen \\(\\widehat{RR}= 1\\) …\n\nRisk is the same for the two groups\nIn other words, the group and the outcome are independent\n\nWhen computing \\(\\widehat{RR}\\) it is important to identify which variable is the response variable and which is explanatory variable\n\nWe may say “risk for Trt A” but this translates to the risk (or probability) of outcome success for those receiving Trt A"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-16",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-16",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Relative Risk (1/6)",
    "text": "SHS Example: Relative Risk (1/6)\n\n\n\n\nRelative risk\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Relative risk between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\nNeeded steps:\n\nCompute the relative risk\nFind confidence interval of log RR\nConvert back to RR\nInterpret the estimate"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-26",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-26",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Relative Risk (2/6)",
    "text": "SHS Example: Relative Risk (2/6)\n\n\n\n\nRelative risk\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Relative risk between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nCompute the relative risk \\[\\widehat{RR}=\\dfrac{{\\hat{p}}_1}{{\\hat{p}}_2}=\\dfrac{n_{11}/{n_1}}{n_{21}/{n_2}}=\\ \\frac{ 198/532}{128/1132}=\\dfrac{0.3722}{0.1131}=3.2915\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-36",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-36",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Relative Risk (3/6)",
    "text": "SHS Example: Relative Risk (3/6)\n\n\n\n\nRelative risk\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Relative risk between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nFind confidence interval of log RR\n\n\\[\\begin{aligned} & \\ln(\\widehat{RR}) \\pm 1.96 \\times SE_{\\ln(\\widehat{RR})} \\\\\n= &\\ln(\\widehat{RR}) \\pm z_{\\left(1-\\frac{\\alpha}{2}\\right)}^\\ast \\times \\sqrt{\\frac{1}{n_{11}}\\ -\\frac{1}{n_1}+\\frac{1}{n_{21}}-\\frac{1}{n_2}}\\\\\n=  & 1.1913 \\pm 1.96\\times \\sqrt{\\frac{1}{198}\\ -\\frac{1}{532}+\\frac{1}{128}-\\frac{1}{1132}}\\\\\n=  & (0.9944,\\ 1.3883 )\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-46",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-46",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Relative Risk (4/6)",
    "text": "SHS Example: Relative Risk (4/6)\n\n\n\n\nRelative risk\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Relative risk between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nConvert back to RR\n\n\\[\\begin{aligned} & (\\exp(0.9944),\\ \\exp(1.3883 )) \\\\\n= & (2.703,\\ 4.0081 )\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-56",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-56",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Relative Risk (5/6)",
    "text": "SHS Example: Relative Risk (5/6)\n\n\n\n\nRelative risk\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Relative risk between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n1/2/3. Compute risk ratio and 95% confidence interval\n\nlibrary(epitools)\nSHS_ct = table(SHS$glucimp, SHS$case)\nriskratio(x = SHS_ct, rev = \"rows\")$measure\n\n          risk ratio with 95% C.I.\n           estimate    lower    upper\n  Normal   1.000000       NA       NA\n  Impaired 3.291471 2.702998 4.008061"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#pause-other-option-in-pubh-package",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#pause-other-option-in-pubh-package",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Pause: other option in pubh package",
    "text": "Pause: other option in pubh package\n\nSHS = SHS %&gt;% mutate(glucimp = as.factor(glucimp) %&gt;% relevel(ref = \"Normal\"))\ncontingency(case ~ glucimp, data = SHS)\n\n          Outcome\nPredictor     1    0\n  Impaired  198  334\n  Normal    128 1004\n\n             Outcome +    Outcome -      Total                 Inc risk *\nExposed +          198          334        532     37.22 (33.10 to 41.48)\nExposed -          128         1004       1132      11.31 (9.52 to 13.30)\nTotal              326         1338       1664     19.59 (17.71 to 21.58)\n\nPoint estimates and 95% CIs:\n-------------------------------------------------------------------\nInc risk ratio                                 3.29 (2.70, 4.01)\nInc odds ratio                                 4.65 (3.61, 6.00)\nAttrib risk in the exposed *                   25.91 (21.41, 30.41)\nAttrib fraction in the exposed (%)            69.62 (63.00, 75.05)\nAttrib risk in the population *                8.28 (5.63, 10.94)\nAttrib fraction in the population (%)         42.28 (34.71, 48.98)\n-------------------------------------------------------------------\nUncorrected chi2 test that OR = 1: chi2(1) = 154.239 Pr&gt;chi2 = &lt;0.001\nFisher exact test that OR = 1: Pr&gt;chi2 = &lt;0.001\n Wald confidence limits\n CI: confidence interval\n * Outcomes per 100 population units \n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  dat\nX-squared = 152.6, df = 1, p-value &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-66",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-66",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Relative Risk (6/6)",
    "text": "SHS Example: Relative Risk (6/6)\n\n\n\n\nRelative risk\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Relative risk between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nInterpret the estimate\n\nThe estimated risk of diabetes is 3.29 times greater for American Indians who had impaired glucose tolerance at baseline compared to those who had normal glucose tolerance (95% CI: 2.70, 4.01).\n \nAdditional interpretation of 95% CI (not needed): We are 95% confident that the (population) relative risk is between 2.70 and 4.01.\n \nSince the 95% confidence interval does not include 1, there is sufficient evidence that the risk of diabetes differs significantly between impaired and normal glucose tolerance at baseline."
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#odds-building-up-to-odds-ratio",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#odds-building-up-to-odds-ratio",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Odds (building up to Odds Ratio)",
    "text": "Odds (building up to Odds Ratio)\n\nFor a probability of success \\(p\\) (or sometimes referred to as \\(\\pi\\)), the odds of success is: \\[\\text{odds}=\\frac{p}{1-p}=\\frac{\\pi}{1-\\pi}\\]\n\nExample: if \\(\\pi=0.75\\), then odds of success \\(= \\dfrac{0.75}{0.25}=3\\)\n\nIf odds &gt; 1, it implies a success is more likely than a failure\n\nExample: for \\(odds = 3\\), we expect to observe three times as many successes as failures\n\nIf odds is known, the probability of success can be computed \\[\\pi = \\dfrac{\\text{odds}}{\\text{odds}+1}\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#odds-ratio-or",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#odds-ratio-or",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Odds Ratio (OR)",
    "text": "Odds Ratio (OR)\n\nOdds ratio is the ratio of two odds:\\[\\widehat{OR}=\\frac{odds_1}{odds_2}=\\frac{{\\hat{p}}_1/(1-{\\hat{p}}_1)}{{\\hat{p}}_2/(1-{\\hat{p}}_2)}\\]\n\nRange: \\([0, \\infty]\\)\nInterpretation: The odds of success for “group 1” is “\\(\\widehat{OR}\\)” times the odds of success for “group 2”\n\n\n \n\nWhat do values of odds ratios mean?\n\n\n\n\n\n\n\nOdds Ratio\nClinical Meaning\n\n\n\n\n\\(\\widehat{OR} &lt; 1\\)\nOdds of success is smaller in group 1 than in group 2\n\n\n\\(\\widehat{OR} = 1\\)\nExplanatory and response variables are independent\n\n\n\\(\\widehat{OR} &gt; 1\\)\nOdds of success is greater in group 1 than in group 2"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-3",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-3",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#odds-ratio-or-1",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#odds-ratio-or-1",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Odds Ratio (OR)",
    "text": "Odds Ratio (OR)\n\nValues of OR farther from 1.0 in a given direction represent stronger association\n\nAn OR = 4 is farther from independence than an OR = 2\nAn OR = 0.25 is farther from independence than an OR = 0.5\nFor OR = 4 and OR = 0.25, they are equally away from independence (because ¼ = 0.25)\n\n\n \n\nWe take the inverse of the OR for success of group 1 compared to group 2 to get…\n\nOR for failure of group 1 compared to group 2\nOR for success of group 2 compared to group 1"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#log-transformation-of-or",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#log-transformation-of-or",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Log-transformation of OR",
    "text": "Log-transformation of OR\n\nLike RR, sampling distribution of the odds ratio is highly skewed\n\nLog transformation results in approximately normal distribution\nThus, compute confidence interval using normally distributed, log-transformed OR\n\n\n\n\n\nApproximate standard error for \\(\\ln (\\widehat{OR})\\): \\[SE_{\\ln(\\widehat{OR})}=\\sqrt{\\frac{1}{n_{11}}\\ +\\frac{1}{n_{12}}+\\frac{1}{n_{21}}+\\frac{1}{n_{22}}}\\]\n95% confidence interval for \\(\\ln(\\widehat{OR})\\): \\[\\ln(\\widehat{OR}) \\pm 1.96 \\times SE_{\\ln(\\widehat{OR})}\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#how-do-we-get-back-to-the-or-scale",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#how-do-we-get-back-to-the-or-scale",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "How do we get back to the OR scale?",
    "text": "How do we get back to the OR scale?\n\nWe computed confidence interval using normally distributed, log-transformed RR (\\(\\ln(\\widehat{OR})\\)):\n\n\\[ \\bigg(\\ln(\\widehat{OR}) - 1.96 \\times SE_{\\ln(\\widehat{OR})}, \\ \\ln(\\widehat{OR}) + 1.96 \\times SE_{\\ln(\\widehat{OR})}\\bigg)\\]\n\nNow we need to exponentiate the CI to get back to interpretable values\n\nTake exponential of lower and upper bounds\n\n95% confidence interval for RR: two ways to display equation\n\n\\[ \\bigg(e^{\\ln(\\widehat{OR}) - 1.96 \\times SE_{\\ln(\\widehat{OR})}}, \\ e^{\\ln(\\widehat{OR}) + 1.96 \\times SE_{\\ln(\\widehat{OR})}}\\bigg)\\] \\[ \\bigg(\\exp\\big(\\ln(\\widehat{OR}) - 1.96 \\times SE_{\\ln(\\widehat{OR})}\\big), \\ \\exp\\big(\\ln(\\widehat{OR}) + 1.96 \\times SE_{\\ln(\\widehat{OR})}\\big)\\bigg)\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-16",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-16",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Odds Ratio (1/6)",
    "text": "SHS Example: Odds Ratio (1/6)\n\n\n\n\nOdds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes odds ratio between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\nNeeded steps:\n\nCompute the odds ratio\nFind confidence interval of log OR\nConvert back to OR\nInterpret the estimate"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-26",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-26",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Odds Ratio (2/6)",
    "text": "SHS Example: Odds Ratio (2/6)\n\n\n\n\nOdds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Odds ratio between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nCompute the odds ratio\n\n\\(\\widehat{p}_1 = 198/532 = 0.3722\\), \\(\\widehat{p}_2 = 128/1132 = 0.1131\\) \\[\\widehat{OR}=\\frac{\\widehat{p_1}/(1-\\widehat{p_1})}{\\widehat{p_2}/(1-\\widehat{p_2})}= \\dfrac{0.3722/(1-0.3722)}{0.1131/(1-0.1131)}= 4.6499\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-36",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-36",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Odds Ratio (3/6)",
    "text": "SHS Example: Odds Ratio (3/6)\n\n\n\n\nOdds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Odds ratio between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nFind confidence interval of log OR\n\n\\[\\begin{aligned} & \\ln(\\widehat{OR}) \\pm 1.96 \\times SE_{\\ln(\\widehat{OR})} \\\\\n= &\\ln(\\widehat{OR}) \\pm z_{\\left(1-\\frac{\\alpha}{2}\\right)}^\\ast \\times \\sqrt{\\frac{1}{n_{11}}\\ +\\frac{1}{n_{12}}+\\frac{1}{n_{21}}+\\frac{1}{n_{22}}}\\\\\n=  & 1.5368 \\pm 1.96\\times \\sqrt{\\frac{1}{198}\\ +\\frac{1}{334}+\\frac{1}{128}+\\frac{1}{1004}}\\\\\n=  & (1.2824,\\ 1.7913 )\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-46",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-46",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Odds Ratio (4/6)",
    "text": "SHS Example: Odds Ratio (4/6)\n\n\n\n\nOdds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Odds ratio between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nConvert back to OR\n\n\\[\\begin{aligned} & (\\exp(1.2824),\\ \\exp(1.7913 )) \\\\\n= & (3.6053,\\ 5.9971 )\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-56",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-56",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Odds Ratio (5/6)",
    "text": "SHS Example: Odds Ratio (5/6)\n\n\n\n\nOdds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Odds ratio between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n1/2/3. Compute OR and 95% confidence interval\n\nlibrary(epitools)\nSHS_ct = table(SHS$glucimp, SHS$case)\n# no `rev` needed below bc we set the reference level in slide 32\noddsratio(x = SHS_ct, method = \"wald\")$measure \n\n          odds ratio with 95% C.I.\n           estimate    lower    upper\n  Normal   1.000000       NA       NA\n  Impaired 4.649888 3.605289 5.997148"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#pause-other-option-in-pubh-package-1",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#pause-other-option-in-pubh-package-1",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Pause: other option in pubh package",
    "text": "Pause: other option in pubh package\n\ncontingency(case ~ glucimp, data = SHS, digits = 3)\n\n          Outcome\nPredictor     1    0\n  Impaired  198  334\n  Normal    128 1004\n\n             Outcome +    Outcome -      Total                 Inc risk *\nExposed +          198          334        532  37.218 (33.097 to 41.482)\nExposed -          128         1004       1132   11.307 (9.521 to 13.298)\nTotal              326         1338       1664  19.591 (17.709 to 21.581)\n\nPoint estimates and 95% CIs:\n-------------------------------------------------------------------\nInc risk ratio                                 3.291 (2.703, 4.008)\nInc odds ratio                                 4.650 (3.605, 5.997)\nAttrib risk in the exposed *                   25.911 (21.408, 30.413)\nAttrib fraction in the exposed (%)            69.618 (63.004, 75.050)\nAttrib risk in the population *                8.284 (5.631, 10.937)\nAttrib fraction in the population (%)         42.284 (34.713, 48.976)\n-------------------------------------------------------------------\nUncorrected chi2 test that OR = 1: chi2(1) = 154.239 Pr&gt;chi2 = &lt;0.001\nFisher exact test that OR = 1: Pr&gt;chi2 = &lt;0.001\n Wald confidence limits\n CI: confidence interval\n * Outcomes per 100 population units \n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  dat\nX-squared = 152.6, df = 1, p-value &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-66",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-66",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Odds Ratio (6/6)",
    "text": "SHS Example: Odds Ratio (6/6)\n\n\n\n\nOdds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Odds ratio between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nInterpret the estimate\n\nThe estimated odds of diabetes for American Indians with impaired glucose tolerance at baseline is 4.65 times the odds for American Indians with normal glucose tolerance at baseline.\n \nAdditional interpretation of 95% CI (not needed): We are 95% confident that the odds ratio is between 3.61 and 6.00.\n \nSince the 95% confidence interval does not include 1, there is sufficient evidence that the odds of diabetes differs significantly between impaired and normal glucose tolerance at baseline."
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#inversing-an-odds-ratio",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#inversing-an-odds-ratio",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Inversing an Odds Ratio",
    "text": "Inversing an Odds Ratio\n\nSome clinicians may prefer interpretations of OR &gt; 1 instead of an OR &lt; 1\nThe transformation can easily be done by inverse\n\nRemember we discussed that OR = 4 is an equivalent a strong association as OR = 0.25 (1/4)\n\nOR comparing group 1 to group 2 = inverse of OR comparing group 2 to group 1\n\n\\[ OR_{1v2}=\\frac{{\\hat{p}}_1/(1-{\\hat{p}}_1)}{{\\hat{p}}_2/(1-{\\hat{p}}_2)}=\\frac{1}{\\frac{{\\hat{p}}_2/(1-{\\hat{p}}_2)}{{\\hat{p}}_1/(1-{\\hat{p}}_1)}}=\\frac{1}{OR_{2v1}}\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-4",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-4",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-inversing-odds-ratio",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-inversing-odds-ratio",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Inversing Odds Ratio",
    "text": "SHS Example: Inversing Odds Ratio\n\n\n\n\nInversing odds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes odds ratio between normal and impaired glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\nNeeded steps:\n\nInverse point estimate and confidence interval\n\n\\[\\widehat{OR}=\\frac{1}{4.6499}=0.2151\\] The 95% Confidence interval is then\n\\[ \\left(\\frac{1}{5.9971}, \\frac{1}{3.6053}\\right)\\ =\\ (0.1667, 0.2774)\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-inversing-odds-ratio-1",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-inversing-odds-ratio-1",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Inversing Odds Ratio",
    "text": "SHS Example: Inversing Odds Ratio\n\n\n\n\nInversing odds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes odds ratio between normal and impaired glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\nNeeded steps:\n\nInverse point estimate and confidence interval\n\n\nlibrary(epitools)\noddsratio(x = SHS_ct, method = \"wald\", rev = \"rows\")$measure \n\n          odds ratio with 95% C.I.\n           estimate     lower     upper\n  Impaired 1.000000        NA        NA\n  Normal   0.215059 0.1667459 0.2773702"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-inversing-odds-ratio-2",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-inversing-odds-ratio-2",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Inversing Odds Ratio",
    "text": "SHS Example: Inversing Odds Ratio\n\n\n\n\nInversing odds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes odds ratio between normal and impaired glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\nNeeded steps:\n\nInterpret the estimate\n\nThe estimated odds of diabetes for American Indians with normal glucose tolerance at baseline is 0.22 times the odds for American Indians with impaired glucose tolerance at baseline.\n \nAdditional interpretation of 95% CI (not needed): We are 95% confident that the odds ratio is between 0.17 and 0.28.\n \nSince the 95% confidence interval does not include 1, there is sufficient evidence that the odds of diabetes differs significantly between impaired and normal glucose tolerance at baseline."
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#pubh-vs.-epitools",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#pubh-vs.-epitools",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "pubh vs. epitools",
    "text": "pubh vs. epitools\n\nIn pubh with contingency()\n\nGet all the info at once\nReally nice to double check how the code is interpreting your input\n\nIn epitools with riskratio() or oddsratio()\n\nMuch easier to grab the numbers!\nIn Quarto you can take R code and directly put it in your text\n\ng = oddsratio(x = SHS_ct, method = \"wald\", rev = \"rows\")\ng$measure[2,1]\n\n[1] 0.215059\n\n\n\nI can write {r eval=\"false\" echo=\"true\"} round(g$measure[2,1], 3) to print the number 0.215"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Week\nDate\nLes-son\nTopic\nKey Info\nSlides HTML\nSlides PDF\nSlides Notes\nExit tix\nEcho 360\nMuddy Points\n\n\n\n\n1\n3/31\n1\nWelcome\n\n\n\n\n\n\n\n\n\n\n\n2\nIntroduction to Categorical Analysis\n\n\n\n\n\n\n\n\n\n\n4/2\n3\nMeasurement of Association for Contingency Tables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2\n4/7\n4\nMeasurements of Association and Agreement\n\n\n\n\n\n\n\n\n\n\n4/9\n5\nSimple Logistic Regression\n\n\n\n\n\n\n\n\n\n\n4/11\n\nLab 1 due @ 11 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3\n4/14\n6\nTests for GLMs using Likelihood function\n\n\n\n\n\n\n\n\n\n\n4/16\n7\nPredictions and Visualizations in Simple Logistic Regression\n\n\n\n\n\n\n\n\n\n\n\n\nRequired in-person attendance: Lab 1 Discussion\n\n\n\n\n\n\n\n\n\n\n4/18\n\nHW 1 due @ 11 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4\n4/21\n8\nInterpretations and Visualizations of Odds Ratios\n\n\n\n\n\n\n\n\n\n\n4/23\n10\nMultiple Logistic Regression\n\n\n\n\n\n\n\n\n\n\n4/23\n\nQuiz 1 opens at 3pm\n\n\n\n\n\n\n\n\n\n\n4/25\n\nLab 2 due @ 11 pm\n\n\n\n\n\n\n\n\n\n\n4/27\n\nQuiz 1 closes at 11pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5\n4/28\n11\nInteractions\n\n\n\n\n\n\n\n\n\n\n4/30\n\nRequired in-person attendance: Lab 2 Discussion\n\n\n\n\n\n\n\n\n\n\n5/2\n\nHW 2 due @ 11 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6\n5/5\n12\nAssessing Model Fit\n\n\n\n\n\n\n\n\n\n\n5/7\n13\nNumerical Problems\n\n\n\n\n\n\n\n\n\n\n5/9\n\nLab 3 due @ 11 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7\n5/12\n14\nModel Diagnostics\n\n\n\n\n\n\n\n\n\n\n\n\nRequired in-person attendance: Lab 3 Discussion\n\n\n\n\n\n\n\n\n\n\n5/14\n15\nModel Building\n\n\n\n\n\n\n\n\n\n\n5/14\n\nQuiz 2 opens at 3pm\n\n\n\n\n\n\n\n\n\n\n5/16\n\nHW 3 due @ 11 pm\n\n\n\n\n\n\n\n\n\n\n5/18\n\nQuiz 2 closes at 11pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8\n5/19\n16\nPoisson Regression, potentially virtual\n\n\n\n\n\n\n\n\n\n\n5/21\n17\nOther types of categorical regressions!, , potentially virtual\n\n\n\n\n\n\n\n\n\n\n5/23\n\nLab 4 due 11pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9\n5/26\n\nNo class, Memorial Day\n\n\n\n\n\n\n\n\n\n\n5/28\n18\nOther types of categorical regressions: More Analysis\n\n\n\n\n\n\n\n\n\n\n\n\nRequired in-person attendance: Lab 4 Discussion\n\n\n\n\n\n\n\n\n\n\n5/28\n\nQuiz 3 opens at 3pm\n\n\n\n\n\n\n\n\n\n\n5/30\n\nHW 4 due 11 pm\n\n\n\n\n\n\n\n\n\n\n6/1\n\nQuiz 3 closes at 11pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10\n6/2\n9\nMissing Data\n\n \n\n\n\n\n\n\n\n\n6/4\n\nCatch-up day?\n\n\n\n\n\n\n\n\n\n\n6/6\n\nHW 5 due 11 pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n11\n6/9\n\nRequired in-person attendance: Project Day!!\n\n\n\n\n\n\n\n\n\n\n6/9\n\nProject due 11pm\n\n\n\n\n\n\n\n\n\n\n6/15\n\nAll late coursework due 11pm"
  },
  {
    "objectID": "lessons.html",
    "href": "lessons.html",
    "title": "Lessons",
    "section": "",
    "text": "Lesson\n1\nTopic\nFile Organization within R\nSlides | :=======================================================================================================================================================================================================================:+  |\n\n\n\n\n\n\n\n2\nIntroduction to Categorical Analysis\n |\n\n\n3\nMeasurement of Association for Contingency Tables\n |\n\n\n4\nMeasurements of Association and Agreement\n |\n\n\n5\nSimple Logistic Regression\n |\n\n\n6\nTests for GLMs using Likelihood function\n |\n\n\n7\nPredictions and Visualizations in Simple Logistic Regression\n |\n\n\n8\nInterpretations and Visualizations of Odds Ratios\n |\n\n\n9\nMissing Data\nActivity:\n  | |\n\n\n10\nMultiple Logistic Regression\n |\n\n\n11\nInteractions\n |\n\n\n12\nAssessing Model Fit\n |\n\n\n13\nNumerical Problems\n |\n\n\n14\nModel Diagnostics\n |\n\n\n15\nModel Building\n |\n\n\n16\nPoisson Regression\n |\n\n\n17\nOther types of categorical regressions!\n |"
  },
  {
    "objectID": "syllabus.html#description",
    "href": "syllabus.html#description",
    "title": "BSTA 513/613 Syllabus",
    "section": "Description",
    "text": "Description\nWelcome to BSTA 513/613! In this course, we will continue to learn about regression analysis, but not with categorical outcomes. We will build some theoretical understanding in order to interpret and apply logistic regression models appropriately. We will learn how to build a logistic regression model, interpret the model and coefficients, and diagnose potential issues with our model.\n\nCourse Learning Objectives\nAt the end of this course, students should be able to…\n\nApply and interpret a variety of hypothesis-testing procedures for two-way and three-way contingency tables\nCompute and interpret measures of association for binary and ordinal data.\nCalculate and correctly interpret odds ratios using logistic regression, make comparison across groups and examine relationship between binary outcome and predictor variables.\nApply appropriate model-building strategies for logistic regression. Effectively use statistical computing packages for contingency table and logistic regression procedures.\nPerform Poisson regression analysis using count data and interpret model estimates, make comparison across groups and examine relationship between outcome and predictor variables.\nCoherently summarize methods and results of data analyses, and discuss in context of original health-related research questions to audiences with varied statistical background."
  },
  {
    "objectID": "syllabus.html#instructors",
    "href": "syllabus.html#instructors",
    "title": "BSTA 513/613 Syllabus",
    "section": "Instructors",
    "text": "Instructors\nHere is the instructor page. This also has office hours!"
  },
  {
    "objectID": "syllabus.html#meeting-times",
    "href": "syllabus.html#meeting-times",
    "title": "BSTA 513/613 Syllabus",
    "section": "Meeting Times",
    "text": "Meeting Times\nMondays          1:00 PM – 2:50 PM PST in RLSB 3A001\nWednesdays    1:00 PM – 2:50 PM PST in RLSB 3A001"
  },
  {
    "objectID": "syllabus.html#materials",
    "href": "syllabus.html#materials",
    "title": "BSTA 513/613 Syllabus",
    "section": "Materials",
    "text": "Materials\n\nTextbooks\n\nApplied Logistic Regression by David Hosmer, Stanley Lemeshow, and Rodney Sturdivant\nAn Introduction to Categorical Data Analysis by Alan Agresti\nIntroduction to Regression Methods for Public Health Using R by Ramzi W. Nahhas\n\nSpecifically Chapter 6\n\n\n\nSupplemental Readings (Optional)\n\nAn Introduction to R\n\n\n\n\nOnline Resources\n\nSakai\nWhile most course materials will be delivered online through this website, assignments will be turned in through Sakai, OHSU’s course management system. I will include a link on this website to the Sakai assignment page. \n\n\nWebex\nWebex software will be used for virtual office hours. To give everyone the best possible experience with Webex, I recommend the following best practices:\n\nPlease stay muted until you want to participate\nDuring office hours, please send a message in chat with your question or with a statement like “I have a question.” This makes sure I or the TA can address everyone’s questions in order. \nI encourage you to attend office hours with your video on. This helps me recognize you, and keep mental notes on what techniques/concepts I emphasize to facilitate your specific understanding. \n\n\n\nPoll Everywhere\nWe will use the Poll Everywhere tool as an interactive feature of the course. Poll Everywhere is a web-based application that allows students to participate by responding via text messages or by visiting a web page on an internet-enabled device (smartphone, tablet, laptop). Instructions will be displayed on-screen. The poll that is embedded within the presentation will update in real time. While there is no cost to use this software, standard text messaging rates will apply if you use your phone. Please make sure that you have a Poll Everywhere account before our first class. You are not required to use your OHSU/PSU email to make an account. \nDuring lectures I will pose questions to the class. These questions are designed to provide real-time feedback to both students and the instructor on how well students are grasping the material. This is meant to be an interactive, learning activity with NO contribution to your grade. Your identity will never be connected to your answers, so I encourage you to answer honestly.\n\n\nPennState STAT 504 Website\nPennState has a class offered to online MS students that has some overlap with our class. They have all their course notes posted on this page. This is a great source if you would like to see class notes with different phrasing.\nNot all of our topics are covered in their notes, but the most important ones are. If you are having trouble finding our course’s concepts on their page, please make ask me at Office Hours, after class, or in a private meeting. I do not explicitly state corresponding sections under our schedule because I believe it is important for you to develop skills involving resources and learning key words that can help you find answers. \n\n\nR: Statistical Computing Software\nStudents will use statistical software to complete homework assignments. Students are required to use R/RStudio for this course. R can be freely downloaded. Helpful documentation on installing R is available. I encourage you to install R prior to attending our first lecture. Please email me if you need help installing R or RStudio.\nYou will need to download the following three things:\n\nR https://www.r-project.org/\nRstudio https://posit.co/download/rstudio-desktop/\nQuarto https://quarto.org/docs/get-started/\n\n\nAdditional R Resources\nYour learning and practicing of R will hopefully not be limited to this course. One of the best aspects of programming in R is that many resources are freely available online. Here are just a few additional resources you may explore beyond this class to continue your training in R.\n\n\nUseful online R resources\n\nR for the rest of us\nStatistical tools for high-throughput data analysis. ggplot2 essentials\nR-bloggers\nStack Overflow for troubleshooting\nR Graphical Manual\nQuick-R. Accessing the power of R\nR for SAS, STATA, and SPSS Users\nggplot2\nLearn R 4 free\nJoin a local R user groups\nLearning Machines\n\n\n\n\nOnline R courses to complement or refresh material from class\n\nR for the rest of us\nCoursera: R programming\nedX: R basics\nData Carpentry: For Biologists\nData Carpentry: For Ecologists\nPsychiatric R\nR coder"
  },
  {
    "objectID": "syllabus.html#assessment",
    "href": "syllabus.html#assessment",
    "title": "BSTA 513/613 Syllabus",
    "section": "Assessment",
    "text": "Assessment\nThe course is structured around the following four components:\n\n\n\n\n\n\n\n\n\nComponent\nModality\nFrequency\nDescription\n\n\nLecture\nIn person\nTwice, Weekly\nCourse content is provided through in-person lectures. Lectures will consist of didactic lessons, interactive examples, and PollEverywhere questions. Sessions will be recorded through Explain Everything and posted to Sakai. Attending or viewing the lecture within 7 days of the original lecture date is mandatory. Class attendance will be taken through an Exit Ticket. If viewing the lecture asynchronously, you must take the Exit Ticket to verify your attendance.\n\n\nHomework\nOnline\nWeekly\nThe course includes 5 homework assignments. They are an opportunity for you to engage with important concepts, practice coding, and apply calculating skills. Homework assignments should be submitted online, and will be graded by me. Students are encouraged to work in groups for homework assignments, but each person should do their own summary and hand in their work. Homework assignments will be due on Friday at 11 PM.\n\n\nQuizzes\nOnline\nEvery 3 weeks\nThe purpose of the quizzes is to assess how well you have achieved the learning objectives through questions covering important concepts, conducting statistical processes, and interpreting output. We will have our quizzes in-class, and it will be open book. Students must work on the quizzes independently.\n\n\nProject (Labs and Poster)\nOnline\n4 labs, 1 final poster\nThe project will be a combination of submitted labs that will span the quarter and one final report submitted at the end of the quarter. This is meant to translate the tools learned in the course to the work one may do in the workforce. This will help instill the procedure for shaping research goals, model selection, analyzing data, and interpreting meaningful results. Labs will guide you through the needed analysis and background for the project. The final poster will summarize your work over the labs by giving context and results to your research question. Students will work independently on each lab.\n\n\n\n\nTypes of assessments\nThis class will use a combination of formative and summative assessments to build and test our knowledge. Below I define each of these types of assessments:\n\nFormative assessment: Activity or work meant to help students learn and practice. Feedback on these assessments are meant to help the instructor and student identify gaps in knowledge and highlight accomplishments.\nSummative assessment: Work meant to test how well students have achieved learning objectives. Grading of these assessments are meant to gauge how well a student grasps the learning objectives and will be able to use their knowledge outside of the classroom."
  },
  {
    "objectID": "syllabus.html#assessment-breakdown",
    "href": "syllabus.html#assessment-breakdown",
    "title": "BSTA 513/613 Syllabus",
    "section": "Assessment Breakdown",
    "text": "Assessment Breakdown\n\nGrading & Requirements\nLetter grades will be assigned roughly according to the following scheme: A (&gt;=93%), A- (90-92%), B+ (88-89%), B(83-87%), B- (82-80%), C+(78-79%), C(73-77%), C- (70-72%), D (60 – 69%), F(&lt;60%).\nGrades will be based on homework assignments, midterm exam, class “attendance”, and final exam, as follows:\n\n\n\n\n\n\n\n\n\n\nCourse activity\nType of Assessment\nDue Date\nPercentage of final grade (BSTA 513)\nPercentage of final grade (BSTA 613)\n\n\nHomework\nFormative\nEvery 1-2 weeks\n30%\n25%\n\n\nQuizzes\nSummative\n4/27, 5/18, 6/1\n25%\n25%\n\n\nProject Labs\nFormative/Summative\nEvery 2-3 weeks\n25%\n25%\n\n\nLab Discussion + Poster Day Participation\nN/A\nEvery 2-3 weeks\n5%\n5%\n\n\nProject Poster\nSummative\n6/9\n10%\n10%\n\n\nExit tickets (Attendance)\nN/A\nTwice Weekly\n5%\n5%\n\n\n613 Readings\nFormative\nApprox. every other week\n0%\n5%\n\n\n\n\n\nHomework grading\nNo student has the same amount of time available to dedicate to homework. This class may not be a priority to you, you may be taking several other courses, or you may need to dedicate time to other activities. Homeworks are formative assessments, meaning its purpose is to help you learn and practice. To reduce the pressure on you to have perfect or complete homework, I have a very simple grading policy: Your homework will be given a check mark if you turn in 75% of the questions parts completed (whether the 75% is correct or wrong). I highly encourage you to stay up-to-date with the homeworks and put in as much effort as you can. This will be the most helpful work in this class!\nIf you turn in the homework on time, the TAs will give you feedback (on one or more complete problems). There is no penalty for turning in the homework late, but you will not get feedback on your work. Please make sure to check the solutions or go to office hours to assess your work.\n\n\nViewing Grades in Sakai\nPoints you receive for graded activities will be posted to the Sakai Gradebook. Click on the Gradebook link on the left navigation to view your points."
  },
  {
    "objectID": "syllabus.html#course-instructor-evaluations",
    "href": "syllabus.html#course-instructor-evaluations",
    "title": "BSTA 513/613 Syllabus",
    "section": "Course & Instructor Evaluations",
    "text": "Course & Instructor Evaluations\n\nOngoing Course Feedback\nThroughout the duration of the course, you are also welcome to informally and anonymously submit your feedback through this Microsoft Form or Class Exit Tickets. This form will be available on Sakai. Students can submit feedback at any time and this form will be reviewed regularly by me. Your responses will be anonymous unless you elect to leave your email address. If I have done anything to make you feel uncomfortable, please give me feedback so I can change my behavior. Ultimately, this class is for you, and my individual social identity/behavior should not inhibit your learning. Thank you for your help making BSTA 513/613 a more successful class! Examples of ongoing feedback are:\n\nNicky talks a little fast during lecture time. May you speak slower?\nDuring Office Hours, Dr. Wakim made a face when I asked a question. This face made me feel self-conscious about my question.\nDr. W asked me a question about my experience that made me feel like a monolith. Please do not assume I can speak on behalf of my social identity groups.\nThe in-class examples do not make me more interested in the material.\n\n\n\nFinal Course Feedback\nAt the conclusion of the course, you will be asked to complete a formal online review of the course and the instructor. Your feedback on this University evaluation is critical to improving future student learning in this course as well as providing metrics relevant to the instructor’s career advancement (or lack of). Since our class is on the smaller side, everyone’s participation is needed for feedback to be released."
  },
  {
    "objectID": "syllabus.html#schedule",
    "href": "syllabus.html#schedule",
    "title": "BSTA 513/613 Syllabus",
    "section": "Schedule",
    "text": "Schedule\nPlease refer to the Schedule page. I will make changes to this schedule if we need more or less time on a concept. You do not need to read the corresponding chapters in the textbook for each class."
  },
  {
    "objectID": "syllabus.html#how-to-succeed-in-this-course",
    "href": "syllabus.html#how-to-succeed-in-this-course",
    "title": "BSTA 513/613 Syllabus",
    "section": "How to succeed in this course",
    "text": "How to succeed in this course\nEvery professor has different expectations when assigning certain work or providing certain resources. I want to walk through each class resource and assignment so that you know what you can do to succeed in this class. For resources, I want you to optimize the opportunities to learn. For assignments, I want you to know the strategies that students can use to learn the most and prepare for future exams.\n\nResources\n\n\n\n\n\n\n\n\nResource\nWhat is it?\nHow do I use it?\n\n\nOffice Hours\nBlocks of time a professor or TA dedicates for questions. The teaching staff will be located in a specific room. Several students may enter the space at a time and will ask specific or broad questions. If many students attend office hours, a queue will be created so that students can be served equally.\nThe main use of office hours is to ask questions about an assignment or lecture notes. You are welcome to sit and do homework in office hours. OH are also an informal way of meeting fellow students to collaborate with.\n\n\nLectures and lecture recordings\nTime shared between the professor and students where the professor conveys important class material. Material discussed in lectures include concepts, calculations, code, and examples. Lectures are a mix of presentation of information, working through examples together, interactive activities, and in-class polls.\nStudents should attend lectures in person if possible. You should attempt to understand new material presented by following the presentation slides, taking notes on additional details that may conveyed verbally, and working through examples with the professor. Students are encouraged to ask questions when you don’t understand the material at any point in the lecture.\n\n\nTextbooks\nWritten and published material that explains concepts, steps through calculations, provides examples, and provides practice problems. The listed textbooks is the basis for this course. While I am to cover all topics in class, the textbook provides alternative explanations and additional examples.\nWhile coming to class having read the accompanying textbook chapters helps understanding during class, I do not expect students to have read it. I see the textbook as a good resource if you are struggling with a specific topic after class, in need of an example while working on homework, or want additional practice when studying for the exam.\n\n\nWebsite\nThe course website is designed by me so that you have access to all the course materials in a more organized and flexible way. All resources delivered from me to you will be available on the website. Any assignments turned in will be through Sakai.\nYou can navigate through different course resources and information using the left-side tabs or top navigation bar. Course materials, like lecture notes, homework, data examples, and recordings, can be found under each week’s page under the schedule tab. You can also find the individual resources under the “Course Materials” tab on the left. Links to turn in assignments through Sakai will be given on the website. Please explore the tabs and get a sense of the organization.\n\n\nSakai\nSakai is a learning management system for higher ed. This is the university sanctioned LMS where we will submit assignments.\nYou will turn in assignments through Sakai under the “Submissions” tab. Generally, there will be a link to each assignment on the course website. You can also view your grades under “Gradebook” and links to Webex under “Webex.”\n\n\n\n\n\nAssignments\n\n\n\n\n\n\n\n\n\nAssignment\nType of assessment\nBefore you submit/take it\nAfter it is graded\n\n\nHomework\nFormative\n\nWork out each problem on your own as much as you can\nTalk through problems with a peer\nGo to Office Hours for help\nWrite down work that shows your thought process\nSearch your issue on Stack Exchange/Stack Overflow\n\n\nReview the solutions\nReview your mistakes\nFor solutions that involve writing sentences, check with me or a TA if your answer fits the solution\nGo to Office Hours to ask about your solutions\n\n\n\nQuizzes\nSummative\n\nIdentify and achieve learning objectives in each lecture\nUnderstand why certain statistics tools are used for certain cases\nPractice testing yourself and others on concepts\nCome to Office Hours for help with specific problems or concepts\n\n\nReview the solutions\nReview your mistakes\nFor solutions that involve writing sentences, check with me or a TA if your answer fits the solution\nGo to Office Hours to ask about your solutions\nDo not ask for a regrade unless you have viewed the solutions\n\n\n\nProject Labs and Poster\nFormative and Summtive\n\nStart the lab as early as possible\nWork on R coding and check with classmates on work\nCome to Office Hours for help with specific R work\nFor the report, compile your work from the labs, and decide what is important in the analysis.\n\n\nThis will be graded at the end of the semester, so you will not have a chance to interact with my feedback as much\nIf you have questions about your grade, you may email me\nKeep the project paper for future reference\nYou can add this project to your resume!\n\n\n\nClass Exit Tickets\nN/A\n\nBring appropriate electronic device to participate in polls\nComplete the survey during the last 5 minutes of class or after class within 7 days\n\n\nReview muddiest and clearest points from the week\n\n\n\n\nIf you would like any other course resources explained in this format, please request it through the Ongoing Course Feedback."
  },
  {
    "objectID": "syllabus.html#course-policies-and-resources",
    "href": "syllabus.html#course-policies-and-resources",
    "title": "BSTA 513/613 Syllabus",
    "section": "Course Policies and Resources",
    "text": "Course Policies and Resources\n\nLate Work Policy\nI encourage you to make your best effort to submit all assignments on time, but I understand circumstances arise that are beyond our control. Please see this Swansea University’s page on extenuating circumstances for some examples. Not all circumstances are covered here, so please reach out if you have questions. \n\nThe class will end on June 9, 2025. All coursework is expected to be completed by June 15, 2025 at 11pm. If you have extenuating circumstances, and need additional time to complete class assignments, please contact me. Together, we will come up with a plan for completion and to sort out registrar logistics.\nIf you have extenuating circumstances that may jeopardize your ability to do work for several weeks, please contact me. We will come up with a plan to keep you on track in the course and prevent any delay in your education.\nFor homework, there is a due date posted, but you may turn in the assignment any time before the class ends. I will give you the check regardless of when you submit the assignment. However, if you would like feedback on the homework, you must turn it in on time OR email me asking for feedback for your late homework.\nFor non-homework assignments, including labs, I ask you to email me directly. You can explain your circumstances and may ask me for an extension, but I won’t necessarily grant one.\nFor labs, you will have ONE no-questions-asked, 3-day extension. Please use this wisely! You just need to send me a quick email saying “I am using my no-questions-asked extension for Lab __.”\nIf you have a emergency involving your self, family, pet, friend, classmate, or anything/one deemed important to you, please do not worry about immediately contacting me. We can work something out after your emergency. If I contact you during an emergency, it is only because I am worried, and you do NOT need to respond until you are able. \n\n\n\nRegrade Policy\nIf you think a question was incorrectly graded, first compare your answer to the answer key. If you believe a re-grade would be appropriate, write an email to me containing the question and a short explanation as to why the question(s) was/were incorrectly graded. Deadline: One week after assignments were returned to class (late requests will not be considered).\n\n\nAttendance Policy\nYou are expected to attend class, participate in-class polls, and complete the exit ticket. For students who miss class or need a review, I will make video and audio recordings of lectures available. There are no guarantees against technical or other challenges for the recording availability or quality. For students who are unable to attend the class in-person and synchronously, viewing the recording within 7 days is acceptable. This is meant to keep you on track within the course and prevent a pile up of material. Make sure to complete the exit ticket to demonstrate attendance.\n\nAttendance for the required in-person days\nThe goal of the lab discussions is to get feedback from a fellow student and gain insight from someone else’s approach to the analysis.\nIf you have extenuating circumstances and cannot make one of the four lab discussion days, please email Nicky to explain your situation before class. From those who were unable to attend, you will be placed in a group of 2-3 people and exchange labs. You will read your groups work and complete a make-up assignment.\nThe make-up assignment will be due one week after the in-class discussion day. To turn in the assignment, you can email your document to Nicky.\nPlease note that Nicky’s approval of a missed discussion, and thus a make-up, is not guaranteed.\n\n\n\nPlagiarism and Attribution\nPlease note that this section has been motivated by Dr. Steven Bedrick’s Course Policies and Grading site for BMI 525. (Note that this is a good example of informal attribution of someone else’s work.)\nIn this class, it is easy to use ChatGPT or other AI tools to solve your homework for you. Many problems follow a basic structure that is especially easy for ChatGPT to solve. In this class, you may use ChatGPT to help with your homework. You may even ask for direct answers. However, there are a few things I do not want you to do:\n\nDo not copy ChatGPT’s answer directly into your homework. Your homework is graded for full credit if you turn it in, in any state, so turning in ChatGPT’s answers is unacceptable. I rather see half-written answers that show what you’re thinking than see a correct answer from ChatGPT.\nDo not stop once ChatGPT answered a question. If it gives an explanation, interact with it! Make sure you understand the thought process of ChatGPT. Try writing out the process to help cement it in your head. Check the answer with what we learn in class.\nDo not use ChatGPT on our quizzes! Hence, you need to really understand how to solve these problems even if you use ChatGPT on the homework.\n\nAt the end of the day, ChatGPT is a resource that will be available to you in a job and outside of school. Thus, we should use it as a tool in school as well! Let me know if ChatGPT helped you understand something! I would love to incorporate it into future classes!\n\n\n\n\n\n\nImportant\n\n\n\nYou can think of this class as assembling a toolbox. When a handyperson starts working for the first time, they need to buy their tools. For their first few jobs, they might need help finding their tools, or remembering which tool is best used for what action. Eventually, they get to know their tools well, and using them appropriately becomes second nature.\nFor now, ChatGPT can help us find and use our tools, but we need to work towards using them as second nature!"
  },
  {
    "objectID": "syllabus.html#course-expectations",
    "href": "syllabus.html#course-expectations",
    "title": "BSTA 513/613 Syllabus",
    "section": "Course Expectations",
    "text": "Course Expectations\n\nInstructor Expectations\nCommitment to your learning and your success\nI believe that everyone has the ability to be successful in this course and I have put a lot of effort into designing the course in a way that maximizes your learning to ensure your success. Please talk to me before or after class or stop by my office if there is anything you want to discuss or about which you are unclear. I want to be supportive of your learning and growth.\nInclusive & supportive learning community\nI believe that learning happens best when we all learn together, as a community. This means creating a space characterized by generous listening, civility, humility, patience, and hospitality. I will attempt to promote a safe climate where we examine content from multiple cultural perspectives, and I will strive to create and maintain a classroom atmosphere in which you feel free to both listen to others and express your views and ask questions to increase your learning.\nOpenness to feedback\nI appreciate straightforward feedback from you regarding how well the class is meeting your needs. Let me know if material is not clear or when its relevance to the student learning outcomes for the course is not apparent. In particular, let me know if you identify bias or stereotyping in my teaching materials as I will seek to continuously improve. Please also let me know if there’s an aspect of the class you find particularly interesting, helpful, or enjoyable!\nResponsiveness\nI will monitor email as well as the discussion board daily and try respond to all messages within 24 hours Monday-Friday.\nClear guidelines and prompt feedback on assignments\nI will provide clear instructions for all assignments, and a grading rubric when applicable. I will provide detailed feedback on your submissions and will update grades promptly in Sakai.\n\n\nStudent Expectations and Resources\nAttend class\nYou are expected to attend all scheduled class meetings synchronously or watch the recording within 7 days. Attendance is taken through exit tickets. If you have issues accessing the poll on a specific day, please let me know. \nParticipate\nI encourage you to participate actively in class and online discussions. I will expect all students, and all instructors, to be respectful of each other’s contributions, whether I agree with them or not. Professional interactions are expected.\nBuild rapport\nIf you find that you have any trouble keeping up with assignments or other aspects of the course, make sure you let me know as early as possible. As you will find, building rapport and effective relationships are key to becoming an effective professional. Make sure that you are proactive in informing me when difficulties arise during the quarter so that I can help you find a solution.\nComplete assignments\nAll assignments for this course will be submitted electronically through Sakai unless otherwise instructed.  I encourage you to make your best effort to submit all assignments on time, but I understand that sometimes circumstances arise that are beyond our control. If you need an extension, please contact me in congruence with the Late Policy.\nSeek help if you need it\nI believe it is important to support the physical and emotional well‐being of my students. If you are experiencing physical or mental health issues, I encourage you to use the resources on campus such as those listed below. If you have a health issue that is affecting your performance or participation in the course, and/or if you need help connecting with these resources, please contact me.\n\nStudent Health and Wellness Center (SHW), Website, 503-494-8665 (OHSU Students only)\nStudent Health and Counseling (SHAC), Website, 503-725-2800\n\nInform your instructor of any accommodations needed\nYou should speak with or email me before or during the first week of classes regarding any special needs. Students seeking academic accommodations should register with the appropriate service under the School policies below.\nSome religious holidays may occur on regularly scheduled class days. Because available class hours are so limited in number, we will have to hold class on all such days. Class video recordings will be available and you are encouraged to engage with the material outside of the regular class time. You are also encouraged to come to office hours with questions from the session.\nCommit to integrity\nAs a student in this course (and at PSU or OHSU) you are expected to maintain high degrees of professionalism, commitment to active learning and participation in this class and also integrity in your behavior in and out of the classroom.\nCheating and other forms of academic misconduct will not be tolerated in this course and will be dealt with firmly. Student academic misconduct refers to behavior that includes plagiarism, cheating on assignments, fabrication of data, falsification of records or official documents, intentional misuse of equipment or materials (including library materials), or aiding and abetting the perpetration of such acts. Preparation of exams, assigned on an individual basis, must represent each student’s own individual effort. When used, resource materials should be cited in conventional reference format."
  },
  {
    "objectID": "syllabus.html#course-communications",
    "href": "syllabus.html#course-communications",
    "title": "BSTA 513/613 Syllabus",
    "section": "Course Communications",
    "text": "Course Communications\nSakai/Slack announcements\nFor important/urgent matters, I will communicate with you using announcements via Sakai that will be delivered to your OHSU Email account as well as displayed in the Sakai course site Announcements section. I will copy these announcements in Slack if they do not involve changes to the schedule. Unfortunately, there are certain announcements that OHSU requires I initiate behind the firewall.\nGeneral course questions\nIt is normal to have many questions about things that relate to the course, such as clarification about assignments, course materials, or assessments. Please post these on our Slack Workspace. Please use the channels that I created for questions. You are encouraged to give answers and help each other. I will monitor these threads, so I will endorse or correct responses as needed. Please give me 24 hours to respond to questions within Monday-Friday. Work-life balance is important for me as well, so I will try to respond as quickly as I can within my healthy limits. \nE-mail\nE-mail should be used only for messages that are private in nature. Please send private messages to my OHSU email address (wakim@ohsu.edu). Messages sent through Sakai Inbox will not be answered. Do not send messages asking general information about the class; please post those on Slack instead."
  },
  {
    "objectID": "syllabus.html#further-student-resources",
    "href": "syllabus.html#further-student-resources",
    "title": "BSTA 513/613 Syllabus",
    "section": "Further Student Resources",
    "text": "Further Student Resources\n\nSPH Writing Lab\nThe School of Public Health Writing Support serves graduate students (master’s and PhD) in SPH, offering help on all professional writing tasks, including class papers, dissertations, job application documents, personal statements, and grant applications, to name a few. Leslie Bienen, MFA, DVM offers one-on-one writing support and other workshops. Appointments are virtual for the time being. You can make an appointment by contacting writingsupportsph@pdx.edu or making an appointment through Calendly.\n\n\nGrammarly Subscription\nThe School of Public Health students have access to a subscription version of Grammarly. While Grammarly cannot improve the argument and flow of your work, it can help with spelling, grammar, and sentence structure. If you are interested in this tool, please add your name to this email form and they will get you added to the subscription. Be sure to use your PSU login credentials to access the form.\n\n\nStudent Wellness\nI am committed to supporting the physical and emotional well-being of my students. Both PSU and OHSU have designated centers for student health. For OHSU, students can visit the Behavioral Health site, where you can find more information including the number to make an appointment. All student visits are free. OHSU students also have access to PSU’s Counseling Services through the school’s Student Health & Counseling. Information on additional student resources for OHSU students are available on the OHSU Health and Wellness Resource page. \n\n\nSupport for Food Insecurity\nStudents across the country experience food insecurity at alarming rates. OHSU and PSU both provide a list of resources to help combat food insecurity. Of note, the Committee to Improve Student Food Security (CISFS) at PSU provides a Free Food Market on the second Monday of each month. OHSU also provides SNAP Enrollment Assistance. The Supplemental Nutrition Assistance Program (SNAP) allocates money towards food for individuals below a certain income level. If you make less than $2,430 monthly, you may wish to enroll.\n\n\nSupport for Students with Children\nStudents who have children can use the PSU resource: Resource Center for Students with Children. Resources are mostly focused on students with younger children. There are several great resources available, including: family-friendly study spaces, new baby starter packs, free kids clothing, and further information on financial resources for childcare."
  },
  {
    "objectID": "syllabus.html#school-policies-and-resources",
    "href": "syllabus.html#school-policies-and-resources",
    "title": "BSTA 513/613 Syllabus",
    "section": "School Policies and Resources",
    "text": "School Policies and Resources\n\nSchool of Public Health Handbook\nAll students are responsible for following the policies and expectations outlined in the student handbook for their program of study. Students are responsible for their own academic work and are expected to have read and practice principles of academic honesty, as presented in the handbook.\n\n\nStudent Access & Accommodations\nThe School of Public Health values diversity and inclusion; we are committed to fostering mutual respect and full participation for all students. My goal is to create a learning environment that is equitable, usable, inclusive, and welcoming. If any aspects of instruction or course design result in barriers to your inclusion or learning, please notify me. \n\nIf you are already registered with disability services at either OHSU or PSU and you are taking a course at the opposite institution, you need to contact the office you’re registered with to transfer your accommodations.\nIf you are not already registered with a disability services office, and you have, or think you may have, a disability that may affect your work in this class, and feel you need accommodations, use the following table for guidance about which office to contact to initiate accommodations.\n\nResource Table\n\n\n\nEnrollment University and Standing\nWhere to Seek Accommodations\n\n\nUndergraduate School of Public Health major\nPSU’s Disability Resource Center\n\n503-725-4150\nSmith Memorial Student Union, Room 116\ndrc@pdx.edu\n\n\n\nAll PSU-registering Dual Degree (MSW/MPH and MURP/MPH) Graduate School of Public Health Majors and all PSU-registering PhD students admitted prior to fall 2016.\nPSU’s Disability Resource Center\n\n503-725-4150\nSmith Memorial Student Union, Room 116\ndrc@pdx.edu\nwww.pdx.edu/drc\n\n\n\nGraduate School of Public Health major (irrespective of institution at which you register)\nOHSU’s Office for Student Access\n(503) 494-0082\nStudentAccess@OHSU.edu\nOHSU Auditorium Building 330\n\n\nNon-SPH major, PSU-enrolled student\nPSU’s Disability Resource Center\n503-725-4150\nSmith Memorial Student Union, Room 116\ndrc@pdx.edu\nwww.pdx.edu/drc\n\n\nNon-SPH major, OHSU-enrolled student\nOHSU’s Office for Student Access\n(503) 494-0082\nStudentAccess@OHSU.edu\nOHSU Auditorium Building 330\n\n\n\n \nFor more information related accessibility and accommodations, please see the “Statement Regarding Students with Disabilities” within the Institutional Policies section of this syllabus.\n\n\nTitle IX\nThe School of Public Health is committed to providing an environment free of all forms of prohibited discrimination and discriminatory harassment. The School of Public Health students who have questions about an incident related to Title IX are welcome to contact either the OHSU or PSU’s Title IX Coordinator and they will direct you to the appropriate resource or office. Title IX pertains to any form of sex/gender discrimination, discriminatory harassment, sexual harassment or sexual violence.\n\nPSU’s Title IX Coordinator is Julie Caron, she may be reached at titleixccordinator@pdx.edu or 503-725-4410. Julie’s office is located at 1600 SW 4th Ave, In the Richard and Maureen Neuberger Center RMNC - Suite 830.\nThe OHSU Title IX Coordinator’s may be reachedat 503-494-0258 or titleix@ohsu.edu and is located at 2525 SW 3rd St.\n\nPlease note that faculty and the Title IX Coordinators will keep the information you disclose private but are not confidential. If you would like to speak with a confidential advocate, who will not disclose the information to a university official without your written consent, you may contact an advocate at PSU or OHSU.\n\nPSU’s confidential advocates are available in Women’s Resource Center (serving all genders) in Smith Student Memorial Union 479. You may schedule an appointment by (503-725-5672) or schedule on line at https://psuwrc.youcanbook.me. For more information about resources at PSU, please see PSU’s Response to Sexual Misconduct website.\nOHSU’s advocates are available through the Confidential Advocacy Program (CAP) at 833-495-CAPS (2277) or by email CAPsupport@ohsu.edu, but please note, email is not a secure form of communication. Also visit www.ohsu.edu/CAP.\n\nAt OHSU, if you encounter any harassment, or discrimination based on race, color, religion, age, national origin or ancestry, veteran or military status, sex, marital status, pregnancy or parenting status, sexual orientation, gender identity or expression, disability or any other protected status, please contact the Affirmative Action and Equal Opportunity (AAEO) Department at 503-494-5148 or aaeo@ohsu.edu.\nAt PSU, you may contact the Office of Equity and Compliance if you experience any form of discrimination or discriminatory harassment as listed above at equityandcompliance@pdx.edu or by calling 503-725-5919.\n\n\nTechnical Support\nThe OHSU ITG Help Desk is available to assist students with email account or network account access issues between 6 a.m. and 6 p.m., Monday through Friday at 503-494-2222. For technical support in using the Sakai Course Management System, please contact the Sakai Help Desk at 877-972-5249 or email us at sakai@ohsu.edu"
  },
  {
    "objectID": "syllabus.html#ohsu-competencies",
    "href": "syllabus.html#ohsu-competencies",
    "title": "BSTA 513/613 Syllabus",
    "section": "OHSU Competencies",
    "text": "OHSU Competencies\n\nList of OHSU Graduation Core Competencies\n\nProfessional Knowledge and Skills\nProfessionalism\nInformation Literacy\nCommunication\nTeamwork\nCommunity Engagement, Social Justice and Equity\nPatient Centered Care\n\nTo access a descriptive list of OHSU Graducation Core Competencies: OHSU Graduation Core Competencies"
  },
  {
    "objectID": "syllabus.html#institutional-policies-and-resources",
    "href": "syllabus.html#institutional-policies-and-resources",
    "title": "BSTA 513/613 Syllabus",
    "section": "Institutional Policies and Resources",
    "text": "Institutional Policies and Resources\n\nStatement Regarding Students with Disabilities\nOHSU is committed to inclusive and accessible learning environments in compliance with federal and state law. If you have a disability or think you may have a disability (mental health, attention-related, learning, vision, hearing, physical or health impacts) contact the Office for Student Access at (503) 494-0082 or OHSU Student Access to have a confidential conversation about academic accommodations. Information is also available at Student Access Website. Because accommodations may take time to implement and cannot be applied retroactively, it is important to have this discussion as soon as possible.\nPortland State students also have similar resources available via the PSU Disability Resource Center (website http://www.pdx.edu/drc ). Please contact the DRC at tel. (503) 725-4150 or email at drc@pdx.edu\n\n\nStudent Evaluation of Courses\nCourse evaluation results are extremely important and used to help improve courses and the learning experience of future students. Responses will always remain anonymous and will only be available to instructors after grades have been posted. The results of scaled questions and comments go to both the instructor and their unit head/supervisor. Refer to Student Evaluation of Courses and Instructional Effectiveness, *Policy No. 02-50-035.\n*To access the OHSU Student Evaluation of Courses and Instructional Effectiveness Policy, you must log into the OHSU O2 website.\n\n\nCopyright Information\nCopyright laws and fair use policies protect the rights of those who have produced the material. The copy in this course has been provided for private study, scholarship, or research. Other uses may require permission from the copyright holder. The user of this work is responsible for adhering to copyright law of the U.S. (Title 17, U.S. Code). To help you familiarize yourself with copyright and fair use policies, the University encourages you to visit its Copyright Web Page\nSakai course web sites contain material protected by copyrights held by the instructor, other individuals or institutions. Such material is used for educational purposes in accord with copyright law and/or with permission given by the owners of the original material. You may download one copy of the materials on any single computer for non-commercial, personal, or educational purposes only, provided that you (1) do not modify it, (2) use it only for the duration of this course, and (3) include both this notice and any copyright notice originally included with the material. Beyond this use, no material from the course web site may be copied, reproduced, re-published, uploaded, posted, transmitted, or distributed in any way without the permission of the original copyright holder. The instructor assumes no responsibility for individuals who improperly use copyrighted material placed on the web site.\n\n\nSyllabi Changes and Retention\nSyllabi are considered to be a learning agreement between students and the faculty of record. Information contained in syllabi, other than the minimum requirements, may be subject to change as deemed appropriate by the faculty of record in concurrence with the academic program and the Office of the Provost. Refer to the *Course Syllabi Policy, 02-50-050.\n*To access the OHSU Course Syllabus Policy, you must log into the OHSU O2 website.\n\n\nCommitment to Diversity & Inclusion\nOHSU is committed to creating and fostering a learning and working environment based on open communication and mutual respect. If you encounter sexual harassment, sexual misconduct, sexual assault, or discrimination based on race, color, religion, age, national origin, veteran’s status, ancestry, sex, marital status, pregnancy or parenting status, sexual orientation, gender identity, disability or any other protected status please contact the Affirmative Action and Equal Opportunity Department at 503-494-5148 or aaeo@ohsu.edu. Inquiries about Title IX compliance or sex/gender discrimination and harassment may be directed to the OHSU Title IX Coordinator at 503-494-0258 or titleix@ohsu.edu.\n\n\nModified Operations, Policy 01-40-010\nPortland Campus:  Marquam Hill and South Waterfront\nStudents should review O2 or call OHSU’s weather alert line at 503-494-9021 for the most up-to-date information on OHSU-wide modified operations which include but are not limited to delays or closures for inclement weather.\nIf your home institution is not on the Portland campus (Marquam Hill or South Waterfront, contact your home institution for more information.\n\n\nOHSU Resources Available to Students*:\nRemote Learning Resources\nThe Remote Learning webpage on O2 contains concise, practical resources, and strategies for students that need to quickly transition to a fully remote instructional format.\nRegistrar’s Office\nMackenzie Hall, Rm. 1120\n503-494-7800; Email the Registrar\nStudent Registration Information: \nTo Register for Classes\nOHSU ITG Help Desk\nRegular staff hours are 6 a.m. to 6 p.m., Monday through Friday, but phones are answered seven days a week, 24 hours a day. Call 503 494-2222.\nTeaching and Learning Center\nAcademic Support Counseling and Sakai Course Management System, please contact the TLC Help Desk at 877-972-5249 or email TLC Help Desk\nStudent Academic Support Services\nFor resources on improving student’s study strategies, time management, motivation, test-taking skills and more, Please access the Student Academic Support Services Sakai page. For one-on-one appointments or to arrange a workshop for students, please contact Emily Hillhouse.\nConfidential Advocacy Program\nSupport for OHSU employees, students, and volunteers who have experienced any form of sexual misconduct, including sexual harassment, sexual assault, intimate-partner violence, stalking, relationship/dating violence, and other forms — regardless of when or where it took place. Contact Us.\nConcourse Syllabus Management\nFor help with accessing your Concourse Syllabus:  Please contact the Sakai help Desk for all other Concourse inquiries please visit the Concourse Support - Sakai or please contact the Mark Rivera at rivermar@ohsu.edu or call 503-494-0934\nPublic Safety\nOHSU Public Safety-Portland Campus (Marquam Hill and South Waterfront)\n\nEmergency on Campus: 503-494-4444 (Portland)\nNon-emergency: 503-494-7744; Contact Public Safety\n\nStudent Health & Wellness Center \nBaird Hall, Rm. 18 (Primary Care) and Rm. 6 (Behavioral Health)\n503-494-8665; For urgent care after hours, 503-494-8311 and ask for the Nurse on call.\nWellness Center Information  \nWellness Center Website\nIf your home institution is not on the Portland campus, contact your home institution student support services for more information.\nOmbudsman Office\nGaines Hall, Rm. 117\n707 SW Gaines Street, Portland, OR 97239\n503-494-5397; Contact Ombudsman; Ombudsman Website\nLibrary: Biomedical Information Communication Center\nBICC Library Hours of Operation\n\n\nPrivacy While Learning\nStudents may be asked to take classes remotely through videoconferencing software like WebEx. Some of these remote classes will be recorded. Any recording will capture the presenter’s audio, video, and computer screen. Student video and audio will be recorded if and when you unmute your audio and share your video during the recorded sessions. These recordings will not be shared with or accessible to the public without prior written consent. \n\n\nStudent Central\nKey information for students across OHSU’s Schools of Dentistry, Medicine, Nursing, the OHSU-PSU School of Public Health and the College of Pharmacy. Student Central helps you find out more about student services, resources, policies and technology."
  },
  {
    "objectID": "instructors.html",
    "href": "instructors.html",
    "title": "Instructors",
    "section": "",
    "text": "Email: wakim@ohsu.edu\nOffice: VPT 622A\n\nPronouns: she/her/hers\nYou are welcome to address me as Nicky (pronounced “nik-EE”), Dr. Wakim (pronounced “wah-KEEM”), Dr. W, Dr. Nicky, Professor, Professor Wakim, or any combination of the prior.\nBest method to contact: Office hours or Slack for general course questions or E-mail/Calendly appointments for private communication.\n\n\n\n\n\n\n\n\n\n\nBrief professor statement: As a professor, my main goal is to instill a growth mindset into my students. Growth mindset means we are NOT stuck in our abilities or knowledge, and that we all can and will grow! This course aims to be as transparent as possible. I want you to understand my motivation for assessments, questions, and lessons. I also want those assessments to be clear, so please ask for clarification whenever needed.\n\n\n\nTBD"
  },
  {
    "objectID": "instructors.html#instructor-nicole-nicky-wakim-phd",
    "href": "instructors.html#instructor-nicole-nicky-wakim-phd",
    "title": "Instructors",
    "section": "",
    "text": "Email: wakim@ohsu.edu\nOffice: VPT 622A\n\nPronouns: she/her/hers\nYou are welcome to address me as Nicky (pronounced “nik-EE”), Dr. Wakim (pronounced “wah-KEEM”), Dr. W, Dr. Nicky, Professor, Professor Wakim, or any combination of the prior.\nBest method to contact: Office hours or Slack for general course questions or E-mail/Calendly appointments for private communication.\n\n\n\n\n\n\n\n\n\n\nBrief professor statement: As a professor, my main goal is to instill a growth mindset into my students. Growth mindset means we are NOT stuck in our abilities or knowledge, and that we all can and will grow! This course aims to be as transparent as possible. I want you to understand my motivation for assessments, questions, and lessons. I also want those assessments to be clear, so please ask for clarification whenever needed.\n\n\n\nTBD"
  },
  {
    "objectID": "instructors.html#teaching-assistants",
    "href": "instructors.html#teaching-assistants",
    "title": "Instructors",
    "section": "Teaching Assistants",
    "text": "Teaching Assistants\nKatie Hand and Miyuki Sun will serve as our TAs for the quarter!! They will have the following office hours and will help answer questions over email.\n\nKatie Hand\n\nOffice hours: TBD\n\nOnline Webex Link\n\nEmail: handka@ohsu.edu\n\n\n\nMiyuki Sun\n\nOffice hours: TBD\n\nOnline Webex Link\n\nEmail: sunm@ohsu.edu"
  },
  {
    "objectID": "instructors.html#statistics-tutor-for-epidemiology-students",
    "href": "instructors.html#statistics-tutor-for-epidemiology-students",
    "title": "Instructors",
    "section": "Statistics Tutor for Epidemiology Students",
    "text": "Statistics Tutor for Epidemiology Students\n\nBecky Lanford\n\nEmail: lanford@ohsu.edu\nLink to Becky’s Calendly\n\nBecky can help with:\n\nStatistical coding support\nSupport with stats concepts you are learning in class\nData management and analysis plan scheming during your PE\n\nIntroduction from Becky:\n\nHello fellow MPH classmates! My name is Becky Lanford. I’m looking forward to helping support you in your coursework this quarter. A little about my background: I am currently in my final year in the MPH Epidemiology track and have completed most of my coursework including the biostatistics and epidemiology series (mostly working in R). I enrolled at the SPH as someone re-entering the workforce and quite new to statistical programming. Though I had previously completed a graduate degree (as a Physician Assistant/Associate), re-acclimating to graduate work and learning programming skills made for a steep learning curve my first academic year. I credit the collaborative learning environment at SPH - support of TA’s and classmates and availability of instructors - for helping me be successful. I hope I can help answer course-content questions, problem solve with you and find answers if I don’t have them myself. I know there are many challenges to being a graduate student and I am excited to help our public health student community grow stronger and more knowledgeable together."
  },
  {
    "objectID": "homeworks.html",
    "href": "homeworks.html",
    "title": "Homework",
    "section": "",
    "text": "Homework\nAssignment\nAssignment due (@11pm)\nAnswers\nSolutions (.qmd)\nSolutions (.html)*\n\n\n\n\n1\n\n4/18\n\n\n\n\n\n2\n\n5/2\n\n\n\n\n\n3\n\n5/16\n\n\n\n\n\n4\n\n5/30\n\n\n\n\n\n5\n\n6/6\n*Please note that you need to download the .html file to see the LaTeX math properly."
  },
  {
    "objectID": "homeworks.html#assignments",
    "href": "homeworks.html#assignments",
    "title": "Homework Assignments and Solutions",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\n4/11/24\n\n\nHomework 1\n\n\n6 min\n\n\n\n\n4/25/24\n\n\nHomework 2\n\n\n7 min\n\n\n\n\n5/9/24\n\n\nHomework 3\n\n\n5 min\n\n\n\n\n5/23/24\n\n\nHomework 4\n\n\n6 min\n\n\n\n\n6/6/24\n\n\nHomework 5\n\n\n5 min\n\n\n\n\n\nNo matching items\n\n\n\nHomework 4 Part e help"
  },
  {
    "objectID": "homeworks.html#solutions",
    "href": "homeworks.html#solutions",
    "title": "Homework Assignments and Solutions",
    "section": "Solutions",
    "text": "Solutions\nPlease note that you need to download the .html file to see the LaTeX math properly.\n\n\n\nHomework\n.qmd file\n.html file\n\n\n\n\n1\n\n\n\n\n2\n\n\n\n\n3\n\n\n\n\n4\n\n\n\n\n5"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#what-is-categorical-data-analysis",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#what-is-categorical-data-analysis",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "What is Categorical Data Analysis?",
    "text": "What is Categorical Data Analysis?\n\nIn BSTA 512/612 (linear regression), we focused on continuous responses/outcomes\n\nWe included categorical variables only as covariates (aka predictors, independent variables, explanatory variables)\nExamples from 512/612: life expectancy (in years), IAT score (ranging from -2 to 2)\n\n\n   \n\nCategorical data analysis focuses on the statistical methods for categorical responses/outcomes\n\nExplanatory (or ‘independent’) variable can be of any type (continuous or categorical)"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#types-of-variables",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#types-of-variables",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Types of Variables",
    "text": "Types of Variables"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#types-of-variables-outcomes-we-will-cover-in-this-course",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#types-of-variables-outcomes-we-will-cover-in-this-course",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Types of Variables: Outcomes we will cover in this course",
    "text": "Types of Variables: Outcomes we will cover in this course"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#what-does-this-course-cover",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#what-does-this-course-cover",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "What does this course cover?",
    "text": "What does this course cover?\n\nStrategies for assessing association between categorical response variable and a one explanatory variable\n\nHypothesis testing\nMeasure of association\nSimple logistic regression\n\n\n   \n\nStatistical modeling strategies for assessing association between the categorical response variable and a set of explanatory variables\n\nLogistic regression\n\nFor binary, ordinal, and multinomial outcomes\n\nPoisson regression\n\nFor counts outcomes"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#poll-everywhere-question-1",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#poll-everywhere-question-1",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Poll everywhere question 1",
    "text": "Poll everywhere question 1"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#binomial-distribution",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#binomial-distribution",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\nConsider a sample of \\(n\\) independent trials, each of which can have only two possible outcomes (“success” and “failure”)\nFor each trial: \\[\\begin{align} P( \\text{success}) & = p \\\\\n                  P( \\text{failure}) & = 1- p = q \\end{align}\\]\nBinomial distribution: distribution of the number of successes in n independent trials\nThe probability mass function for the binomial distribution is: \\[P(X=k) = {n \\choose k} p^k q^{n-k}, \\text{ for } k = 0, 1, ..., n\\]\n\n\\(E(X) = np\\)\n\\(Var(X) = npq = np(1-p)\\)"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#binomial-distribution-1",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#binomial-distribution-1",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\nConsider a sample of \\(n\\) independent trials, each of which can have only two possible outcomes (“success” and “failure”)\nFor each trial: \\[\\begin{align} P( \\text{success}) & = p \\\\\n                  P( \\text{failure}) & = 1- p = q \\end{align}\\]\nBinomial distribution: distribution of the number of successes in n independent trials\nThe probability mass function for the binomial distribution is: \\[P(X=k) = {n \\choose k} p^k q^{n-k}, \\text{ for } k = 0, 1, ..., n\\]\n\n\\(E(X) = np\\)\n\\(Var(X) = npq = np(1-p)\\)"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#binomial-distribution-r-commands",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#binomial-distribution-r-commands",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Binomial Distribution: R commands",
    "text": "Binomial Distribution: R commands\nR commands with their input and output:\n\n\n\n\n\n\n\nR code\nWhat does it return?\n\n\n\n\nrbinom()\nreturns sample of random variables with specified binomial distribution\n\n\ndbinom()\nreturns probability of getting certain number of successes\n\n\npbinom()\nreturns cumulative probability of getting certain number or less successes\n\n\nqbinom()\nreturns number of successes corresponding to desired quantile"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#binomial-distribution-example",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#binomial-distribution-example",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Binomial Distribution Example",
    "text": "Binomial Distribution Example\n\n\nExample\n\n\nIf the probability that one white blood cell is a lymphocyte is 0.2, compute the probability of 2 lymphocytes out of 10 white blood cells\n\n\n\\[P(X=2) = {10 \\choose 2} 0.2^2 (1-0.2)^{10-2}  = 0.3020\\]\n\ndbinom(2, 10, 0.2) %&gt;% round(4)\n\n[1] 0.302"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#normal-approximation-of-the-binomial-distribution",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#normal-approximation-of-the-binomial-distribution",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Normal Approximation of the Binomial Distribution",
    "text": "Normal Approximation of the Binomial Distribution\n\nAlso known as: Sampling distribution of \\(\\widehat{p}\\)\nIF \\(X\\sim \\text{Binomial}(n,p)\\) and \\(np&gt;10\\) and \\(nq = n(1-p) &gt; 10\\)\n\nEnsures sample size (\\(n\\)) is moderately large and the \\(p\\) is not too close to 0 or 1\nOther resources use other criteria (like \\(npq&gt;5\\) or \\(np&gt;5\\))\nWhen looking at a sample, we use \\(\\widehat{p}\\) instead of \\(p\\) to check this!!\n\n\n \n\nTHEN approximately \\(𝑋\\sim \\text{Normal}\\big(\\mu_X = np, \\sigma_X = \\sqrt{np(1-p)} \\big)\\)\n\nOr we often write this as the sampling distribution of \\(\\widehat{p}\\): \\[\\widehat{p} \\sim \\text{Normal}\\bigg(\\mu_{\\widehat{p}} = p, \\sigma_{\\widehat{p}} = \\sqrt{\\dfrac{p(1-p)}{n}}\\bigg)\\]\n\nPretty good video behind the intuition of this (Watch 00:00 - 05:40)"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#estimate-and-confidence-interval-of-single-proportion",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#estimate-and-confidence-interval-of-single-proportion",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Estimate and Confidence Interval of Single Proportion",
    "text": "Estimate and Confidence Interval of Single Proportion\n\nEstimate of proportion:\n\n\\[\n\\widehat{p} = \\dfrac{\\# \\text{successes}}{\\# \\text{successes} + \\# \\text{failures}}\n\\]\n\nUse the sampling distribution of \\(\\widehat{p}\\) to construct the confidence interval:\n\n\\((1-\\alpha)\\%\\) confidence interval for estimate proportion:\n\n\n\\[\\begin{align} \\widehat{p} &\\pm z^*_{(1-\\alpha/2)} \\cdot SE_{\\hat{p}} \\\\ \\widehat{p} &\\pm z^*_{(1-\\alpha/2)} \\cdot \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\end{align}\\]\n\nUsing \\(SE_{\\hat{p}} = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\) - instead of \\(\\sigma_{p} = \\sqrt{\\frac{p(1-p)}{n}}\\) - because we don’t know exactly what \\(p\\) is"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#section",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#section",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "",
    "text": "Smoking status example\n\n\nA cross-sectional study of 8681 patients was conducted to evaluate the nature of smoking status among people. Of the 8681 people, 4840 were nonsmokers and 3841 were smokers.\n\n\nNeeded steps:\n\nEstimate proportion \\(\\widehat{p}\\)\nCheck that \\(n\\widehat{p}&gt;10\\) and \\(n(1-\\widehat{p})&gt;10\\) in order to make normal approximation\nConstruct 95% confidence interval\nWrite interpretation"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#section-1",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#section-1",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "",
    "text": "Smoking status example\n\n\nA cross-sectional study of 8681 patients was conducted to evaluate the nature of smoking status among people. Of the 8681 people, 4840 were nonsmokers and 3841 were smokers.\n\n\n\n\n\nEstimate proportion \\(\\widehat{p}\\) \\[ \\widehat{p} = \\dfrac{3841}{3841 + 4840} = \\dfrac{3841}{8681} = 0.44246\\]\nCheck that \\(n\\widehat{p}&gt;10\\) and \\(n(1-\\widehat{p})&gt;10\\) in order to make normal approximation \\[ n\\widehat{p} = 8681\\cdot0.4425 = 3841 &gt; 10\\] \\[ n(1-\\widehat{p}) = 8681\\cdot(1-0.4425) = 4840 &gt; 10\\]\n\n\n\nConstruct 95% confidence interval\n\n\\[ \\widehat{p} \\pm z^*_{0.975} \\cdot \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\]\n\nprop.test(x = 3841, n = 8681, correct = T)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  3841 out of 8681, null probability 0.5\nX-squared = 114.73, df = 1, p-value &lt; 2.2e-16\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.4319827 0.4529896\nsample estimates:\n        p \n0.4424605 \n\n\n\nWrite interpretation of estimate\n\nThe estimated proportion of smokers is 0.442 (95% CI: 0.432, 0.453).\nAdditional interpretation of CI: We are 95% confident that the (population) proportion of smokers is between 0.432 and 0.453."
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#poll-everywhere-question-2",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#poll-everywhere-question-2",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Poll everywhere question 2",
    "text": "Poll everywhere question 2"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#estimate-and-confidence-interval-for-difference-in-proportions",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#estimate-and-confidence-interval-for-difference-in-proportions",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Estimate and Confidence Interval for Difference in Proportions",
    "text": "Estimate and Confidence Interval for Difference in Proportions\n\nUse the sampling distribution of \\(\\widehat{p}_1\\) and \\(\\widehat{p}_2\\) to construct the confidence interval:\n\n\\((1-\\alpha)\\%\\) confidence interval for estimate difference in proportions:\n\n\n\\[\\begin{align} \\widehat{p}_1 - \\widehat{p}_2 &\\pm z^*_{(1-\\alpha/2)} \\cdot SE_{\\hat{p}_1 - \\hat{p}_2} \\\\ \\widehat{p}_1 - \\widehat{p}_2 &\\pm z^*_{(1-\\alpha/2)} \\cdot \\sqrt{\n\\frac{\\hat{p}_1\\cdot(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2\\cdot(1-\\hat{p}_2)}{n_2}} \\end{align}\\]\n\nUsing \\(SE_{\\hat{p}_1 - \\hat{p}_2} = \\sqrt{\\frac{\\hat{p}_1\\cdot(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2\\cdot(1-\\hat{p}_2)}{n_2}}\\) because we don’t know exactly what \\(p_1\\) and \\(p_2\\) are"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#poll-everywhere-question-3",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#poll-everywhere-question-3",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#example-strong-heart-study-12",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#example-strong-heart-study-12",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Example: Strong Heart Study (1/2)",
    "text": "Example: Strong Heart Study (1/2)\n\n\nThe Strong Heart Study is an ongoing study of American Indians residing in 13 tribal communities in three geographic areas (AZ, OK, and SD/ND) to study prevalence and incidence of cardiovascular disease and to identify risk factors. We will be examining the 4-year cumulative incidence of diabetes with one risk factor, glucose tolerance.\n \n\nImpaired glucose: normal or impaired glucose tolerance at baseline visit (between 1988 and 1991)\n \nDiabetes: Indicator of diabetes at follow-up visit (roughly four years after baseline) according to two-hour oral glucose tolerance test"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#example-strong-heart-study-22",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#example-strong-heart-study-22",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Example: Strong Heart Study (2/2)",
    "text": "Example: Strong Heart Study (2/2)\nThere is a total of 1664 American Indians in the dataset, with the following distribution of folks with diabetes and glucose tolerance:\n \n\n\n\n#shs_data = read.csv(file = here(\"./data/SHS_data.csv\"))\n\n\nSHS = tibble(Diabetes = c(rep(\"Not diabetic\", \n                   1338), \n                   rep(\"Diabetic\", 326)),\n              Glucose = c(rep(\"Normal\", \n                  1004),#Not diabetic\n          rep(\"Impaired\", 334),\n          rep(\"Normal\", \n              128), #Diabetic\n          rep(\"Impaired\", 198)))\n\n\n\n\nDisplaying the contingency table in R\nSHS %&gt;% tabyl(Glucose, Diabetes) %&gt;% \n  adorn_totals(where = c(\"row\", \"col\")) %&gt;% \n  gt() %&gt;% \n  tab_stubhead(label = \"Glucose Impairment\") %&gt;%\n  tab_spanner(label = \"Diabetes\", \n              columns = c(\"Not diabetic\", \"Diabetic\")) %&gt;%\n  tab_options(table.font.size = 45)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose\n\nDiabetes\n\nTotal\n\n\nNot diabetic\nDiabetic\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#section-2",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#section-2",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "",
    "text": "Strong Heart Study\n\n\nWhat is the difference in proportions for American Indians that have diabetes comparing individuals with normal vs. impaired glucose?\n\n\nNeeded steps:\n\nEstimate the difference in proportions\nCheck that each cell has at least 10 individuals\nConstruct 95% confidence interval\nWrite interpretation of estimate"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#section-3",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#section-3",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "",
    "text": "Strong Heart Study\n\n\nWhat is the difference in proportions for American Indians that have diabetes comparing individuals with normal vs. impaired glucose?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose\n\nDiabetes\n\nTotal\n\n\nNot diabetic\nDiabetic\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\nEstimate the difference in proportions \\[ \\widehat{p}_1 -\\widehat{p}_2 = \\dfrac{198}{532} - \\dfrac{128}{1132} = 0.2591\\]\nCheck that each cell has at least 10 individuals\n\n\n\nConstruct 95% confidence interval\n\n\nprop.test(x = table(SHS$Glucose, SHS$Diabetes), \n          correct = T)\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  table(SHS$Glucose, SHS$Diabetes)\nX-squared = 152.6, df = 1, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n95 percent confidence interval:\n 0.2126963 0.3055162\nsample estimates:\n   prop 1    prop 2 \n0.3721805 0.1130742 \n\n\n\nWrite interpretation of estimate\n\nThe estimated difference in proportion of diabetic American Indians comparing is 0.259 (95% CI: 0.213, 0.306).\nAdditional interpretation of CI: We are 95% confident that the difference in (population) proportions of American Indians who have normal glucose tolerance and impaired glucose tolerance that developed diabetes is between 0.213 and 0.306."
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#mcnemars-test",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#mcnemars-test",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "McNemar’s Test",
    "text": "McNemar’s Test\n\nMcNemar’s test should be used if data is from a matched pairs study\n\n \n\nWhat is a matched-pairs study?\n\nParticipants are paired based on key characteristics\nEach participant within a pair will be assigned to different treatment groups\n\nCategorical test that is parallel to the “paired t-test”\n\n \n\nR packages and functions\n\nNormal approximation: mcnemar.test() in built-in stats package\nExact test: mcnemar.exact() in exact2x2 package\n\n\n \n\nIf you would like more information of McNemar’s test, please see Rosner TB: 10.4 and 10.5: Paired Samples"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#summary-so-far",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#summary-so-far",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Summary so far",
    "text": "Summary so far\n\nIntroduced categorical data as the response in analysis\nReviewed an important distribution (Binomial distribution) for categorical data analysis\nEstimated a single proportion from a sample with its confidence interval\nEstimated a difference in proportions from a sample with its confidence interval\n\n   \n\nCan we expand this to ask a more general question about association between a response and explanatory variable?\n\nWhat if there is more than 2 categories for either variable?"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#contingency-tables-r-x-c",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#contingency-tables-r-x-c",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Contingency Tables (R x C)",
    "text": "Contingency Tables (R x C)\n\nR X C contingency tables\n\nContains information for two discrete variables: one has R categories and the other has C categories.\nRefers to the number of rows (R) and number of columns (C) in the table\n\n\n \n\nFor two proportions: focused on 2 X 2 contingency tables\n\nR = 2, C = 2\n\n\n \n\nExpand our contingency tables to variables with 2 or more categories\n\nCategories can be ordinal or nominal"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#contingency-table-example",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#contingency-table-example",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Contingency Table: Example",
    "text": "Contingency Table: Example\nLet’s say we are interested in learning the association between the development of breast cancer and age at first birth. Our first step is typically to present the observed data:\n\n\nThis is a 2 x 5 contingency table"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-associationtrend-of-r-x-c-contingency-table",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-associationtrend-of-r-x-c-contingency-table",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Test Association/Trend of R X C Contingency Table",
    "text": "Test Association/Trend of R X C Contingency Table\n \n\n\nIf both variables are nominal, a test of general association will be sufficient\n\nTest of general association is the same regardless of R and C\nTest used for 2x2 contingency table same as 5x3 contingency table\nWe will cover:\n\nChi-squared test\nFisher Exact test\n\n\n\n\n\nIf one or both variables are ordinal, a test of trend may be of interest\n\nTreats ordinal variables as quantitative rather than qualitative (nominal scale)\nTest of trend has greater power than the test of general association\nWe will cover:\n\nCochran-Armitage test\nMantel-Haenszel test"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#poll-everywhere-question-4",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#poll-everywhere-question-4",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-general-association",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-general-association",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Test of General Association",
    "text": "Test of General Association\n\nGeneral research question: Are two variables (both categorical, nominal) associated with each other?\n\n \n\nTranslated to a hypothesis test:\n\n\\(H_0\\) : There is no association between the two variables / The variables are independent\n\\(H_1\\) : There is an association between the two variables / The variables are not independent\n\n\n   \n\nWe have two options for testing general association:\n\nChi-squared test\nFisher’s Exact test"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-general-association-shs-example",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-general-association-shs-example",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Test of General Association: SHS Example",
    "text": "Test of General Association: SHS Example\n\nMain question: Do American Indians with impaired glucose tolerance have a different incidence of diabetes?\n\nIs glucose tolerance associated with diabetes incidence among American Indians?\n\nWe have two variables, and both variables have two nominal categories\n\nDiabetes outcome: Not diabetic and Diabetic\nGlucose tolerance: Normal or Impaired\n\n\n \n\nAnswer research question with a test of general association\nHypothesis:\n\n\\(H_0\\) : There is no association between glucose tolerance and diabetes / Glucose tolerance and diabetes are independent\n\\(H_1\\) : There is an association between glucose tolerance and diabetes / Glucose tolerance and diabetes are not independent"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Chi-squared test",
    "text": "Chi-squared test\n\nTest to see how likely is it that we observe our data given the null hypothesis (no association)\n\n \n\nWe use the null to calculate the expected cell counts and compare them to the observed cell counts\n\n \n\nRequirements to conduct Chi-squared test (expected cell counts)\n\nFor 2 x 2 contingency table:\n\nNo expected cell counts should be less than 10\n\nFor contingency table with 3x2, 3x3, 4x4, etc.:\n\nNo more than 20% of expected cell counts are less than 5\nNo expected cell counts are less than 1"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test-process",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test-process",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Chi-squared test: Process",
    "text": "Chi-squared test: Process\n\nCheck that the expected cell counts threshold is met\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic and p-value for Chi-squared test in R\n\nWe will not discuss the test statistic’s equation\n\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test-expected-cell-counts",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test-expected-cell-counts",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Chi-squared test: Expected Cell Counts",
    "text": "Chi-squared test: Expected Cell Counts\n\nIs the sample size big enough for the chi-square test to be adequate? What are the expected cell counts?\n\n \n\nIf you want an explanation of how to calculate by hand, please see Vu and Harringtion TB (section 8.3.1, page 405)\n\n \n\nToo time consuming for this class, but R does it quickly using the expected() function in the epitools package"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test-expected-cell-counts-1",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test-expected-cell-counts-1",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Chi-squared test: Expected Cell Counts",
    "text": "Chi-squared test: Expected Cell Counts\n\nIn the Strong Heart Study…\n\n\nSHS_table = table(SHS$Glucose, SHS$Diabetes)\nSHS_table\n\n          \n           Diabetic Not diabetic\n  Impaired      198          334\n  Normal        128         1004\n\nlibrary(epitools)\nexpected(SHS_table)\n\n          \n           Diabetic Not diabetic\n  Impaired  104.226      427.774\n  Normal    221.774      910.226\n\n\n \n\n\n\n\n\nAll expected counts &gt; 5"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test-shs-example",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test-shs-example",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Chi-squared test: SHS Example",
    "text": "Chi-squared test: SHS Example\n\n\n\nCheck expected cell counts threshold\n\n\nexpected(SHS_table)\n\n          \n           Diabetic Not diabetic\n  Impaired  104.226      427.774\n  Normal    221.774      910.226\n\n\nAll expected cells are greater than 5.\n\n\\(\\alpha = 0.05\\)\nHypothesis test:\n\n\\(H_0\\) : There is no association between glucose tolerance and diabetes\n\\(H_1\\) : There is an association between glucose tolerance and diabetes\n\n\n\n\nCalculate the test statistic and p-value for Chi-squared test in R\n\n\nchisq.test(x = SHS_table, correct = T)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  SHS_table\nX-squared = 152.6, df = 1, p-value &lt; 2.2e-16\n\n\n\nConclusion to the hypothesis test\n\nWe reject the null hypothesis that glucose tolerance and diabetes are not associated (\\(p&lt;2.2\\cdot10^{-16}\\)). There is sufficient evidence that glucose tolerance and diabetes incidence are associated among American Indians."
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Fisher’s Exact Test",
    "text": "Fisher’s Exact Test\n\nOnly necessary when expected counts in one or more cells is less than 5\nGiven row and column totals fixed, computes exact probability that we observe our data or more extreme data\nConsider a general 2 x 2 table:\n\n\n\nThe exact probability of observing a table with cells (a, b, c, d) can be computed based on the hypergeometric distribution\n\n\\[P(a, b, c, d) = \\dfrac{(a+b)!\\cdot(c+d)!\\cdot(a+c)!\\cdot(b+d)!}{n!\\cdot a!\\cdot b!\\cdot c!\\cdot d!}\\]\n\nNumerator is fixed and denominator changes"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-process",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-process",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Fisher’s Exact test: Process",
    "text": "Fisher’s Exact test: Process\n\nCheck the expected cell counts\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic and p-value for Fisher Exact test in R\n\nWe will not discuss the test statistic’s equation\n\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-shs-example-not-appropriate-use-of-fishers-exact",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-shs-example-not-appropriate-use-of-fishers-exact",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Fisher’s Exact test: SHS Example (not appropriate use of Fisher’s Exact)",
    "text": "Fisher’s Exact test: SHS Example (not appropriate use of Fisher’s Exact)\n\n\n\nCheck expected cell counts threshold\n\n\nexpected(SHS_table)\n\n          \n           Diabetic Not diabetic\n  Impaired  104.226      427.774\n  Normal    221.774      910.226\n\n\nWe’re going to pretend they are less than 5.\n\n\\(\\alpha = 0.05\\)\nHypothesis test:\n\n\\(H_0\\) : There is no association between glucose tolerance and diabetes\n\\(H_1\\) : There is an association between glucose tolerance and diabetes\n\n\n\n\nCalculate the test statistic and p-value for Chi-squared test in R\n\n\nfisher.test(x = SHS_table)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  SHS_table\np-value &lt; 2.2e-16\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 3.576595 6.048639\nsample estimates:\nodds ratio \n  4.644825 \n\n\n\nConclusion to the hypothesis test\n\nWe reject the null hypothesis that glucose tolerance and diabetes are not associated (\\(p&lt;2.2\\cdot10^{-16}\\)). There is sufficient evidence that glucose tolerance and diabetes incidence are associated among American Indians."
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-trend",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-trend",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Test of Trend",
    "text": "Test of Trend\n\nIf one or both variables are ordinal, a test of trend may be of interest\n\nTreats ordinal variables as quantitative rather than qualitative (nominal scale)\nTest of trend has greater power than the test of general association\nYou can use a test of general association for non-ordinal variables\n\n\n \n\nTwo tests of trend that we we learn:\n\nCochran-Armitage test\n\nTests association between a binary response and an ordinal explanatory variable\n\nMantel-Haenszel test\n\nTest association between an ordinal response and an ordinal explanatory variable"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Cochran-Armitage test",
    "text": "Cochran-Armitage test\n\nCochran-Armitage test for trend will determine if there is association between a binary response variable and an ordinal variable with 3 or more categories\n\n \n\nIt will test the trend of the proportions over the ordinal variable\n\nAnswers the question: Does the proportion of people with a “successful” outcome increase as the ordinal explanatory variable increases?\n\n\n \n\nCochran-Armitage test for trend is only suitable for 2 x C contingency tables"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-hypothesis-test",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-hypothesis-test",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Cochran-Armitage test: Hypothesis Test",
    "text": "Cochran-Armitage test: Hypothesis Test\n\n\n\n\nNull Hypothesis (\\(H_0\\))\n\n\nThe proportions of successes are the same across all C ordinal values of the explanatory variable. \\[p_1 = p_2 = ... = p_C\\]\n\n\n\n\n\nAlternative Hypothesis (\\(H_1\\))\n\n\nThe proportions of successes tend to increase as ordinal value of the explanatory variable increases\n\\[p_1 \\leq p_2 \\leq ... \\leq p_C\\]\nOR\nThe proportions of successes tend to decrease as ordinal value of the explanatory variable increases\n\\[p_1 \\geq p_2 \\geq ... \\geq p_C\\]"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-process",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-process",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Cochran-Armitage test: Process",
    "text": "Cochran-Armitage test: Process\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic and p-value for Cochran-Armitage test in R\n\nWe will not discuss the test statistic’s equation\nJust know it follows a Normal distribution\n\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-example-13",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-example-13",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Cochran-Armitage test: Example (1/3)",
    "text": "Cochran-Armitage test: Example (1/3)\nWe are interested in learning the association between the development of breast cancer and age at first birth among people who have given birth"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-example-23",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-example-23",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Cochran-Armitage test: Example (2/3)",
    "text": "Cochran-Armitage test: Example (2/3)\n\nBefore we go into the hypothesis test procedure, we need to construct the contingency table in R\n\n\nCancer = c(320, 1206, 1011, 463, 220)\nNo_Cancer = c(1422, 4432, 2893, 1092, 406)\nbscancer = matrix (c(Cancer, No_Cancer), nrow = 2, byrow = T)\nrownames(bscancer) = c(\"Cancer\",\"No Cancer\")\ncolnames(bscancer) = c(\"&lt;20\",\"20-24\",\"25-29\",\"30-34\",\"&gt;=35\")\nbscancer\n\n           &lt;20 20-24 25-29 30-34 &gt;=35\nCancer     320  1206  1011   463  220\nNo Cancer 1422  4432  2893  1092  406\n\n\n\nIt does not need to be pretty to use in function"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-example-33",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-example-33",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Cochran-Armitage test: Example (3/3)",
    "text": "Cochran-Armitage test: Example (3/3)\n\n\n\n\\(\\alpha = 0.05\\)\nHypothesis test:\n\n\\(H_0\\): The proportions of breast cancer are the same for all age levels of first birth. \\[p_1 = p_2 = ... = p_5\\]\n\\(H_1\\): The proportions of breast cancer tends to increase as level of age of first birth increases\n\n\n\\[p_1 \\leq p_2 \\leq ... \\leq p_5\\]\n\n\nCalculate the test statistic and p-value for Cochran-Armitage test in R\n\n\nlibrary(DescTools)\nCochranArmitageTest(bscancer)\n\n\n    Cochran-Armitage test for trend\n\ndata:  bscancer\nZ = 11.358, dim = 5, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\nConclusion to the hypothesis test\n\nWe reject the null hypothesis that proportions of breast cancer are the same for all age levels of first birth (\\(p&lt;2.2\\cdot10^{-16}\\)). There is sufficient evidence that the proportion of of breast cancer increase as the the age at first birth increases."
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Mantel-Haenszel test",
    "text": "Mantel-Haenszel test\n\nWhen both variables are ordinal, we can conduct Mantel-Haenszel test of trend for linear association\nMantel-Haenszel test for linear trend is suitable for any R x C contingency tables with two ordinal variables\nHypothesis test:\n\n\n\n\n\nNull Hypothesis (\\(H_0\\))\n\n\nThere is no correlation between the two variables \\[ \\rho = 0\\]\n\n\n\n\n\nAlternative Hypothesis (\\(H_1\\))\n\n\nThere is correlation between the two variables\n\\[ \\rho \\neq 0\\]"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-process",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-process",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Mantel-Haenszel test: Process",
    "text": "Mantel-Haenszel test: Process\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic and p-value for Mantel-Haenszel test in R\n\nWe will not discuss the test statistic’s equation\n\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-example-13",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-example-13",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Mantel-Haenszel test: Example (1/3)",
    "text": "Mantel-Haenszel test: Example (1/3)\nA water treatment company is studying water additives and investigating how they affect clothes washing (through measurements of abrasions, wearing, and color loss).\nThe treatments studies where no treatment (plain water), the standard treatment, and a double dose of the standard treatment, called super. Washability was measured as low, medium and high.\nAre levels of washability associated with treatment?"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-example-23",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-example-23",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Mantel-Haenszel test: Example (2/3)",
    "text": "Mantel-Haenszel test: Example (2/3)\n\nBefore we go into the hypothesis test procedure, we need to construct the contingency table in R\n\n\nwater = matrix (c(27, 14, 5, 10, 17, 26, 5, 12, 50), nrow = 3, byrow = T)\nrownames(water) = c(\"plain\",\"standard\",\"super\")\ncolnames(water) = c(\"low\",\"medium\",\"high\")\nwater\n\n         low medium high\nplain     27     14    5\nstandard  10     17   26\nsuper      5     12   50\n\n\n\nIt does not need to be pretty to use in function"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-example-33",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-example-33",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Mantel-Haenszel test: Example (3/3)",
    "text": "Mantel-Haenszel test: Example (3/3)\n\n\n\n\\(\\alpha = 0.05\\)\nHypothesis test:\n\n\\(H_0\\): Water treatment and washability are not correlated. \\[ \\rho = 0\\]\n\\(H_1\\): Water treatment and washability are correlated.\n\n\n\\[ \\rho \\neq 0\\]\n\n\nCalculate the test statistic and p-value for Mantel-Haenszel test in R\n\n\nlibrary(DescTools)\nMHChisqTest(water)\n\n\n    Mantel-Haenszel Chi-Square\n\ndata:  water\nX-squared = 50.602, df = 1, p-value = 1.132e-12\n\n\n\nConclusion to the hypothesis test\n\nWe reject the null hypothesis that there is no correlation between washability and water treatment (\\(p = 1.13 \\cdot 10^{-12} &lt; 0.05\\)). There is sufficient evidence that level of water treatment is associated with washability."
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#more-resources",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#more-resources",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "More resources",
    "text": "More resources\n\nFor a refresher or review of one proportion and differences in proportions\n\nAnd their power calculations\nFrom Meike’s BSTA 511 course (see Day 12!)\n\nFor a refresher or review of Chi-squared test or Fisher’s Exact test\n\nFrom Meike’s BSTA 511 course (see Day 13!)"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#last-class",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#last-class",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Last class",
    "text": "Last class\n\nLooked at simple logistic regression for binary outcome with\n\nOne continuous predictor \\[\\text{logit}(\\pi(X)) = \\beta_0 + \\beta_1 \\cdot X\\]\nOne binary predictor \\[\\text{logit}\\left(\\pi(X) \\right) = \\beta_0 + \\beta_1 \\cdot I(X=1)\\]\nOne multi-level predictor \\[\\text{logit}\\left(\\pi(X) \\right) = \\beta_0 + \\beta_1 \\cdot I(X=b) + \\beta_2 \\cdot I(X=c) + \\beta_3 \\cdot I(X=d)\\]"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#breast-cancer-example",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#breast-cancer-example",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Breast Cancer example",
    "text": "Breast Cancer example\n\nFor breast cancer diagnosis example, recall:\n\nOutcome: early or late stage breast cancer diagnosis (binary, categorical)\n\n\n \n\nPrimary covariate: Race/ethnicity\n\nNon-Hispanic white individuals are more likely to be diagnosed with breast cancer\n\nBut POC are more likely to be diagnosed at a later stage\n\n\n\n \n\nAdditional covariate: Age\n\nRisk factor for cancer diagnosis\n\n\n \n\nWe want to fit a multiple logistic regression model with both risk factors included as independent variables"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#introduction-to-multiple-logistic-regression",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#introduction-to-multiple-logistic-regression",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Introduction to Multiple Logistic Regression",
    "text": "Introduction to Multiple Logistic Regression\n\nIn multiple logistic regression model, we have &gt; 1 independent variable\n\nSometimes referred to as the “multivariable regression”\nThe independent variable can be any type:\n\nContinuous\nCategorical (ordinal or nominal)\n\n\n\n \n\nWe will follow similar procedures as we did for simple logistic regression\n\nBut we need to change our interpretation of estimates because we are adjusting for other variables"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#multiple-logistic-regression-model",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#multiple-logistic-regression-model",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Multiple Logistic Regression Model",
    "text": "Multiple Logistic Regression Model\n\n\n\nAssume we have a collection of \\(k\\) independent variables, denoted by \\(\\mathbf{X}=\\left( X_1, X_2, ..., X_k \\right)\\)\n\n \n\nThe conditional probability is \\(P(Y=1 | \\mathbf{X}) = \\pi(\\mathbf{X})\\)\n\n \n\nWe then model the probability with logistic regression: \\[\\text{logit}\\left(\\pi(\\mathbf{X})\\right) = \\beta_0\n+\\beta_1 \\cdot X_1 +\\beta_2 \\cdot X_2  +\\beta_3 \\cdot X_3 + ... + \\beta_k \\cdot X_k\\]\n\n\n\n\n\n\nWhy the bold \\(X\\)?\n\n\n\\(\\mathbf{X}\\) represents the vector of all the \\(X\\)’s. This is how we represent our group of covariates in our model."
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#fitting-the-multiple-logistic-regression-model",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#fitting-the-multiple-logistic-regression-model",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Fitting the Multiple Logistic Regression Model",
    "text": "Fitting the Multiple Logistic Regression Model\n\nFor a multiple logistic regression model with \\(k\\) independent variables, the vector of coefficients can be denoted by \\[\\boldsymbol{\\beta}^{T} = \\left(\\beta_0, \\beta_1, \\beta_2, ..., \\beta_k \\right)\\]\n\n \n\nAs with the simple logistic regression, we use maximum likelihood method for estimating coefficients\n\nVector of estimated coefficients: \\[\\widehat{\\boldsymbol{\\beta}}^{T} = \\left(\\widehat{\\beta}_0, \\widehat{\\beta}_1, \\widehat{\\beta}_2, ..., \\widehat{\\beta}_k \\right)\\]\n\n\n \n\nFor a model with \\(k\\) independent variables, there is \\(k+1\\) coefficients to estimate\n\nUnless one of those independent variables is a multi-level categorical variables, then we need more than \\(k+1\\) coefficients"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#breast-cancer-example-population-model",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#breast-cancer-example-population-model",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Breast Cancer Example: Population Model",
    "text": "Breast Cancer Example: Population Model\n\nWe can fit a logistic regression model with both race and ethnicity and age:\n\n\\[ \\begin{aligned}\n\\text{logit}\\left(\\pi(\\mathbf{X})\\right) = & \\beta_0\n+\\beta_1 \\cdot I \\left( R/E = H/L \\right)\n+\\beta_2 \\cdot I \\left( R/E = NH AIAN \\right) \\\\\n& +\\beta_3 \\cdot I \\left( R/E = NH API \\right)\n+\\beta_4 \\cdot I \\left( R/E = NH B \\right)\n+\\beta_5 \\cdot Age\n\\end{aligned}\\]\n \n \n\nNote that race and ethnicity requires 4 coefficients to include the indicator for each category\n\n \n\nCan replace \\(\\pi(\\mathbf{X})\\) with \\(\\pi({\\text{Race/ethnicity, Age}})\\)\n\n \n\n6 total coefficients (\\(\\beta_0\\) to \\(\\beta_5\\))"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#fitting-multiple-logistic-regression-model",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#fitting-multiple-logistic-regression-model",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Fitting Multiple Logistic Regression Model",
    "text": "Fitting Multiple Logistic Regression Model\n\nmulti_bc = glm(Late_stage_diag ~ Race_Ethnicity + Age_c, data = bc, family = binomial)\nsummary(multi_bc)\n\n\nCall:\nglm(formula = Late_stage_diag ~ Race_Ethnicity + Age_c, family = binomial, \n    data = bc)\n\nCoefficients:\n                                                 Estimate Std. Error z value\n(Intercept)                                     -1.038389   0.027292 -38.048\nRace_EthnicityHispanic-Latino                   -0.015424   0.083653  -0.184\nRace_EthnicityNH American Indian/Alaskan Native -0.085704   0.484110  -0.177\nRace_EthnicityNH Asian/Pacific Islander          0.133965   0.083797   1.599\nRace_EthnicityNH Black                           0.357692   0.071789   4.983\nAge_c                                            0.057151   0.003209  17.811\n                                                Pr(&gt;|z|)    \n(Intercept)                                      &lt; 2e-16 ***\nRace_EthnicityHispanic-Latino                      0.854    \nRace_EthnicityNH American Indian/Alaskan Native    0.859    \nRace_EthnicityNH Asian/Pacific Islander            0.110    \nRace_EthnicityNH Black                          6.27e-07 ***\nAge_c                                            &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 11861  on 9999  degrees of freedom\nResidual deviance: 11484  on 9994  degrees of freedom\nAIC: 11496\n\nNumber of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#breast-cancer-example-fitted-model",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#breast-cancer-example-fitted-model",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Breast Cancer Example: Fitted Model",
    "text": "Breast Cancer Example: Fitted Model\n\nWe now have the fitted logistic regression model with both race and ethnicity and age:\n\n\\[ \\begin{aligned}\n\\text{logit}\\left(\\widehat{\\pi}(\\mathbf{X})\\right) = & \\widehat{\\beta}_0\n+ \\widehat{\\beta}_1 \\cdot I \\left( R/E = H/L \\right)\n+ \\widehat{\\beta}_2 \\cdot I \\left( R/E = NH AIAN \\right) \\\\\n& + \\widehat{\\beta}_3 \\cdot I \\left( R/E = NH API \\right)\n+ \\widehat{\\beta}_4 \\cdot I \\left( R/E = NH B \\right)\n+ \\widehat{\\beta}_5 \\cdot Age \\\\\n\\\\\n\\text{logit}\\left(\\widehat{\\pi}(\\mathbf{X})\\right) = &-4.56\n-0.02 \\cdot I \\left( R/E = H/L \\right)\n-0.09 \\cdot I \\left( R/E = NH AIAN \\right) \\\\\n& +0.13 \\cdot I \\left( R/E = NH API \\right)\n+0.36 \\cdot I \\left( R/E = NH B \\right)\n+0.06 \\cdot Age\n\\end{aligned}\\]\n\n6 total coefficients (\\(\\widehat{\\beta}_0\\) to \\(\\widehat{\\beta}_5\\))"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#testing-significance-of-the-coefficients",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#testing-significance-of-the-coefficients",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Testing Significance of the Coefficients",
    "text": "Testing Significance of the Coefficients\n\nRefer to Lesson 6 for more information on each test!!\n\n \n\nWe use the same three tests that we discussed in Simple Logistic Regression to test individual coefficients\n\nWald test\n\nCan be used to test a single coefficient\n\nScore test\nLikelihood ratio test (LRT)\n\nCan be used to test a single coefficient or multiple coefficients\n\n\n\n \n\nTextbook and our class focuses on Wald and LRT only"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#a-note-on-wording",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#a-note-on-wording",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "A note on wording",
    "text": "A note on wording\n\nWhen I say “test a single coefficient” or “test multiple coefficients” I am referring to the \\(\\beta\\)’s\n\nA single variable can have a single coefficient\n\nExample: testing age\n\nA single variable can have multiple coefficients\n\nExample: testing race and ethnicity\n\nMuliple variables will have multiple coefficients\n\nExample: testing age and race and ethnicity together\n\n\n \nWhen I say “test a variable” I mean “determine if the model with the variable is more likely than the model without that variable”\n\nWe can use the Wald test to do this is some scenarios (single, continuous covariate)\nBUT I advise you practice using the LRT whenever comparing models (aka testing variables)"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#all-three-tests-together",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#all-three-tests-together",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "All three tests together",
    "text": "All three tests together"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#from-lesson-6-wald-test",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#from-lesson-6-wald-test",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "From Lesson 6: Wald test",
    "text": "From Lesson 6: Wald test\n\nAssumes test statistic W follows a standard normal distribution under the null hypothesis\nTest statistic: \\[W=\\frac{{\\hat{\\beta}}_j}{SE_{\\hat{\\beta}_j}}\\sim N(0,1)\\]\n\nwhere \\(\\widehat{\\beta}_j\\) is a MLE of coefficient \\(j\\)\n\n95% Wald confidence interval: \\[{\\hat{\\beta}}_1\\pm1.96 \\cdot SE_{{\\hat{\\beta}}_j}\\]\nThe Wald test is a routine output in R (summary() of glm() output)\n\nIncludes \\(SE_{{\\hat{\\beta}}_j}\\) and can easily find confidence interval with tidy()\n\nImportant note: Wald test is best for confidence intervals of our coefficient estimates or estimated odds ratios."
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#wald-test-procedure-with-confidence-intervals",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#wald-test-procedure-with-confidence-intervals",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Wald test procedure with confidence intervals",
    "text": "Wald test procedure with confidence intervals\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the confidence interval and determine if it overlaps with null\n\nOverlap with null (usually 0 for coefficient) = fail to reject null\nNo overlap with null (usually 0 for coefficient) = reject null\n\nWrite a conclusion to the hypothesis test\n\nWhat is the estimate and its confidence interval?\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#wald-test-in-our-example",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#wald-test-in-our-example",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Wald test in our example",
    "text": "Wald test in our example\n\nFrom our multiple logistic regression model:\n\n\\[ \\begin{aligned}\n\\text{logit}\\left(\\pi(x_i)\\right) = & \\beta_0\n+\\beta_1 \\cdot I \\left( R/E = H/L \\right)\n+\\beta_2 \\cdot I \\left( R/E = NH AIAN \\right) \\\\\n& +\\beta_3 \\cdot I \\left( R/E = NH API \\right)\n+\\beta_4 \\cdot I \\left( R/E = NH B \\right)\n+\\beta_5 \\cdot Age\n\\end{aligned}\\]\n \n\nWald test can help us construct the confidence interval for ALL coefficient estimates\n\n \n\nIf we want to use the Wald test to determine if a covariate is significant in our model\n\nCan only do so for age"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\nNeeded steps:\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\nCalculate the confidence interval and determine if it overlaps with null\nWrite a conclusion to the hypothesis test"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-1",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\n\nSet the level of significance \\(\\alpha\\)\n\n\\(\\alpha = 0.05\\)\n\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\n\\(H_0: \\beta_5 = 0\\)\n\\(H_1: \\beta_5 \\neq 0\\)"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-2",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-2",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\n\nCalculate the confidence interval and determine if it overlaps with null\n\n\nLook at coefficient estimates (CI should not contain 0) OR estimated odds ratio (CI should not contain 1)\n\n\n\n\ntidy(multi_bc, conf.int=T) %&gt;% \n  gt() %&gt;% \n  tab_options(table.font.size = 28) %&gt;%\n  fmt_number(decimals = 2)\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n−1.04\n0.03\n−38.05\n0.00\n−1.09\n−0.99\n\n\nRace_EthnicityHispanic-Latino\n−0.02\n0.08\n−0.18\n0.85\n−0.18\n0.15\n\n\nRace_EthnicityNH American Indian/Alaskan Native\n−0.09\n0.48\n−0.18\n0.86\n−1.12\n0.81\n\n\nRace_EthnicityNH Asian/Pacific Islander\n0.13\n0.08\n1.60\n0.11\n−0.03\n0.30\n\n\nRace_EthnicityNH Black\n0.36\n0.07\n4.98\n0.00\n0.22\n0.50\n\n\nAge_c\n0.06\n0.00\n17.81\n0.00\n0.05\n0.06"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-3",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-3",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\n\nCalculate the confidence interval and determine if it overlaps with null\n\n\nLook at coefficient estimates (CI should not contain 0) OR estimated odds ratio (CI should not contain 1)\n\n\n\n\ntbl_regression(multi_bc, \n               exponentiate = TRUE) %&gt;%\n  as_gt() %&gt;% \n  tab_options(table.font.size = 30)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\n\nRace_Ethnicity\n\n\n\n\n\n\n\n\n    NH White\n—\n—\n\n\n\n\n    Hispanic-Latino\n0.98\n0.83, 1.16\n0.9\n\n\n    NH American Indian/Alaskan Native\n0.92\n0.33, 2.25\n0.9\n\n\n    NH Asian/Pacific Islander\n1.14\n0.97, 1.35\n0.11\n\n\n    NH Black\n1.43\n1.24, 1.65\n&lt;0.001\n\n\nAge_c\n1.06\n1.05, 1.07\n&lt;0.001\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-4",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-4",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\n\nWrite a conclusion to the hypothesis test\n\nGiven race and ethnicity is already in the model, the regression model with age is more likely than the model without age (p-val &lt; 0.001)."
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#likelihood-ratio-test",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#likelihood-ratio-test",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Likelihood ratio test",
    "text": "Likelihood ratio test\n\nLikelihood ratio test answers the question:\n\nFor a specific covariate, which model tell us more about the outcome variable: the model including the covariate (or set of covariates) or the model omitting the covariate (or set of covariates)?\nAka: Which model is more likely given our data: model including the covariate or the model omitting the covariate?\n\n\n \n\nTest a single coefficient by comparing different models\n\nVery similar to the F-test\n\n\n \n\nImportant: LRT can be used conduct hypothesis tests for multiple coefficients\n\nJust like F-test, we can test a single coefficient, continuous/binary covariate, multi-level covariate, or multiple covariates"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#likelihood-ratio-test-33",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#likelihood-ratio-test-33",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Likelihood ratio test (3/3)",
    "text": "Likelihood ratio test (3/3)\n\nIf testing single variable and it’s continuous or binary, still use this hypothesis test:\n\n\\(H_0\\): \\(\\beta_j = 0\\)\n\\(H_1\\): \\(\\beta_j \\neq 0\\)\n\n\n \n\nIf testing single variable and it’s categorical with mroe than 2 groups, use this hypothesis test:\n\n\\(H_0\\): \\(\\beta_j=\\beta_{j+1}=\\ldots=\\beta_{j+i-1}=0\\)\n\\(H_1\\): at least one of the above \\(\\beta\\)’s is not equal to 0\n\n\n \n\nIf testing a set of variables, use this hypothesis test:\n\n\\(H_0\\): \\(\\beta_1=\\beta_{2}=\\ldots=\\beta_{k}=0\\)\n\\(H_1\\): at least one of the above \\(\\beta\\)’s is not equal to 0"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#lrt-procedure",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#lrt-procedure",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "LRT procedure",
    "text": "LRT procedure\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic and p-value\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#likelihood-ratio-test-33-1",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#likelihood-ratio-test-33-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Likelihood ratio test (3/3)",
    "text": "Likelihood ratio test (3/3)\n\nFrom our multiple logistic regression model:\n\n\\[ \\begin{aligned}\n\\text{logit}\\left(\\pi(\\mathbf{X})\\right) = & \\beta_0\n+\\beta_1 \\cdot I \\left( R/E = H/L \\right)\n+\\beta_2 \\cdot I \\left( R/E = NH AIAN \\right) \\\\\n& +\\beta_3 \\cdot I \\left( R/E = NH API \\right)\n+\\beta_4 \\cdot I \\left( R/E = NH B \\right)\n+\\beta_5 \\cdot Age\n\\end{aligned}\\]\n\nWe can test a single coefficient or multiple coefficients\n \n\nExample 1: Single, continuous variable: Age\n\n \n\nExample 2: Single, &gt;2 categorical variable: Race and Ethnicity\n\n \n\nExample 3: Set of variables: Race and Ethnicity, and Age"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#reminder-on-nested-models",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#reminder-on-nested-models",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Reminder on nested models",
    "text": "Reminder on nested models\n\nLikelihood ratio test is only suitable to test “nested” models\n“Nested” models means the bigger model (full model) contains all the independent variables of the smaller model (reduced model)\nWe cannot compare the following two models using LRT:\n\nModel 1: \\[ \\begin{aligned} \\text{logit}\\left(\\pi(\\mathbf{X})\\right) = & \\beta_0 +\\beta_1 \\cdot I \\left( R/E = H/L \\right) +\\beta_2 \\cdot I \\left( R/E = NH AIAN \\right) \\\\ & +\\beta_3 \\cdot I \\left( R/E = NH API \\right) +\\beta_4 \\cdot I \\left( R/E = NH B \\right) \\end{aligned}\\]\nModel 2: \\[\\begin{aligned} \\text{logit}\\left(\\pi(Age)\\right) = & \\beta_0+\\beta_1 \\cdot Age  \\end{aligned}\\]\n\nIf the two models to be compared are not nested, likelihood ratio test should not be used"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-5",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-5",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\nNeeded steps:\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\nCalculate the test statistic and p-value\nWrite a conclusion to the hypothesis test"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-6",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-6",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\n\nSet the level of significance \\(\\alpha\\)\n\n\\(\\alpha = 0.05\\)\n\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\n\\(H_0: \\beta_5 = 0\\) or model without age is more likely\n\\(H_1: \\beta_5 \\neq 0\\) or model with age is more likely"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-7",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-7",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\n\nCalculate the test statistic and p-value\n\n\nmulti_bc = glm(Late_stage_diag ~ Race_Ethnicity + Age_c, data = bc, family = binomial)\nre_bc = glm(Late_stage_diag ~ Race_Ethnicity, data = bc, family = binomial)\n\nlmtest::lrtest(multi_bc, re_bc)\n\nLikelihood ratio test\n\nModel 1: Late_stage_diag ~ Race_Ethnicity + Age_c\nModel 2: Late_stage_diag ~ Race_Ethnicity\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n1   6 -5741.8                         \n2   5 -5918.1 -1 352.63  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-8",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-8",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\n\nWrite a conclusion to the hypothesis test\n\nGiven race and ethnicity is already in the model, the regression model with age is more likely than the model without age (p-val &lt; \\(2.2\\cdot10^{-16}\\))."
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-2-single-2-categorical-variable-race-and-ethnicity",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-2-single-2-categorical-variable-race-and-ethnicity",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 2: Single, >2 categorical variable: Race and Ethnicity",
    "text": "Example 2: Single, &gt;2 categorical variable: Race and Ethnicity\n\n\nSingle, &gt;2 categorical variable: Race and Ethnicity\n\n\nGiven age is already in the model, is the regression model with race and ethnicity more likely than the model without race and ethnicity?\n\n\nNeeded steps:\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\nCalculate the test statistic and p-value\nWrite a conclusion to the hypothesis test"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-2-single-2-categorical-variable-race-and-ethnicity-1",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-2-single-2-categorical-variable-race-and-ethnicity-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 2: Single, >2 categorical variable: Race and Ethnicity",
    "text": "Example 2: Single, &gt;2 categorical variable: Race and Ethnicity\n\n\nSingle, &gt;2 categorical variable: Race and Ethnicity\n\n\nGiven age is already in the model, is the regression model with race and ethnicity more likely than the model without race and ethnicity?\n\n\n\nSet the level of significance \\(\\alpha\\)\n\n\\(\\alpha = 0.05\\)\n\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\n\\(H_0: \\beta_1 = \\beta_2 = \\beta_3 = \\beta_4 = 0\\) or model without race and ethnicity is more likely\n$H_1: at least one \\(\\beta\\) is not 0 or model with race and ethnicity is more likely"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-2-single-2-categorical-variable-race-and-ethnicity-2",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-2-single-2-categorical-variable-race-and-ethnicity-2",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 2: Single, >2 categorical variable: Race and Ethnicity",
    "text": "Example 2: Single, &gt;2 categorical variable: Race and Ethnicity\n\n\nSingle, &gt;2 categorical variable: Race and Ethnicity\n\n\nGiven age is already in the model, is the regression model with race and ethnicity more likely than the model without race and ethnicity?\n\n\n\nCalculate the test statistic and p-value\n\n\nmulti_bc = glm(Late_stage_diag ~ Race_Ethnicity + Age_c, data = bc, family = binomial)\nage_bc = glm(Late_stage_diag ~ Age_c, data = bc, family = binomial)\n\nlmtest::lrtest(multi_bc, age_bc)\n\nLikelihood ratio test\n\nModel 1: Late_stage_diag ~ Race_Ethnicity + Age_c\nModel 2: Late_stage_diag ~ Age_c\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n1   6 -5741.8                         \n2   2 -5754.8 -4 26.053  3.087e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-2-single-2-categorical-variable-race-and-ethnicity-3",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-2-single-2-categorical-variable-race-and-ethnicity-3",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 2: Single, >2 categorical variable: Race and Ethnicity",
    "text": "Example 2: Single, &gt;2 categorical variable: Race and Ethnicity\n\n\nSingle, &gt;2 categorical variable: Race and Ethnicity\n\n\nGiven age is already in the model, is the regression model with race and ethnicity more likely than the model without race and ethnicity?\n\n\n\nWrite a conclusion to the hypothesis test\n\nGiven age is already in the model, the regression model with race and ethnicity is more likely than the model without race and ethnicity (p-val = \\(3.1\\cdot10^{-5}\\) &lt; 0.05)."
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-3-set-of-variables-race-and-ethnicity-and-age",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-3-set-of-variables-race-and-ethnicity-and-age",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 3: Set of variables: Race and Ethnicity, and Age",
    "text": "Example 3: Set of variables: Race and Ethnicity, and Age\n\n\nSet of variables: Race and Ethnicity, and Age\n\n\nIs the regression model with race and ethnicity and age more likely than the model without race and ethnicity nor age?\n\n\nNeeded steps:\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\nCalculate the test statistic and p-value\nWrite a conclusion to the hypothesis test"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-3-set-of-variables-race-and-ethnicity-and-age-1",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-3-set-of-variables-race-and-ethnicity-and-age-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 3: Set of variables: Race and Ethnicity, and Age",
    "text": "Example 3: Set of variables: Race and Ethnicity, and Age\n\n\nSet of variables: Race and Ethnicity, and Age\n\n\nIs the regression model with race and ethnicity and age more likely than the model without race and ethnicity nor age?\n\n\n\nSet the level of significance \\(\\alpha\\)\n\n\\(\\alpha = 0.05\\)\n\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\n\\(H_0: \\beta_1 = \\beta_2 = \\beta_3 = \\beta_4 = \\beta_5 = 0\\) or model without race and ethnicity and age is more likely\n$H_1: at least one \\(\\beta\\) is not 0 or model with race and ethnicity and age is more likely"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-3-set-of-variables-race-and-ethnicity-and-age-2",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-3-set-of-variables-race-and-ethnicity-and-age-2",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 3: Set of variables: Race and Ethnicity, and Age",
    "text": "Example 3: Set of variables: Race and Ethnicity, and Age\n\n\nSet of variables: Race and Ethnicity, and Age\n\n\nIs the regression model with race and ethnicity and age more likely than the model without race and ethnicity nor age?\n\n\n\nCalculate the test statistic and p-value\n\n\nmulti_bc = glm(Late_stage_diag ~ Race_Ethnicity + Age_c, data = bc, family = binomial)\nintercept_bc = glm(Late_stage_diag ~ 1, data = bc, family = binomial)\n\nlmtest::lrtest(multi_bc, intercept_bc)\n\nLikelihood ratio test\n\nModel 1: Late_stage_diag ~ Race_Ethnicity + Age_c\nModel 2: Late_stage_diag ~ 1\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n1   6 -5741.8                         \n2   1 -5930.5 -5 377.32  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-3-set-of-variables-race-and-ethnicity-and-age-3",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-3-set-of-variables-race-and-ethnicity-and-age-3",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 3: Set of variables: Race and Ethnicity, and Age",
    "text": "Example 3: Set of variables: Race and Ethnicity, and Age\n\n\nSet of variables: Race and Ethnicity, and Age\n\n\nIs the regression model with race and ethnicity and age more likely than the model without race and ethnicity nor age?\n\n\n\nWrite a conclusion to the hypothesis test\n\nThe regression model with race and ethnicity and age is more likely than the model omitting race and ethnicity and age (p-val &lt; \\(2.2\\cdot10^{-16}\\))."
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#estimatedpredicted-probability-for-mlr",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#estimatedpredicted-probability-for-mlr",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Estimated/Predicted Probability for MLR",
    "text": "Estimated/Predicted Probability for MLR\n\nBasic idea for predicting/estimating probability stays the same\n\n \n\nCalculations will be slightly different\n\nEspecially for the confidence interval\n\n\n \n\nRecall our fitted model for late stage breast cancer diagnosis: \\[ \\begin{aligned}\n\\text{logit}\\left(\\widehat{\\pi}(\\mathbf{X})\\right) = &-4.56\n-0.02 \\cdot I \\left( R/E = H/L \\right)\n-0.09 \\cdot I \\left( R/E = NH AIAN \\right) \\\\\n& +0.13 \\cdot I \\left( R/E = NH API \\right)\n+0.36 \\cdot I \\left( R/E = NH B \\right)\n+0.06 \\cdot Age\n\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicted-probability",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicted-probability",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Predicted Probability",
    "text": "Predicted Probability\n\nWe may be interested in predicting probability of having a late stage breast cancer diagnosis for a specific age.\nThe predicted probability is the estimated probability of having the event for given values of covariate(s)\nRecall our fitted model for late stage breast cancer diagnosis: \\[ \\begin{aligned}\n\\text{logit}\\left(\\widehat{\\pi}(\\mathbf{X})\\right) = &-4.56\n-0.02 \\cdot I \\left( R/E = H/L \\right)\n-0.09 \\cdot I \\left( R/E = NH AIAN \\right) \\\\\n& +0.13 \\cdot I \\left( R/E = NH API \\right)\n+0.36 \\cdot I \\left( R/E = NH B \\right)\n+0.06 \\cdot Age\n\\end{aligned}\\]\nWe can convert it to the predicted probability: \\[\\hat{\\pi}(\\mathbf{X})=\\dfrac{\\exp \\left( \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot I \\left( R/E = H/L \\right) + ... + \\widehat{\\beta}_5 \\cdot Age \\right)}  {1+\\exp \\left(\\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot I \\left( R/E = H/L \\right) + ... + \\widehat{\\beta}_5 \\cdot Age \\right)}\\]\n\nThis is an inverse logit calculation\n\nWe can calculate this using the the predict() function like in BSTA 512\n\nAnother option: taking inverse logit of fitted values from augment() function"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Predicting probability in R",
    "text": "Predicting probability in R\n\n\nPredicting probability of late stage breast cancer diagnosis\n\n\nFor someone who is 60 years old and Non-Hispanic Asian/Pacific Islander, what is the predicted probability for late stage breast cancer diagnosis (with confidence intervals)?\n\n\nNeeded steps:\n\nCalculate probability prediction\nCheck if we can use Normal approximation\nCalculate confidence interval\n\nUsing logit scale then converting\nUsing Normal approximation\n\nInterpret results"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r-1",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Predicting probability in R",
    "text": "Predicting probability in R\n\n\nPredicting probability of late stage breast cancer diagnosis\n\n\nFor someone who is 60 years old and Non-Hispanic Asian/Pacific Islander, what is the predicted probability for late stage breast cancer diagnosis (with confidence intervals)?\n\n\n\nCalculate probability prediction\n\n\nnewdata = data.frame(Age_c = 60 - mean_age, \n                     Race_Ethnicity = \"NH Asian/Pacific Islander\")\npred1 = predict(multi_bc, newdata, se.fit = T, type=\"response\")\npred1\n\n$fit\n        1 \n0.2685667 \n\n$se.fit\n         1 \n0.01572695 \n\n$residual.scale\n[1] 1"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r-2",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r-2",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Predicting probability in R",
    "text": "Predicting probability in R\n\n\nPredicting probability of late stage breast cancer diagnosis\n\n\nFor someone who is 60 years old and Non-Hispanic Asian/Pacific Islander, what is the predicted probability for late stage breast cancer diagnosis (with confidence intervals)?\n\n\n\nCheck if we can use Normal approximation\n\nWe can use the Normal approximation if: \\(\\widehat{p}n = \\widehat{\\pi}(X)\\cdot n &gt; 10\\) and \\((1-\\widehat{p})n = (1-\\widehat{\\pi}(X))\\cdot n &gt; 10\\).\n\nn = nobs(multi_bc)\np = pred1$fit\nn*p\n\n       1 \n2685.667 \n\nn*(1-p)\n\n       1 \n7314.333 \n\n\nWe can use the Normal approximation!"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r-3",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r-3",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Predicting probability in R",
    "text": "Predicting probability in R\n\n\nPredicting probability of late stage breast cancer diagnosis\n\n\nFor someone who is 60 years old and Non-Hispanic Asian/Pacific Islander, what is the predicted probability for late stage breast cancer diagnosis (with confidence intervals)?\n\n\n3b. Calculate confidence interval (Option 2: with Normal approximation)\n\npred = predict(multi_bc, newdata, se.fit = T, type = \"response\")\n\nLL_CI = pred$fit - qnorm(1-0.05/2) * pred$se.fit\nUL_CI = pred$fit + qnorm(1-0.05/2) * pred$se.fit\n\nc(Pred = pred$fit, LL_CI, UL_CI) %&gt;% round(digits=3)\n\nPred.1      1      1 \n 0.269  0.238  0.299"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r-4",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r-4",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Predicting probability in R",
    "text": "Predicting probability in R\n\n\nPredicting probability of late stage breast cancer diagnosis\n\n\nFor someone who is 60 years old and Non-Hispanic Asian/Pacific Islander, what is the predicted probability for late stage breast cancer diagnosis (with confidence intervals)?\n\n\n\nInterpret results\n\nFor someone who is 60 years old and Non-Hispanic Asian/Pacific Islander, the predicted probability of late stage breast cancer diagnosis is 0.269 (95% CI: 0.238, 0.299)."
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-table",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-table",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "How to present odds ratios: Table",
    "text": "How to present odds ratios: Table\n\ntbl_regression() in the gtsummary package is helpful for presenting the odds ratios in a clean way\n\n\nlibrary(gtsummary)\ntbl_regression(multi_bc, exponentiate = TRUE) %&gt;% \n  as_gt() %&gt;% # allows us to use tab_options()\n  tab_options(table.font.size = 38)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\n\nRace_Ethnicity\n\n\n\n\n\n\n\n\n    NH White\n—\n—\n\n\n\n\n    Hispanic-Latino\n0.98\n0.83, 1.16\n0.9\n\n\n    NH American Indian/Alaskan Native\n0.92\n0.33, 2.25\n0.9\n\n\n    NH Asian/Pacific Islander\n1.14\n0.97, 1.35\n0.11\n\n\n    NH Black\n1.43\n1.24, 1.65\n&lt;0.001\n\n\nAge_c\n1.06\n1.05, 1.07\n&lt;0.001\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-forest-plot-setup",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-forest-plot-setup",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "How to present odds ratios: Forest Plot Setup",
    "text": "How to present odds ratios: Forest Plot Setup\n\nlibrary(broom.helpers)\n\n\nAttaching package: 'broom.helpers'\n\n\nThe following objects are masked from 'package:gtsummary':\n\n    all_categorical, all_continuous, all_contrasts, all_dichotomous,\n    all_interaction, all_intercepts\n\nMLR_tidy = tidy_and_attach(multi_bc, conf.int=T, exponentiate = T) %&gt;%\n  tidy_remove_intercept() %&gt;%\n  tidy_add_reference_rows() %&gt;%\n  tidy_add_estimate_to_reference_rows() %&gt;%\n  tidy_add_term_labels()\nglimpse(MLR_tidy)\n\nRows: 6\nColumns: 16\n$ term           &lt;chr&gt; \"Race_EthnicityNH White\", \"Race_EthnicityHispanic-Latin…\n$ variable       &lt;chr&gt; \"Race_Ethnicity\", \"Race_Ethnicity\", \"Race_Ethnicity\", \"…\n$ var_label      &lt;chr&gt; \"Race_Ethnicity\", \"Race_Ethnicity\", \"Race_Ethnicity\", \"…\n$ var_class      &lt;chr&gt; \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"nume…\n$ var_type       &lt;chr&gt; \"categorical\", \"categorical\", \"categorical\", \"categoric…\n$ var_nlevels    &lt;int&gt; 5, 5, 5, 5, 5, NA\n$ contrasts      &lt;chr&gt; \"contr.treatment\", \"contr.treatment\", \"contr.treatment\"…\n$ contrasts_type &lt;chr&gt; \"treatment\", \"treatment\", \"treatment\", \"treatment\", \"tr…\n$ reference_row  &lt;lgl&gt; TRUE, FALSE, FALSE, FALSE, FALSE, NA\n$ label          &lt;chr&gt; \"NH White\", \"Hispanic-Latino\", \"NH American Indian/Alas…\n$ estimate       &lt;dbl&gt; 1.0000000, 0.9846940, 0.9178662, 1.1433526, 1.4300256, …\n$ std.error      &lt;dbl&gt; NA, 0.083653090, 0.484110085, 0.083796726, 0.071788616,…\n$ statistic      &lt;dbl&gt; NA, -0.1843845, -0.1770333, 1.5986877, 4.9825778, 17.81…\n$ p.value        &lt;dbl&gt; NA, 8.537118e-01, 8.594822e-01, 1.098900e-01, 6.274274e…\n$ conf.low       &lt;dbl&gt; NA, 0.8344282, 0.3262638, 0.9688184, 1.2414629, 1.05221…\n$ conf.high      &lt;dbl&gt; NA, 1.158411, 2.254643, 1.345732, 1.645053, 1.065538"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-forest-plot-setup-1",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-forest-plot-setup-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "How to present odds ratios: Forest Plot Setup",
    "text": "How to present odds ratios: Forest Plot Setup\n\nMLR_tidy = MLR_tidy %&gt;%\n  mutate(var_label = case_match(var_label, \n                               \"Race_Ethnicity\" ~ \"Race and ethnicity\", \n                               \"Age_c\" ~ \"\"), \n         label = case_match(label, \n                            \"NH White\" ~ \"Non-Hispanic White\", \n                            \"Hispanic-Latino\" ~ \"Hispanic-Latinx\", \n                            \"NH American Indian/Alaskan Native\" ~ \"Non-Hispanic American \\n Indian/Alaskan Native\", \n                            \"NH Asian/Pacific Islander\" ~ \"Non-Hispanic \\n Asian/Pacific Islander\",\n                            \"NH Black\" ~ \"Non-Hispanic Black\", \n                               \"Age_c\" ~ \"Age (yrs)\"))\n  # %&gt;%\n  #                       fct_relevel(\"Age (yrs)\", \"Non-Hispanic \\n Asian/Pacific Islander\", \"Non-Hispanic American \\n Indian/Alaskan Native\", \"Non-Hispanic White\", \"Hispanic-Latinx\", \"Non-Hispanic Black\"))"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-forest-plot",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-forest-plot",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "How to present odds ratios: Forest Plot",
    "text": "How to present odds ratios: Forest Plot\n\nMLR_tidy = MLR_tidy %&gt;% mutate(label = fct_reorder(label, term))\n\nplot_MLR = ggplot(data=MLR_tidy, \n       aes(y=label, x=estimate, xmin=conf.low, xmax=conf.high)) + \n  geom_point(size = 3) +  geom_errorbarh(height=.2) + \n  \n  geom_vline(xintercept=1, color='#C2352F', linetype='dashed', alpha=1) +\n  theme_classic() +\n  \n  facet_grid(rows = vars(var_label), scales = \"free\",\n             space='free_y', switch = \"y\") + \n  \n  labs(x = \"OR (95% CI)\", \n       title = \"Odds ratios of Late Stage Breast Cancer Diagnosis\") +\n  theme(axis.title = element_text(size = 25), \n        axis.text = element_text(size = 25), \n        title = element_text(size = 25), \n        axis.title.y=element_blank(), \n        strip.text = element_text(size = 25), \n        strip.placement = \"outside\", \n        strip.background = element_blank())\n# plot_MLR"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-forest-plot-1",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-forest-plot-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "How to present odds ratios: Forest Plot",
    "text": "How to present odds ratios: Forest Plot\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_errorbarh()`)."
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#adding-odds-ratios",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#adding-odds-ratios",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Adding odds ratios",
    "text": "Adding odds ratios\n\nMLR_tidy = MLR_tidy %&gt;% \n  mutate(estimate_r = round(estimate, 2), \n         conf.low_r = round(conf.low, 2), \n         conf.high_r = round(conf.high, 2), \n         OR_char = paste0(estimate_r, \" (\", conf.low_r, \", \", conf.high_r, \")\"), \n         OR_char = ifelse(reference_row == F | is.na(reference_row), OR_char, NA))\n\n \n\n“Plot” of the text for odds ratios estimates\n\n\nOR_labs = ggplot(data=MLR_tidy, aes(y=label)) +\n  geom_text(aes(x = -1, label = OR_char), hjust = 0, size=8) +   \n  xlim(-1, 1) +\n  facet_grid(rows = vars(var_label), scales = \"free\",\n             space='free_y') +\n  theme_void() + \n    theme(strip.text = element_blank())"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#combine-them",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#combine-them",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Combine them!!",
    "text": "Combine them!!\n\nlibrary(cowplot)\n\n\nAttaching package: 'cowplot'\n\n\nThe following object is masked from 'package:ggpubr':\n\n    get_legend\n\n\nThe following object is masked from 'package:gt':\n\n    as_gtable\n\n\nThe following object is masked from 'package:lubridate':\n\n    stamp\n\nplot_grid(plot_MLR, OR_labs, ncol=2, align = \"h\", rel_widths = c(4, 1))\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_errorbarh()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_text()`)."
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#multivariable-logistic-regression-model",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#multivariable-logistic-regression-model",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Multivariable Logistic Regression Model",
    "text": "Multivariable Logistic Regression Model\n\nThe multivariable model of logistic regression (called multiple logistic regression) is useful in that it statistically adjusts the estimated effect of each variable in the model\n\n \n\nEach estimated coefficient provides an estimate of the log odds adjusting for all other variables included in the model\n \n\nThe adjusted odds ratio can be different from or similar to the unadjusted odds ratio\n\n \n\nComparing adjusted vs. unadjusted odds ratios can be a useful activity"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#interpretation-of-coefficients-in-mlr",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#interpretation-of-coefficients-in-mlr",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Interpretation of Coefficients in MLR",
    "text": "Interpretation of Coefficients in MLR\n\nThe interpretation of coefficients in multiple logistic regression is essentially the same as the interpretation of coefficients in simple logistic regression\n\n \n\nFor interpretation, we need to\n\npoint out that these are adjusted estimates\nprovide a list of other variables in the model"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-race-and-ethnicity-and-age-model-fit-fixed",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-race-and-ethnicity-and-age-model-fit-fixed",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example: Race and Ethnicity and Age model fit (FIXED)",
    "text": "Example: Race and Ethnicity and Age model fit (FIXED)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\n\nRace_Ethnicity\n\n\n\n\n\n\n\n\n    NH White\n—\n—\n\n\n\n\n    Hispanic-Latino\n0.98\n0.83, 1.16\n0.9\n\n\n    NH American Indian/Alaskan Native\n0.92\n0.33, 2.25\n0.9\n\n\n    NH Asian/Pacific Islander\n1.14\n0.97, 1.35\n0.11\n\n\n    NH Black\n1.43\n1.24, 1.65\n&lt;0.001\n\n\nAge_c\n1.06\n1.05, 1.07\n&lt;0.001\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\n\n\nThe estimated odds of late stage breast cancer diagnosis for Hispanic-Latinx individuals is 0.98 times that of Non-Hispanic White individuals, controlling for age (95% CI: 0.83, 1.16).\nThe estimated odds of late stage breast cancer diagnosis for Non-Hispanic American Indian/Alaskan Natives is 0.92 times that of Non-Hispanic White individuals, controlling for age (95% CI: 0.33, 2.25).\nThe estimated odds of late stage breast cancer diagnosis for Non-Hispanic Asian/Pacific Islanders is 1.14 times that of Non-Hispanic White individuals, controlling for age (95% CI: 0.97, 1.35).\nThe estimated odds of late stage breast cancer diagnosis for Non-Hispanic Black individuals is 1.43 times that of Non-Hispanic White individuals, controlling for age (95% CI: 1.24, 1.65).\nFor every one year increase in age, there is an 6% increase in the estimated odds of late stage breast cancer diagnosis, adjusting for race and ethnicity (95% CI: 5%, 7%)."
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#last-class-glow-study-with-interactions",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#last-class-glow-study-with-interactions",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Last Class: GLOW Study with interactions",
    "text": "Last Class: GLOW Study with interactions\n\nOutcome variable: any fracture in the first year of follow up (FRACTURE: 0 or 1)\nRisk factor/variable of interest: history of prior fracture (PRIORFRAC: 0 or 1)\nPotential confounder or effect modifier: age (AGE, a continuous variable)\nFitted model with interactions: \\[\\begin{aligned} \\text{logit}\\left(\\widehat\\pi(\\mathbf{X})\\right) & = \\widehat\\beta_0 &+ &\\widehat\\beta_1\\cdot I(\\text{PF}) & + &\\widehat\\beta_2\\cdot Age& + &\\widehat\\beta_3 \\cdot I(\\text{PF}) \\cdot Age \\\\  \n\\text{logit}\\left(\\widehat\\pi(\\mathbf{X})\\right) & = -1.376 &+ &1.002\\cdot I(\\text{PF})& + &0.063\\cdot Age& -&0.057 \\cdot I(\\text{PF}) \\cdot Age\n\\end{aligned}\\]\n\n \n\nToday: determine the overall fit of this model"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#last-class-reporting-results-of-glow-study-with-interactions",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#last-class-reporting-results-of-glow-study-with-interactions",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Last Class: Reporting results of GLOW Study with interactions",
    "text": "Last Class: Reporting results of GLOW Study with interactions\n\nRemember our main covariate is prior fracture, so we want to focuse on how age changes the relationship between prior fracture and a new fracture!\n\n\n\nFor individuals 69 years old, the estimated odds of a new fracture for individuals with prior fracture is 2.72 times the estimated odds of a new fracture for individuals with no prior fracture (95% CI: 1.70, 4.35). As seen in Figure 1 (a), the odds ratio of a new fracture when comparing prior fracture status decreases with age, indicating that the effect of prior fractures on new fractures decreases as individuals get older. In Figure 1 (b), it is evident that for both prior fracture statuses, the predict probability of a new fracture increases as age increases. However, the predicted probability of new fracture for those without a prior fracture increases at a higher rate than that of individuals with a prior fracture. Thus, the predicted probabilities of a new fracture converge at age [insert age here].\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n(a) Odds ratio of fracture outcome comparing prior fracture to no prior fracture\n\n\n\n\n\n\n\n\n\n\n\n(b) Predicted probability of fracture\n\n\n\n\n\n\n\nFigure 1: Plots of odds ratio and predicted probability from fitted interaction model"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#overview-12",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#overview-12",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Overview (1/2)",
    "text": "Overview (1/2)\n\nOnce a potential final model has been determined, we need to assess the fit of the model\n\n \n\nVariable selection is no longer our focus at this stage\n\nWe want to find answer to whether the model fits the data adequately\n\n\n \n\nAssessing the Goodness of Fit or Assessing model fit\n\nAssess how well our fitted logistic regression model predicts/estimates the observed outcomes\nComparison: fitted/estimated outcome vs. observed outcome"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#some-good-measurements-for-our-final-models",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#some-good-measurements-for-our-final-models",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Some good measurements for our final model(s)",
    "text": "Some good measurements for our final model(s)\n\nPearson residual statistic\n\n \n\nHosmer-Lemeshaw goodness-of-fit statistic\n\n \n\nAUC-ROC (area under the curve of the receiver operating characteristic)\n\n \n\nAIC/BIC"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#overview-22",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#overview-22",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Overview (2/2)",
    "text": "Overview (2/2)\n\nTo assess the fit of the model, it is good to have a mixture of measurements\n\n \n\nWe want to measure the absolute fit: not comparing to any models, but determining if the model fits the data well\n\nPearson residual statistic\nHosmer-Lemeshaw goodness-of-fit statistic\nAUC-ROC (kind of, often do not use a hypothesis test but you can!)\n\n\n \n\nWe want comparable measures of fit: if we have candidate models that are not nested\n\nAUC-ROC\nAIC/BIC"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#poll-everywhere-question-1",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#poll-everywhere-question-1",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#components-to-assess-model-fit",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#components-to-assess-model-fit",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Components to Assess Model Fit",
    "text": "Components to Assess Model Fit\n\nThe model fits the data well if\n\nSummary measures of the distance between the predicted/estimated/fitted and observed Y are small\n\nToday’s lecture!!\n\nThe contribution of each pair (predicted and observed) to these summary measures is unsystematic and is small relative to the error structure of the model\n\nModel Diagnostics that will be covered in another lecture!\n\n\n\n \n\nNeed both components\n\nIt is possible to see a “good” summary measure of the distance between predicted and observed Y with some substantial deviation from fit for a few subjects"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#summary-measures-of-goodness-of-fit",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#summary-measures-of-goodness-of-fit",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Summary Measures of Goodness of Fit",
    "text": "Summary Measures of Goodness of Fit\n\nAka overall measure of fit\n\n \n\nWhat do we need to calculate them?\n\nNeed to define what the fitted outcome is\nNeed to calculate how close the fitted outcome is to the observed outcome\nSummarize across all observations (or individuals’ data)\n\n\n \n\nTwo tests of goodness-of-fit\n\nPearson residual statistic\nHosmer-Lemeshaw goodness-of-fit statistic"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#comparing-fitted-outcome-to-observed-outcome",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#comparing-fitted-outcome-to-observed-outcome",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Comparing fitted outcome to observed outcome",
    "text": "Comparing fitted outcome to observed outcome\n\nIn logistic regression model, we estimate \\(\\pi(\\mathbf{X}) = P(Y=1|\\mathbf{X})\\)\n\nPredicted value, \\(\\widehat\\pi(\\mathbf{X})\\), is between 0 and 1 for each subject\n\nHowever, we always observe \\(Y=1\\) or \\(Y=0\\)\n\nNot an observed \\(\\pi(\\mathbf{X})\\)\n\n\n \n\nWe can deterimine the fitted outcome by sampling Y’s from a Bernoulli distribution with the fitted probability\n\n\\(\\widehat{Y} \\sim \\text{Bernoulli}(\\widehat\\pi(\\mathbf{X}))\\)\n\nIf there are groups of individuals that share the same covariate observations, then we can use the same \\(\\widehat\\pi(\\mathbf{X})\\)\n\n\\(\\sum_j \\widehat{Y} \\sim \\text{Binomial}(\\sum_j, \\widehat\\pi(\\mathbf{X}))\\)\n\n\n \n\nInstead of comparing the expected vs. observed at individual level, we can compare them at “group” level"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#number-of-covariate-patterns",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#number-of-covariate-patterns",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Number of Covariate Patterns",
    "text": "Number of Covariate Patterns\n\nWhen the logistic regression model contains only categorical covariates, we can think of the number of covariate patterns\nFor example: model contains two binary covariates (history of fracture and smoking status), there will be 4 unique combination of these factors\n\nThis model has 4 covariate patterns\nSubjects can be divided into 4 groups based on the covariates’ values\n\nWe can then compute the predicted number of individuals with Y=1 in each group, and compare that with the actual observed number of individuals with Y=1 in that group\n\nWe don’t need to sample this\nWe use the expected value (mean) of the Binomial to determine the \\(\\widehat{Y}\\) for each covariate pattern\nFor covariate pattern \\(j\\) with \\(m_j\\) observations: \\[\\widehat{Y}_j = m_j \\widehat\\pi(\\mathbf{X_j}) = m_j{\\hat{\\pi}}_j\\]"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#pearson-residual",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#pearson-residual",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Pearson Residual",
    "text": "Pearson Residual\n\nIn logistic regression model, can use Pearson residual for summary measure of goodness-of-fit Uses the \\(\\widehat{Y}_j\\) fitted value from previous slide\nPearson residual for jth covariate pattern is: \\[r\\left(Y_j,{\\hat{\\pi}}_j\\right)=\\frac{(Y_j-m_j{\\hat{\\pi}}_j)}{\\sqrt{m_j{\\hat{\\pi}}_j(1-{\\hat{\\pi}}_j)}}=\\frac{(Y_j-{\\hat{Y}}_i)}{\\sqrt{{\\hat{Y}}_i(1-{\\hat{\\pi}}_j)}}\\]\nThe summary statistics of Pearson residual is thus: \\[X^2=\\sum_{j=1}^{J}{r\\left(Y_j,{\\hat{\\pi}}_j\\right)^2}\\]"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#pearson-residual-procedure",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#pearson-residual-procedure",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Pearson Residual procedure",
    "text": "Pearson Residual procedure\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses: same for all data\n\n\\(H_0\\): model fits well\n\\(H_1\\): model does not fits well\n\nCalculate the test statistic and p-value\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#not-going-to-bother-going-through-an-example",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#not-going-to-bother-going-through-an-example",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Not going to bother going through an example",
    "text": "Not going to bother going through an example\n\nWe can calculate this by hand and test against a chi-squared distribution\n\n \n\nNo set R code to do this\n\n \n\nI do not see this as the main way to determine goodness of fit… for a binary outcome!\n\nOften because of the bigger issues with it…"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#issues-with-pearson-residuals",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#issues-with-pearson-residuals",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Issues with Pearson Residuals",
    "text": "Issues with Pearson Residuals\n\nAssume current model has p covariates…\n\nthen \\(X^2\\) (Pearson residual) follows a chi-squared distribution\n\nunder the null hypothesis based on large sample theory\n\nOnly appropriate if the number of covariate patterns is less than the number of observations\n\n\n \n\nWhen the logistic regression model contains one or more continuous covariates, it is likely that the number of covariate patterns equals to the sample size n\n\n \n\nWe should not use Pearson Residuals to evaluate goodness-of-fit test when the fitted model contains one or more continuous variables"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#hosmer-lemeshow-test",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#hosmer-lemeshow-test",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Hosmer-Lemeshow test",
    "text": "Hosmer-Lemeshow test\n\nIf number of covariate patterns is roughly same as the number of observations\n\nWhenever you include a continuous variable in your model\nHosmer-Lemeshow (HL) goodness-of-fit test should be used instead\n\n\n \n\nHowever, HL test does not work well if the number of covariate patterns is small\n\nHL test should not be used if the number of covariate patterns ≤ 6\n\nFor reference: 3 binary predictors makes 8 covariate patterns\n\nPearson residuals \\(X^2\\) should be used when the number of covariate patterns is small\n\n\n \n\nA large p-value from HL test suggests the model fits well"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#poll-everwhere-question-2",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#poll-everwhere-question-2",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Poll Everwhere question 2",
    "text": "Poll Everwhere question 2"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#hosmer-lemeshow-test-1",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#hosmer-lemeshow-test-1",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Hosmer-Lemeshow test",
    "text": "Hosmer-Lemeshow test\n\nHL test uses groupings from percentiles to basically measure what Pearson residual measures\n\n \n\nSteps to compute HL test statistic:\n\nCompute estimated probability \\(\\widehat\\pi(\\mathbf{X}))\\) for all n subjects (\\(n=1, 2, ..., n\\))\nOrder \\(\\widehat\\pi(\\mathbf{X}))\\) from largest to smallest values\nDivide ordered values into g percentile grouping (usually \\(g = 10\\) based on H-L’s suggestion)\nForm table of observed and expected counts\nCalculate HL test statistic from table\nCompare HL test statistic to chi-squared distribution (\\(\\chi^2_{g-2}\\))"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#hosmer-lemeshow-test-statistic",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#hosmer-lemeshow-test-statistic",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Hosmer-Lemeshow test statistic",
    "text": "Hosmer-Lemeshow test statistic\n\nThe test statistic of Hosmer-Lemeshow goodness-of-fit test is denoted by \\(\\widehat{C}\\), which is obtained by calculating the Pearson chi-squared statistic from the \\(g \\times 2\\) table of observed and estimated expected frequencies \\[\\hat{C}=\\sum_{k=1}^{g}\\frac{\\left(o_k-n_k^\\prime{\\bar{\\pi}}_k\\right)^2}{n_k^\\prime{\\bar{\\pi}}_k(1-{\\bar{\\pi}}_k)}\\]\n\nwhere \\(n'_k\\) is the total number of subjects in the \\(k\\)th group\n\nLet \\(c_k\\) be the number of covariate patterns in the \\(k\\)th decile: \\[o_k=\\sum_{j=1}^{c_k}y_j\\] and \\[{\\bar{\\pi}}_k=\\sum_{j=1}^{c_k}\\frac{m_j{\\hat{\\pi}}_j}{n_k^\\prime}\\]"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#hosmer-lemeshow-test-procedure",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#hosmer-lemeshow-test-procedure",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Hosmer-Lemeshow test procedure",
    "text": "Hosmer-Lemeshow test procedure\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses: same for all data\n\n\\(H_0\\): model fits well\n\\(H_1\\): model does not fits well\n\nCalculate the test statistic and p-value\n\nNote: \\(\\widehat{C} \\sim \\chi^2_{df=g-2}\\)\n\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#glow-study-hosmer-lemeshow-test",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#glow-study-hosmer-lemeshow-test",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "GLOW Study: Hosmer-Lemeshow test",
    "text": "GLOW Study: Hosmer-Lemeshow test\n\nOkay, so let’s look at the interaction model from last class \\[\\text{logit}\\left(\\pi(\\mathbf{X})\\right) = \\beta_0 + \\beta_1\\cdot I(\\text{PF}) +\\beta_2\\cdot Age + \\beta_3 \\cdot I(\\text{PF}) \\cdot Age\\]\nWe need to fit the model and use a new command:\n\n\nglow_m3 = glm(fracture ~ priorfrac + age_c + priorfrac*age_c, \n              data = glow, family = binomial)\nlibrary(ResourceSelection)\n\nResourceSelection 0.3-6      2023-06-27\n\nobs_vals = as.numeric(glow$fracture) -1\nfit_vals = fitted(glow_m3)\nhoslem.test(obs_vals, fit_vals, g = 10)\n\n\n    Hosmer and Lemeshow goodness of fit (GOF) test\n\ndata:  obs_vals, fit_vals\nX-squared = 6.778, df = 8, p-value = 0.5608\n\n\nNote to Nicky: do NOT make conclusion yet! In the poll everywhere!"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#poll-everywhere-question-3",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#poll-everywhere-question-3",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Poll Everywhere question 3",
    "text": "Poll Everywhere question 3"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#glow-study-hosmer-lemeshow-test-1",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#glow-study-hosmer-lemeshow-test-1",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "GLOW Study: Hosmer-Lemeshow test",
    "text": "GLOW Study: Hosmer-Lemeshow test\n\nConclusion: The p-value is 0.5608, so we fail to reject the null hypothesis that the model fits the data well. Thus, the preliminary final model for the GLOW dataset fits the data well\n\n \n\nDon’t forget that we still need to check individual observations (Model Diagnostics!)\n\n \n\nR may give results for the HL test even if it is not appropriate to use it!\n\nIf number of covariate patterns ≤ 6, do not use HL test"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#big-data-issue-in-goodness-of-fit-test",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#big-data-issue-in-goodness-of-fit-test",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Big Data Issue in Goodness-of-fit Test",
    "text": "Big Data Issue in Goodness-of-fit Test\n\nWhen the sample size is really big (&gt; 1000), it is much more likely to find the H-L reject the model fit (even when the expected vs. observed in each decile categories looks pretty similar)\n\n \n\nThis is due to “too much” power in hypothesis testing.\n\nPaul et al. (2012) for samples sizes from 1000 to 25,000, the number of groups g should be equal to \\[g=\\max{\\left(10,\\min{\\left\\{\\frac{n_1}{2},\\ \\frac{n-n_1}{2},\\ 2+8\\left(\\frac{n}{1000}\\right)^2\\right\\}}\\right)}\\]\n\n\n \n\nFor example, if one has a sample with \\(n=10, 000\\) (sample size) and \\(n_1=1,000\\) (number of events) then \\(g=500\\) groups are suggested\nFor n &gt; 25000, other methods, such as partitioning data into a developmental data set (with smaller n) and a validation set"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#final-notes-on-goodness-of-fit-test",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#final-notes-on-goodness-of-fit-test",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Final Notes on Goodness-of-fit Test",
    "text": "Final Notes on Goodness-of-fit Test\n\nThey should not be used for variable selection\n\nThe likelihood ratio tests for significance of coefficients are much more powerful and appropriate (when nested)\n\n\n \n\nThey are not for model comparison\n\nOne should not use the p-value from goodness of fit tests of different models to decide which model is better than the other\nSomething like AUC-ROC, AIC, or BIC can be used"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#roc-curve-and-auc-12",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#roc-curve-and-auc-12",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "ROC Curve and AUC (1/2)",
    "text": "ROC Curve and AUC (1/2)\n\n\n\nReceiver Operating Characteristics (ROC) curve is useful tool to quantify how good is our model predicting binary outcome\n\n \n\nIt is a plot of sensitivity (true positive rate) versus (1-specificity) or false positive rate of fitted binary values\n\nTrue Positive Rate \\(= \\dfrac{TP}{TP + FN}\\)\nFalse Positive Rate \\(=  \\dfrac{FP}{FP + TN}\\)\n\n\n \n\nThe ROC curve shows the tradeoff between sensitivity and specificity"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#roc-curve-and-auc-22",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#roc-curve-and-auc-22",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "ROC Curve and AUC (2/2)",
    "text": "ROC Curve and AUC (2/2)\n\n\n\nArea under the ROC curve (AUC ROC) is a reasonable summary of the overall predictive accuracy of the test\n\nAccuracy means how well the predicted value matches the observed value\n\n\n \n\nThe closer the curve follows the left-hand border and top border of the ROC space, the more accurate the test\n\nAn AUC =1 represents 100% accuracy\n\n\n \n\nThe closer the curve comes to the 45-degree diagonal line, the less accurate the test\n\nAn AUC = 0.5 represents an unhelpful model\n\nRandom predictions"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#poll-everywhere-question-4",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#poll-everywhere-question-4",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#roc-curve-and-auc-33",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#roc-curve-and-auc-33",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "ROC Curve and AUC (3/3)",
    "text": "ROC Curve and AUC (3/3)\n\nOften only report the AUC\n\n \n\nSuggestions of how to interpret model fit through AUC values:"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#glow-study-roc-of-interaction-model",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#glow-study-roc-of-interaction-model",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "GLOW Study: ROC of interaction model",
    "text": "GLOW Study: ROC of interaction model\n\n\n\nlibrary(pROC)\npredicted &lt;- predict(glow_m3, glow, type=\"response\")\n\n# define object to plot and calculate AUC\nrocobj &lt;- roc(glow$fracture, predicted)\nauc &lt;- round(auc(glow$fracture, predicted),4)\n\n#create ROC plot\nggroc(rocobj, colour = 'steelblue', \n      size = 2, legacy.axes = TRUE) +\n  ggtitle(paste0('ROC Curve ','(AUC = ',auc,')')) +\n  theme(text = element_text(size = 23)) +\n  xlab(\"False Positive Rate (1 - Specificity)\") +\n  ylab(\"True Positive Rate (Sensitivity)\")\n\n\n\n\nType 'citation(\"pROC\")' for a citation.\n\n\n\nAttaching package: 'pROC'\n\n\nThe following object is masked from 'package:epiDisplay':\n\n    ci\n\n\nThe following objects are masked from 'package:stats':\n\n    cov, smooth, var\n\n\nSetting levels: control = No, case = Yes\n\n\nSetting direction: controls &lt; cases\n\n\nSetting levels: control = No, case = Yes\n\n\nSetting direction: controls &lt; cases\n\n\n\n\n\n\n\n\n\n\n\nWe have a poorly fitting model\nWe can take auc and compare it to other models: good way to pick a model based on predictive power"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#another-way-to-think-about-auc",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#another-way-to-think-about-auc",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Another way to think about AUC",
    "text": "Another way to think about AUC\n\nGLOW Study: Consider the situation in which the fracture status of each individual is known\n\n \n\nRandomly pick one individual from fractured group and one from non-fractured outcome group\n\nBased on their age, height, prior fracture, and all other covariates, we will correctly predict which is from fractured group\n\n\n \n\nThe AUC is the percentage of randomly drawn pairs for which we predict the pair correctly\n\n \n\nTherefore, AUC represents the ability of our covariates to discriminate between individuals with the outcome (fracture) and those without the outcome"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#aic-and-bic",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#aic-and-bic",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "AIC and BIC",
    "text": "AIC and BIC\n\nTwo widely used non-hypothesis testing based measurements that helps select a good model\n\nAkaike Information Criterion (AIC)\nBayesian Information Criterion (BIC)\n\n\n \n\nUnlike likelihood ratio test which is only suitable for nested model, AIC and BIC are suitable for both nested and non-nested model\n\n \n\nThere is no hypothesis/conclusion testing for the comparison between two models\n\nSo not the best for selecting covariates to include in model\nBUT helpful if you have a few preliminary final models that you want to compare"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#poll-everywhere-question-5",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#poll-everywhere-question-5",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Poll Everywhere Question 5",
    "text": "Poll Everywhere Question 5"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#aic-and-bic-1",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#aic-and-bic-1",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "AIC and BIC",
    "text": "AIC and BIC\n\nBoth AIC and BIC penalize a model for having many parameters\n\n \n\n\n\n\n\n\n\n\nMeasure of fit\nEquation\nR code\n\n\n\n\nAkaike information criterion (AIC)\n\\(AIC = -2 \\cdot \\text{log-likelihood} + 2q\\)\nAIC(model_name)\n\n\nBayesian information criterion (BIC)\n\\(AIC = -2 \\cdot \\text{log-likelihood} + q\\text{log}(n)\\)\nBIC(model_name)\n\n\n\n \n\nWhere q is the number of parameters in the model and n is the sample size\nBoth AIC and BIC can only be used to compare models fitting the same data set\nIn comparing two models, the model with smaller AIC and/or BIC is preferred\n\nWhen the difference in AIC between two models exceeds 3, the difference is viewed as “meaningful”"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#aic-and-bic-in-r",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#aic-and-bic-in-r",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "AIC and BIC in R",
    "text": "AIC and BIC in R\n\nAfter fitting the logistic regression model, can calculate AIC and BIC\nLet’s look at the AIC and BIC of our interaction model:\n\n\nAIC(glow_m3)\n\n[1] 531.2716\n\nBIC(glow_m3)\n\n[1] 548.13"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#summary-12",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#summary-12",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Summary (1/2)",
    "text": "Summary (1/2)\n\n\n\n\n\n\n\n\n\nMeasure of fit\nHypothesis tested?\nEquation\nR code\n\n\n\n\nPearson residual\nYes\n\\(X^2=\\sum_{j=1}^{J}{r\\left(Y_j,{\\hat{\\pi}}_j\\right)^2}\\)\nNot given\n\n\nHosmer-Lemeshow test\nYes\n\\(\\hat{C}=\\sum_{k=1}^{g}\\frac{\\left(o_k-n_k^\\prime{\\bar{\\pi}}_k\\right)^2}{n_k^\\prime{\\bar{\\pi}}_k(1-{\\bar{\\pi}}_k)}\\)\nhoslem.test()\n\n\nAUC-ROC\nKinda\nNot given\nauc(observed, predicted)\n\n\nAIC\nOnly to compare models\n\\(AIC = -2 \\cdot \\text{log-likelihood} + 2q\\)\nAIC(model_name)\n\n\nBIC\nOnly to compare models\n\\(AIC = -2 \\cdot \\text{log-likelihood} + q\\text{log}(n)\\)\nBIC(model_name)\n\n\n\nSpecial notes:\n\nUse Hosmer-Lemshow test over Pearson residual unless number of covariate patterns is less than 6\nCannot use Pearson residual when there is a continuous variable in the model"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#summary-22",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#summary-22",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Summary (2/2)",
    "text": "Summary (2/2)\n\nFor our interaction model: \\[\\begin{aligned} \\text{logit}\\left(\\widehat\\pi(\\mathbf{X})\\right) & = \\widehat\\beta_0 &+ &\\widehat\\beta_1\\cdot I(\\text{PF}) & + &\\widehat\\beta_2\\cdot Age& + &\\widehat\\beta_3 \\cdot I(\\text{PF}) \\cdot Age \\\\  \n\\text{logit}\\left(\\widehat\\pi(\\mathbf{X})\\right) & = -1.376 &+ &1.002\\cdot I(\\text{PF})& + &0.063\\cdot Age& -&0.057 \\cdot I(\\text{PF}) \\cdot Age\n\\end{aligned}\\]\nWe can examine the overall model fit using:\n\nNot comparing to any other models:\n\nPearson residual: Not appropriate for this model\nHosmer-Lemeshow: \\(\\hat{C}=6.778\\), p-value = 0.56\nAUC-ROC: 0.6819\n\nCan be used to compare to other models:\n\nAUC-ROC: 0.6819\nAIC: 531.27\nBIC: 548.13"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#review-of-model-assessment-so-far-12",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#review-of-model-assessment-so-far-12",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Review of model assessment so far (1/2)",
    "text": "Review of model assessment so far (1/2)\n\nOverall measurements of fit\n\nHow well does the fitted logistic regression model predict the outcome?\nDifferent ways to measure the answer to this question\n\n\n\n\n\n\n\n\n\n\n\nMeasure of fit\nHypothesis tested?\nEquation\nR code\n\n\n\n\nPearson residual\nYes\n\\(X^2=\\sum_{j=1}^{J}{r\\left(Y_j,{\\hat{\\pi}}_j\\right)^2}\\)\nNot given\n\n\nHosmer-Lemeshow test\nYes\n\\(\\hat{C}=\\sum_{k=1}^{g}\\frac{\\left(o_k-n_k^\\prime{\\bar{\\pi}}_k\\right)^2}{n_k^\\prime{\\bar{\\pi}}_k(1-{\\bar{\\pi}}_k)}\\)\nhoslem.test()\n\n\nAUC-ROC\nKinda\nNot given\nauc(observed, predicted)\n\n\nAIC\nOnly to compare models\n\\(AIC = -2 \\cdot \\text{log-likelihood} + 2q\\)\nAIC(model_name)\n\n\nBIC\nOnly to compare models\n\\(BIC = -2 \\cdot \\text{log-likelihood} + q\\text{log}(n)\\)\nBIC(model_name)"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#review-of-model-assessment-so-far-22",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#review-of-model-assessment-so-far-22",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Review of model assessment so far (2/2)",
    "text": "Review of model assessment so far (2/2)\n\nNumerical problems\n\nAssess pre and post model fit\nNumerical problems often depend on the final model (which variables and interactions are included)\n\nDifferent numerical problems to look out for\n\nZero cell count\nComplete separation\nMulticollinearity"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#today",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#today",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Today",
    "text": "Today\n\nWe now use model diagnostics to identify any observations that the model does not fit well"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#review-of-number-of-covariate-patterns",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#review-of-number-of-covariate-patterns",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Review of Number of Covariate Patterns",
    "text": "Review of Number of Covariate Patterns\n\nCovariate patterns are the unique covariate combinations that are observed\n\n \n\nFor example: model contains two binary covariates (history of fracture and smoking status), there will be 4 unique combination of these factors\n\nThis model has 4 covariate patterns\nSubjects can be divided into 4 groups based on the covariates’ values\n\n\n \n\nWhen we have continuous covariates, the number of covariate patterns will be close to the number of individuals in the dataset"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#from-overall-measure-to-diagnostics",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#from-overall-measure-to-diagnostics",
    "title": "Lesson 14: Model Diagnostics",
    "section": "From overall measure to diagnostics",
    "text": "From overall measure to diagnostics\n\nNow we need to investigate diagnostics looking at individual data or covariate pattern data\n\nMake sure the overall measure has not been influenced by certain observations\n\n\n \n\nThe key quantities from logistic regression diagnostics are the components of “residual sum-of-squares”\n\nThe same idea as in the linear regression\nAssessed for each covariate pattern \\(j\\), by computing standardized Pearson residuals and Deviance residuals\n\nStandardization using \\(h_j\\), the leverage values"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#hat-matrix-and-leverage-values-linear-regression",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#hat-matrix-and-leverage-values-linear-regression",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Hat Matrix and Leverage Values: Linear regression",
    "text": "Hat Matrix and Leverage Values: Linear regression\n\nWe have learned “hat” matrix and leverage values from linear regression diagnostics\n\n \n\nIn linear regression, the hat matrix projects the outcome variable onto the covariate space:\n \n\n\\(H=X\\left(X^\\prime X\\right)^{-1}X^\\prime\\) and \\(\\hat{y}=Hy\\)\n\n \n\nThe linear regression residuals is thus \\(y - \\widehat{y}\\), or \\((I-H)y\\)\n\n\n \n\nThe leverage is just the diagonal elements of the hat matrix, which is proportional to the distance of \\(x_j\\) to the mean of the data \\(\\overline{x}\\)"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#hat-matrix-and-leverage-values-logistic-regression",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#hat-matrix-and-leverage-values-logistic-regression",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Hat Matrix and Leverage Values: Logistic regression",
    "text": "Hat Matrix and Leverage Values: Logistic regression\n\nIn logistic regression model, the hat matrix is: \\[H=V^\\frac{1}{2}X\\left(X^\\prime V\\ X\\right)^{-1}X^\\prime V^\\frac{1}{2}\\]\nThe leverage is \\[h_j=m_j\\cdot\\hat{\\pi}\\left(\\textbf{x}_j\\right)\\left[1-\\hat{\\pi}\\left(\\textbf{x}_j\\right)\\right]\\textbf{x}_j^\\prime\\left(\\textbf{X}^\\prime\\textbf{VX}\\right)^{-1}\\textbf{x}_j=v_j\\cdot b_j\\]\n\n\\(b\\): weighted distance of \\(x_j\\) from \\(\\overline{x}\\)\n\\(v_j\\): model based estimator of the variance of \\(y_j\\)\n\n\\(v_j=m_j\\cdot\\hat{\\pi}\\left(\\textbf{x}_j\\right)\\left[1-\\hat{\\pi}\\left(\\textbf{x}_j\\right)\\right]\\)\n\n\n\\(h_j\\) reflects the relative influence of each covariate pattern on the model’s fit"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#poll-everywhere-question-1",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#poll-everywhere-question-1",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#poll-everywhere-question-2",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#poll-everywhere-question-2",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#diagnostic-statistics-computation-12",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#diagnostic-statistics-computation-12",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Diagnostic Statistics Computation (1/2)",
    "text": "Diagnostic Statistics Computation (1/2)\n\nTwo diagnostic statistics computation approach\n\nApproach 1: computed by covariate pattern\n\nRecommendation of Hosmer-Lemeshow textbook\nR uses this approach\nIdentify outliers as group that shares the same covariate values (in the same covariate pattern)\n\nApproach 2: individual observation approach\n\nSAS uses this approach\nIdentify outliers as individual\n\n\nWhy prefer covariate patterns approach?\n\nWhen the number of covariate pattern is much smaller than n, there is risk that we may fail to identify influential and/or poorly fit covariate patterns using individual based on residual"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#diagnostic-statistics-computation-22",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#diagnostic-statistics-computation-22",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Diagnostic Statistics Computation (2/2)",
    "text": "Diagnostic Statistics Computation (2/2)\nConsider a covariate pattern with \\(m_j\\) subjects, all did not have event (some \\(y_i = 0\\)). So the estimated logistic probability is \\(\\widehat\\pi_j\\)\n\nPearson residual computed by individual \\[r_i=-\\sqrt{\\frac{{\\hat{\\pi}}_j}{(1-{\\hat{\\pi}}_j)}}\\]\nPearson residual computed by covariate pattern \\[r_i=-\\sqrt{m_j}\\sqrt{\\frac{{\\hat{\\pi}}_j}{(1-{\\hat{\\pi}}_j)}}\\]\nDifference between aboveresiduals will be large if \\(m_j\\) is large: usually a problem if less covariate patterns\n\nResidual from covariate pattern will identify poorly fit covariate patterns"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#diagnostics-of-logistic-regression",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#diagnostics-of-logistic-regression",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Diagnostics of Logistic Regression",
    "text": "Diagnostics of Logistic Regression\n\nModel diagnostics of logistic regression can be assessed by checking how influential a covariate pattern is:\n \n\nLook at change in residuals if a covariate pattern is excluded\n\nStandardized Pearson residual\nStandardized Deviance residual\n\n\n \n\nLook at change in coefficients if a covariate pattern is excluded"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#change-of-standardized-residuals",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#change-of-standardized-residuals",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Change of Standardized Residuals",
    "text": "Change of Standardized Residuals\n\nChange in standardized Pearson Chi-square statistic due to deletion of subjects with covariate pattern \\(x_j\\): \\[\\Delta X_j^2 = r_{sj}^2 = \\dfrac{r_j^2}{1-h_j}\\]\nDon’t need to know this: change in standardized deviance statistic due to deletion of subjects with covariate pattern \\(x_j\\): \\[\\Delta D_j = \\dfrac{d_j^2}{1-h_j}\\]\nRefer to Lesson 12: Assessing Model Fit for expression of Pearson residual"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#change-of-estimated-coefficients",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#change-of-estimated-coefficients",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Change of Estimated Coefficients",
    "text": "Change of Estimated Coefficients\n\nChange in estimated coefficients due to deletion of subjects with covariate pattern \\(x_j\\): \\[\\Delta \\widehat{\\beta}_j = \\dfrac{r_j^2 h_j}{(1-h_j)^2}\\]\n\n \n\nThis is the logistic regression analog of Cook’s influence statistic (in linear regression)"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#visual-assessment-for-diagnostics-of-logistic-regression-i",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#visual-assessment-for-diagnostics-of-logistic-regression-i",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Visual Assessment for Diagnostics of Logistic Regression (I)",
    "text": "Visual Assessment for Diagnostics of Logistic Regression (I)\n\nIn logistic regression, we mainly rely on graphical methods\n\nBecause the distribution of diagnostic measures under null hypothesis (that the model fits) is only known in certain limited settings\n\n\n \n\nFour plots for analysis of diagnostics in logistic regression:\n\n\\(\\Delta X_j^2\\) vs. \\({\\hat{\\pi}}_j\\)\n\\(\\Delta D_j\\) vs. \\({\\hat{\\pi}}_j\\)\n\\(\\Delta\\widehat{\\beta}_j\\) vs. \\({\\hat{\\pi}}_j\\)\n\\(h_j\\) vs. \\({\\hat{\\pi}}_j\\)"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#recall-the-model-we-fit-glow-study-with-interactions",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#recall-the-model-we-fit-glow-study-with-interactions",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Recall the model we fit: GLOW Study with interactions",
    "text": "Recall the model we fit: GLOW Study with interactions\n\nOutcome variable: any fracture in the first year of follow up (FRACTURE: 0 or 1)\nRisk factor/variable of interest: history of prior fracture (PRIORFRAC: 0 or 1)\nPotential confounder or effect modifier: age (AGE, a continuous variable)\nFitted model with interactions: \\[\\begin{aligned} \\text{logit}\\left(\\widehat\\pi(\\mathbf{X})\\right) & = \\widehat\\beta_0 &+ &\\widehat\\beta_1\\cdot I(\\text{PF}) & + &\\widehat\\beta_2\\cdot Age& + &\\widehat\\beta_3 \\cdot I(\\text{PF}) \\cdot Age \\\\  \n\\text{logit}\\left(\\widehat\\pi(\\mathbf{X})\\right) & = -1.376 &+ &1.002\\cdot I(\\text{PF})& + &0.063\\cdot Age& -&0.057 \\cdot I(\\text{PF}) \\cdot Age\n\\end{aligned}\\]\n\n \n\nLesson 12: determined the overall fit of this model\nToday: determine the if any observations/covariate patterns that model does not fit well"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#how-do-we-get-these-values-in-r",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#how-do-we-get-these-values-in-r",
    "title": "Lesson 14: Model Diagnostics",
    "section": "How do we get these values in R?",
    "text": "How do we get these values in R?\n\nNice function in the R script Logistic_Dx_Functions.R\n\nHighly suggest you save this R script for future use!!\n\n\n\nsource(here(\"lectures\", \"14_Model_diagnostics\", \"Logistic_Dx_Functions.R\"))\ndx_glow = dx(glow_m3)\nglimpse(dx_glow)\n\nRows: 71\nColumns: 16\n$ `(Intercept)`        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ priorfracYes         &lt;dbl&gt; 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0…\n$ age_c                &lt;dbl&gt; 1, -7, 7, -2, 10, 20, 1, -2, 2, 8, 18, -8, 11, 10…\n$ `priorfracYes:age_c` &lt;dbl&gt; 1, 0, 7, 0, 10, 0, 0, -2, 2, 0, 0, 0, 0, 0, -3, 0…\n$ y                    &lt;dbl&gt; 2, 2, 3, 2, 2, 1, 3, 3, 1, 5, 1, 3, 2, 1, 1, 4, 1…\n$ P                    &lt;dbl&gt; 0.4088354, 0.1402159, 0.4162991, 0.1822879, 0.420…\n$ n                    &lt;int&gt; 5, 15, 7, 10, 5, 2, 12, 8, 3, 15, 2, 18, 7, 4, 3,…\n$ yhat                 &lt;dbl&gt; 2.0441770, 2.1032389, 2.9140936, 1.8228786, 2.100…\n$ Pr                   &lt;dbl&gt; -0.04018670, -0.07677228, 0.06586860, 0.14507476,…\n$ dr                   &lt;dbl&gt; -0.04023255, -0.07730975, 0.06577949, 0.14332786,…\n$ h                    &lt;dbl&gt; 0.008844090, 0.003811004, 0.008725450, 0.00290085…\n$ sPr                  &lt;dbl&gt; -0.04036559, -0.07691899, 0.06615786, 0.14528564,…\n$ sdr                  &lt;dbl&gt; -0.04041165, -0.07745749, 0.06606836, 0.14353620,…\n$ dChisq               &lt;dbl&gt; 0.001629381, 0.005916530, 0.004376863, 0.02110791…\n$ dDev                 &lt;dbl&gt; 0.001633102, 0.005999662, 0.004365028, 0.02060264…\n$ dBhat                &lt;dbl&gt; 1.453897e-05, 2.263418e-05, 3.852626e-05, 6.14091…"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#key-to-the-values",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#key-to-the-values",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Key to the values",
    "text": "Key to the values\n\n\n\ncolnames(dx_glow)\n\n [1] \"(Intercept)\"        \"priorfracYes\"       \"age_c\"             \n [4] \"priorfracYes:age_c\" \"y\"                  \"P\"                 \n [7] \"n\"                  \"yhat\"               \"Pr\"                \n[10] \"dr\"                 \"h\"                  \"sPr\"               \n[13] \"sdr\"                \"dChisq\"             \"dDev\"              \n[16] \"dBhat\"             \n\n\n\nFor each covariate pattern (which is each row) …\n\ny: Number of events\nP: Estimated probability of events\nn: Number of observations\nyhat: Estimated number of events\nPr: Pearson residual\ndr: Deviance\nh: leverage\nsPr: Standardized Pearson residual\nsdr: Standardized deviance\ndChisq: Change in standardized Pearson residual\ndDev: Change in standardized deviance\ndBhat: Change in coefficient estimates"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#poll-everywhere-question-3",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#poll-everywhere-question-3",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#visual-assessment-for-diagnostics-of-logistic-regression",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#visual-assessment-for-diagnostics-of-logistic-regression",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Visual Assessment for Diagnostics of Logistic Regression",
    "text": "Visual Assessment for Diagnostics of Logistic Regression\n\nThe plots allow us to identify those covariate patterns that are…\n\nPoorly fit\n\nLarge values of \\(\\Delta X_j^2\\) (and/or \\(\\Delta D_j\\) if we looked at those)\n\nInfluential on estimated coefficients\n\nLarge values of \\(\\Delta\\widehat{\\beta}_j\\)\n\n\nIf you are interested to look at the contribution of leverage (ℎ_𝑗) to the values of the diagnostic statistic, you may also look at plots of:\n\n\\(\\Delta X_j^2\\) vs. \\({\\hat{\\pi}}_j\\)\n\\(\\Delta D_j\\) vs. \\({\\hat{\\pi}}_j\\)\n\\(\\Delta\\widehat{\\beta}_j\\) vs. \\({\\hat{\\pi}}_j\\)"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-standardized-pearson-residuals",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-standardized-pearson-residuals",
    "title": "Lesson 14: Model Diagnostics",
    "section": "GLOW study: standardized Pearson residuals",
    "text": "GLOW study: standardized Pearson residuals\n\n\n\nGenerally, the points that curve from top left to bottom right of plot correspond to covariate patterns with \\(y_j = 1\\)\n\nOpposite corresponds to \\(y_j = 0\\)\n\n\n\n\n\nTo make the plot\nggplot(dx_glow) + geom_point(aes(x=P, y=dChisq), size = 3) + \n  xlab(\"Estimated/Predicted Probability of Fracture\") +\n  ylab(\"Change in Std. Pearson Residual\") +\n  theme(text = element_text(size = 26))"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-standardized-pearson-residuals-1",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-standardized-pearson-residuals-1",
    "title": "Lesson 14: Model Diagnostics",
    "section": "GLOW study: standardized Pearson residuals",
    "text": "GLOW study: standardized Pearson residuals\n\n\n\nGenerally, the points that curve from top left to bottom right of plot correspond to covariate patterns with \\(y_j = 1\\)\n\nOpposite corresponds to \\(y_j = 0\\)\n\nPoints in the top left or top right corners identify the covariate patterns that are poorly fit\nWe may use 4 as a crude approximation to the upper 95th percentile for \\(\\Delta X_j^2\\)\n\n95th percentile of chi-squared distribution is 3.84\n\n\n\n\n\nTo make the plot\nggplot(dx_glow) + geom_point(aes(x=P, y=dChisq), size = 3) + \n  xlab(\"Estimated/Predicted Probability of Fracture\") +\n  ylab(\"Change in Std. Pearson Residual\") +\n  theme(text = element_text(size = 26))"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-standardized-pearson-residuals-2",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-standardized-pearson-residuals-2",
    "title": "Lesson 14: Model Diagnostics",
    "section": "GLOW study: standardized Pearson residuals",
    "text": "GLOW study: standardized Pearson residuals\n\n\n\nGenerally, the points that curve from top left to bottom right of plot correspond to covariate patterns with \\(y_j = 1\\)\n\nOpposite corresponds to \\(y_j = 0\\)\n\nPoints in the top left or top right corners identify the covariate patterns that are poorly fit\nWe may use 4 as a crude approximation to the upper 95th percentile for \\(\\Delta X_j^2\\)\n\n95th percentile of chi-squared distribution is 3.84\n\nWhich point is over 4?\n\n\ndx_glow %&gt;% filter(dChisq &gt; 4) %&gt;% select(priorfracYes, age_c, P, dChisq)\n\n   priorfracYes age_c         P   dChisq\n          &lt;num&gt; &lt;num&gt;     &lt;num&gt;    &lt;num&gt;\n1:            0    -4 0.1643855 4.413937\n\n\n\n\n\nTo make the plot\nggplot(dx_glow) + geom_point(aes(x=P, y=dChisq), size = 3) + \n  xlab(\"Estimated/Predicted Probability of Fracture\") +\n  ylab(\"Change in Std. Pearson Residual\") +\n  theme(text = element_text(size = 26))"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-standardized-deviance-residuals",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-standardized-deviance-residuals",
    "title": "Lesson 14: Model Diagnostics",
    "section": "GLOW study: standardized Deviance residuals",
    "text": "GLOW study: standardized Deviance residuals\n\n\n\nSame investigation as Pearson residuals\nPoints in the top left or top right corners identify the covariate patterns that are poorly fit\nUse 4 as a crude approximation to the upper 95th percentile\nWhich point is over 4?\n\n\ndx_glow %&gt;% filter(dDev &gt; 4) %&gt;% \n  select(priorfracYes, age_c, P, dDev)\n\n   priorfracYes age_c         P     dDev\n          &lt;num&gt; &lt;num&gt;     &lt;num&gt;    &lt;num&gt;\n1:            0   -10 0.1190935 4.841217\n2:            0     7 0.2812460 5.313540\n3:            1     6 0.4150524 4.325664\n\n\n\n\n\nTo make the plot\nggplot(dx_glow) + geom_point(aes(x=P, y=dDev), size = 3) + \n  xlab(\"Estimated/Predicted Probability of Fracture\") +\n  ylab(\"Change in Std. Deviance Residual\") +\n  theme(text = element_text(size = 26))"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-change-in-coefficient-estimates",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-change-in-coefficient-estimates",
    "title": "Lesson 14: Model Diagnostics",
    "section": "GLOW Study: Change in coefficient estimates",
    "text": "GLOW Study: Change in coefficient estimates\n\n\n\nBook recommends flagging certain covariate patterns if change in coefficient estimates are greater than 1\nAll values of \\(\\Delta\\widehat{\\beta}_j\\) are below 0.09\n\n\ndx_glow %&gt;% filter(dBhat &gt; 0.075) %&gt;% \n  select(priorfracYes, age_c, P, dBhat)\n\n   priorfracYes age_c         P      dBhat\n          &lt;num&gt; &lt;num&gt;     &lt;num&gt;      &lt;num&gt;\n1:            1    20 0.4325984 0.08926472\n\n\n\n\n\nTo make the plot\nggplot(dx_glow) + geom_point(aes(x=P, y=dBhat), size = 3) + \n  xlab(\"Estimated/Predicted Probability of Fracture\") +\n  ylab(\"Change in Coefficient Estimates\") +\n  theme(text = element_text(size = 26))"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-leverage",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-leverage",
    "title": "Lesson 14: Model Diagnostics",
    "section": "GLOW Study: Leverage",
    "text": "GLOW Study: Leverage\n\n\n\nWe can use the same rule as linear regression: \\(h_j &gt; 3p/n\\)\n\nFlag these points as high leverage\n\nPoints with high leverage\n\n\\(p=4\\): four regression coefficients\n\\(n=500\\): 500 total observations\nLook for \\(h_j &gt; 3p/n = 3\\cdot4 /500 = 0.024\\)\n\n\n\ndx_glow %&gt;% filter(h &gt; 3*4/500) %&gt;% \n  select(priorfracYes, age_c, P, h) %&gt;% \n  head()\n\n   priorfracYes age_c         P          h\n          &lt;num&gt; &lt;num&gt;     &lt;num&gt;      &lt;num&gt;\n1:            0    20 0.4686423 0.02688958\n2:            1   -12 0.3928116 0.03186122\n3:            0    19 0.4531105 0.02451738\n4:            1   -11 0.3940365 0.02900675\n5:            1    19 0.4313389 0.02895824\n6:            1    18 0.4300804 0.02621708\n\n\n\n\n\nTo make the plot\nggplot(dx_glow) + geom_point(aes(x=P, y=h), size=3) + \n  xlab(\"Estimated/Predicted Probability of Fracture\") +\n  ylab(\"Leverage\") +\n  theme(text = element_text(size = 26))"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#find-out-the-influential-observation-from-the-data-set",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#find-out-the-influential-observation-from-the-data-set",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Find Out the “Influential” Observation From the Data Set",
    "text": "Find Out the “Influential” Observation From the Data Set\n\n\n\nWe identified covariate patterns that may be poorly fit or influential\n\n \n\nLet’s identify the covariate patterns that were not fit well\n\n\n\ndx_glow %&gt;% mutate(Cov_patt = 1:nrow(.)) %&gt;%\n  filter(dChisq &gt; 4 | dDev &gt; 4 | dBhat &gt; 1 | \n          h &gt; 3*4/500) %&gt;%\n  select(Cov_patt, y, P, h, dChisq, dDev, dBhat, h) %&gt;%\n  round(., 3)\n\n    Cov_patt     y     P     h dChisq  dDev dBhat\n       &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;  &lt;num&gt; &lt;num&gt; &lt;num&gt;\n 1:        6     1 0.469 0.027  0.008 0.008 0.000\n 2:       22     1 0.393 0.032  0.046 0.047 0.002\n 3:       36     1 0.453 0.025  0.178 0.183 0.004\n 4:       43     0 0.119 0.005  2.581 4.841 0.012\n 5:       45     6 0.164 0.003  4.414 3.554 0.014\n 6:       47     0 0.281 0.006  3.148 5.314 0.018\n 7:       48     0 0.394 0.029  0.670 1.032 0.020\n 8:       49     2 0.431 0.029  0.698 0.693 0.021\n 9:       50     0 0.430 0.026  0.775 1.155 0.021\n10:       53     0 0.415 0.008  2.862 4.326 0.024\n11:       57     2 0.395 0.026  0.949 0.924 0.026\n12:       63     0 0.484 0.029  0.967 1.364 0.029\n13:       69     0 0.434 0.035  1.588 2.358 0.058\n14:       70     1 0.392 0.035  1.610 1.943 0.058\n15:       71     2 0.433 0.032  2.710 3.462 0.089"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#after-identifying-points",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#after-identifying-points",
    "title": "Lesson 14: Model Diagnostics",
    "section": "After identifying points",
    "text": "After identifying points\n\nDo a data quality check\n\nUnless you have a very good reason to believe the data are not measured correctly, then we leave it in\nCommon to do nothing\n\n\n \n\nIf only a few covariate pattern does not fit well (\\(y_j\\) differs from \\(m_j\\widehat\\pi_j\\) ), we are not too worried\n\nWe had 15 out of 71 covariate patterns\n\n\n \n\nIf quite a few covariate patterns do not fit well, potential reasons can be considered:\n\nThe link used in logistic regression model is not appropriate for outcome\n\nThis is usually unlikely, since logistic regression model is very flexible (think back to why we transformed our outcome from binary form)\n\nOne or more important covariates missing in the model\n\nAt least one of the covariates in the model has been entered in the wrong scale (think age-squared vs. age)"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#how-would-i-report-this-combining-all-model-assessment",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#how-would-i-report-this-combining-all-model-assessment",
    "title": "Lesson 14: Model Diagnostics",
    "section": "How would I report this? (Combining all model assessment)",
    "text": "How would I report this? (Combining all model assessment)\n\nAssuming I have not checked other final models (no other models to compare AIC/BIC or AUC with)\n\nMethods: To assess the overall model fit, we calculated the AUC-ROC. We also calculated several model diagnostics including standardized Pearson residual, standardized deviance, change in coefficient estimates, and leverage. We identified covariate patterns with high standardized Pearson residual (greater than 4), standardized deviance (greater than 4), change in coefficient estimates (greater than 1), and leverage (greater than 0.024).\n \nResults: Our final logistic regression model consisted of the outcome, fracture, and predictors including prior fracture, age, and their interaction. The AUC-ROC was 0.68. We identified 11 covariate patterns with high leverage and 4 with high standardized Pearson residual, standardized deviance, or change in coefficient estimates. No identified observations were omitted.\n \nDiscussion:\n\nAUC-ROC low: Included covariates were pre-determined\nInfluential points were kept in because all observations were within feasible range of the predictors and outcome. (we could try age-sqaured and see if that helps AUC and/or diagnostics)"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#last-class",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#last-class",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Last class",
    "text": "Last class\n\nLooked at simple logistic regression for binary outcome with\n\nOne continuous predictor \\[\\text{logit}(\\pi(X)) = \\beta_0 + \\beta_1 \\cdot X\\]\nOne binary predictor \\[\\text{logit}\\left(\\pi(X) \\right) = \\beta_0 + \\beta_1 \\cdot I(X=1)\\]\nOne multi-level predictor \\[\\text{logit}\\left(\\pi(X) \\right) = \\beta_0 + \\beta_1 \\cdot I(X=b) + \\beta_2 \\cdot I(X=c) + \\beta_3 \\cdot I(X=d)\\]"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#breast-cancer-example",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#breast-cancer-example",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Breast Cancer example",
    "text": "Breast Cancer example\n\nFor breast cancer diagnosis example, recall:\n\nOutcome: early or late stage breast cancer diagnosis (binary, categorical)\n\n\n \n\nPrimary covariate: Race/ethnicity\n\nNon-Hispanic white individuals are more likely to be diagnosed with breast cancer\n\nBut POC are more likely to be diagnosed at a later stage\n\n\n\n \n\nAdditional covariate: Age\n\nRisk factor for cancer diagnosis\n\n\n \n\nWe want to fit a multiple logistic regression model with both risk factors included as independent variables"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#introduction-to-multiple-logistic-regression",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#introduction-to-multiple-logistic-regression",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Introduction to Multiple Logistic Regression",
    "text": "Introduction to Multiple Logistic Regression\n\nIn multiple logistic regression model, we have &gt; 1 independent variable\n\nSometimes referred to as the “multivariable regression”\nThe independent variable can be any type:\n\nContinuous\nCategorical (ordinal or nominal)\n\n\n\n \n\nWe will follow similar procedures as we did for simple logistic regression\n\nBut we need to change our interpretation of estimates because we are adjusting for other variables"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#multiple-logistic-regression-model",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#multiple-logistic-regression-model",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Multiple Logistic Regression Model",
    "text": "Multiple Logistic Regression Model\n\n\n\nAssume we have a collection of \\(k\\) independent variables, denoted by \\(\\mathbf{X}=\\left( X_1, X_2, ..., X_k \\right)\\)\n\n \n\nThe conditional probability is \\(P(Y=1 | \\mathbf{X}) = \\pi(\\mathbf{X})\\)\n\n \n\nWe then model the probability with logistic regression: \\[\\text{logit}\\left(\\pi(\\mathbf{X})\\right) = \\beta_0\n+\\beta_1 \\cdot X_1 +\\beta_2 \\cdot X_2  +\\beta_3 \\cdot X_3 + ... + \\beta_k \\cdot X_k\\]\n\n\n\n\n\n\nWhy the bold \\(X\\)?\n\n\n\\(\\mathbf{X}\\) represents the vector of all the \\(X\\)’s. This is how we represent our group of covariates in our model."
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#fitting-the-multiple-logistic-regression-model",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#fitting-the-multiple-logistic-regression-model",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Fitting the Multiple Logistic Regression Model",
    "text": "Fitting the Multiple Logistic Regression Model\n\nFor a multiple logistic regression model with \\(k\\) independent variables, the vector of coefficients can be denoted by \\[\\boldsymbol{\\beta}^{T} = \\left(\\beta_0, \\beta_1, \\beta_2, ..., \\beta_k \\right)\\]\n\n \n\nAs with the simple logistic regression, we use maximum likelihood method for estimating coefficients\n\nVector of estimated coefficients: \\[\\widehat{\\boldsymbol{\\beta}}^{T} = \\left(\\widehat{\\beta}_0, \\widehat{\\beta}_1, \\widehat{\\beta}_2, ..., \\widehat{\\beta}_k \\right)\\]\n\n\n \n\nFor a model with \\(k\\) independent variables, there is \\(k+1\\) coefficients to estimate\n\nUnless one of those independent variables is a multi-level categorical variables, then we need more than \\(k+1\\) coefficients"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#breast-cancer-example-population-model",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#breast-cancer-example-population-model",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Breast Cancer Example: Population Model",
    "text": "Breast Cancer Example: Population Model\n\nWe can fit a logistic regression model with both race and ethnicity and age:\n\n\\[ \\begin{aligned}\n\\text{logit}\\left(\\pi(\\mathbf{X})\\right) = & \\beta_0\n+\\beta_1 \\cdot I \\left( R/E = H/L \\right)\n+\\beta_2 \\cdot I \\left( R/E = NH AIAN \\right) \\\\\n& +\\beta_3 \\cdot I \\left( R/E = NH API \\right)\n+\\beta_4 \\cdot I \\left( R/E = NH B \\right)\n+\\beta_5 \\cdot Age\n\\end{aligned}\\]\n \n \n\nNote that race and ethnicity requires 4 coefficients to include the indicator for each category\n\n \n\nCan replace \\(\\pi(\\mathbf{X})\\) with \\(\\pi({\\text{Race/ethnicity, Age}})\\)\n\n \n\n6 total coefficients (\\(\\beta_0\\) to \\(\\beta_5\\))"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#fitting-multiple-logistic-regression-model",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#fitting-multiple-logistic-regression-model",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Fitting Multiple Logistic Regression Model",
    "text": "Fitting Multiple Logistic Regression Model\n\nmulti_bc = glm(Late_stage_diag ~ Race_Ethnicity + Age_c, data = bc, family = binomial)\nsummary(multi_bc)\n\n\nCall:\nglm(formula = Late_stage_diag ~ Race_Ethnicity + Age_c, family = binomial, \n    data = bc)\n\nCoefficients:\n                                                 Estimate Std. Error z value\n(Intercept)                                     -1.038389   0.027292 -38.048\nRace_EthnicityHispanic-Latino                   -0.015424   0.083653  -0.184\nRace_EthnicityNH American Indian/Alaskan Native -0.085704   0.484110  -0.177\nRace_EthnicityNH Asian/Pacific Islander          0.133965   0.083797   1.599\nRace_EthnicityNH Black                           0.357692   0.071789   4.983\nAge_c                                            0.057151   0.003209  17.811\n                                                Pr(&gt;|z|)    \n(Intercept)                                      &lt; 2e-16 ***\nRace_EthnicityHispanic-Latino                      0.854    \nRace_EthnicityNH American Indian/Alaskan Native    0.859    \nRace_EthnicityNH Asian/Pacific Islander            0.110    \nRace_EthnicityNH Black                          6.27e-07 ***\nAge_c                                            &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 11861  on 9999  degrees of freedom\nResidual deviance: 11484  on 9994  degrees of freedom\nAIC: 11496\n\nNumber of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#breast-cancer-example-fitted-model",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#breast-cancer-example-fitted-model",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Breast Cancer Example: Fitted Model",
    "text": "Breast Cancer Example: Fitted Model\n\nWe now have the fitted logistic regression model with both race and ethnicity and age:\n\n\\[ \\begin{aligned}\n\\text{logit}\\left(\\widehat{\\pi}(\\mathbf{X})\\right) = & \\widehat{\\beta}_0\n+ \\widehat{\\beta}_1 \\cdot I \\left( R/E = H/L \\right)\n+ \\widehat{\\beta}_2 \\cdot I \\left( R/E = NH AIAN \\right) \\\\\n& + \\widehat{\\beta}_3 \\cdot I \\left( R/E = NH API \\right)\n+ \\widehat{\\beta}_4 \\cdot I \\left( R/E = NH B \\right)\n+ \\widehat{\\beta}_5 \\cdot Age \\\\\n\\\\\n\\text{logit}\\left(\\widehat{\\pi}(\\mathbf{X})\\right) = &-4.56\n-0.02 \\cdot I \\left( R/E = H/L \\right)\n-0.09 \\cdot I \\left( R/E = NH AIAN \\right) \\\\\n& +0.13 \\cdot I \\left( R/E = NH API \\right)\n+0.36 \\cdot I \\left( R/E = NH B \\right)\n+0.06 \\cdot Age\n\\end{aligned}\\]\n\n6 total coefficients (\\(\\widehat{\\beta}_0\\) to \\(\\widehat{\\beta}_5\\))"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#testing-significance-of-the-coefficients",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#testing-significance-of-the-coefficients",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Testing Significance of the Coefficients",
    "text": "Testing Significance of the Coefficients\n\nRefer to Lesson 6 for more information on each test!!\n\n \n\nWe use the same three tests that we discussed in Simple Logistic Regression to test individual coefficients\n\nWald test\n\nCan be used to test a single coefficient\n\nScore test\nLikelihood ratio test (LRT)\n\nCan be used to test a single coefficient or multiple coefficients\n\n\n\n \n\nTextbook and our class focuses on Wald and LRT only"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#a-note-on-wording",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#a-note-on-wording",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "A note on wording",
    "text": "A note on wording\n\nWhen I say “test a single coefficient” or “test multiple coefficients” I am referring to the \\(\\beta\\)’s\n\nA single variable can have a single coefficient\n\nExample: testing age\n\nA single variable can have multiple coefficients\n\nExample: testing race and ethnicity\n\nMuliple variables will have multiple coefficients\n\nExample: testing age and race and ethnicity together\n\n\n \nWhen I say “test a variable” I mean “determine if the model with the variable is more likely than the model without that variable”\n\nWe can use the Wald test to do this is some scenarios (single, continuous covariate)\nBUT I advise you practice using the LRT whenever comparing models (aka testing variables)"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#all-three-tests-together",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#all-three-tests-together",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "All three tests together",
    "text": "All three tests together"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#from-lesson-6-wald-test",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#from-lesson-6-wald-test",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "From Lesson 6: Wald test",
    "text": "From Lesson 6: Wald test\n\nAssumes test statistic W follows a standard normal distribution under the null hypothesis\nTest statistic: \\[W=\\frac{{\\hat{\\beta}}_j}{SE_{\\hat{\\beta}_j}}\\sim N(0,1)\\]\n\nwhere \\(\\widehat{\\beta}_j\\) is a MLE of coefficient \\(j\\)\n\n95% Wald confidence interval: \\[{\\hat{\\beta}}_1\\pm1.96 \\cdot SE_{{\\hat{\\beta}}_j}\\]\nThe Wald test is a routine output in R (summary() of glm() output)\n\nIncludes \\(SE_{{\\hat{\\beta}}_j}\\) and can easily find confidence interval with tidy()\n\nImportant note: Wald test is best for confidence intervals of our coefficient estimates or estimated odds ratios."
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#wald-test-procedure-with-confidence-intervals",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#wald-test-procedure-with-confidence-intervals",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Wald test procedure with confidence intervals",
    "text": "Wald test procedure with confidence intervals\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the confidence interval and determine if it overlaps with null\n\nOverlap with null (usually 0 for coefficient) = fail to reject null\nNo overlap with null (usually 0 for coefficient) = reject null\n\nWrite a conclusion to the hypothesis test\n\nWhat is the estimate and its confidence interval?\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#wald-test-in-our-example",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#wald-test-in-our-example",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Wald test in our example",
    "text": "Wald test in our example\n\nFrom our multiple logistic regression model:\n\n\\[ \\begin{aligned}\n\\text{logit}\\left(\\pi(x_i)\\right) = & \\beta_0\n+\\beta_1 \\cdot I \\left( R/E = H/L \\right)\n+\\beta_2 \\cdot I \\left( R/E = NH AIAN \\right) \\\\\n& +\\beta_3 \\cdot I \\left( R/E = NH API \\right)\n+\\beta_4 \\cdot I \\left( R/E = NH B \\right)\n+\\beta_5 \\cdot Age\n\\end{aligned}\\]\n \n\nWald test can help us construct the confidence interval for ALL coefficient estimates\n\n \n\nIf we want to use the Wald test to determine if a covariate is significant in our model\n\nCan only do so for age"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\nNeeded steps:\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\nCalculate the confidence interval and determine if it overlaps with null\nWrite a conclusion to the hypothesis test"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-1",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\n\nSet the level of significance \\(\\alpha\\)\n\n\\(\\alpha = 0.05\\)\n\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\n\\(H_0: \\beta_5 = 0\\)\n\\(H_1: \\beta_5 \\neq 0\\)"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-2",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-2",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\n\nCalculate the confidence interval and determine if it overlaps with null\n\n\nLook at coefficient estimates (CI should not contain 0) OR estimated odds ratio (CI should not contain 1)\n\n\n\n\ntidy(multi_bc, conf.int=T) %&gt;% \n  gt() %&gt;% \n  tab_options(table.font.size = 28) %&gt;%\n  fmt_number(decimals = 2)\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n−1.04\n0.03\n−38.05\n0.00\n−1.09\n−0.99\n\n\nRace_EthnicityHispanic-Latino\n−0.02\n0.08\n−0.18\n0.85\n−0.18\n0.15\n\n\nRace_EthnicityNH American Indian/Alaskan Native\n−0.09\n0.48\n−0.18\n0.86\n−1.12\n0.81\n\n\nRace_EthnicityNH Asian/Pacific Islander\n0.13\n0.08\n1.60\n0.11\n−0.03\n0.30\n\n\nRace_EthnicityNH Black\n0.36\n0.07\n4.98\n0.00\n0.22\n0.50\n\n\nAge_c\n0.06\n0.00\n17.81\n0.00\n0.05\n0.06"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-3",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-3",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\n\nCalculate the confidence interval and determine if it overlaps with null\n\n\nLook at coefficient estimates (CI should not contain 0) OR estimated odds ratio (CI should not contain 1)\n\n\n\n\ntbl_regression(multi_bc, \n               exponentiate = TRUE) %&gt;%\n  as_gt() %&gt;% \n  tab_options(table.font.size = 30)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\n\nRace_Ethnicity\n\n\n\n\n\n\n\n\n    NH White\n—\n—\n\n\n\n\n    Hispanic-Latino\n0.98\n0.83, 1.16\n0.9\n\n\n    NH American Indian/Alaskan Native\n0.92\n0.33, 2.25\n0.9\n\n\n    NH Asian/Pacific Islander\n1.14\n0.97, 1.35\n0.11\n\n\n    NH Black\n1.43\n1.24, 1.65\n&lt;0.001\n\n\nAge_c\n1.06\n1.05, 1.07\n&lt;0.001\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-4",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-4",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\n\nWrite a conclusion to the hypothesis test\n\nGiven race and ethnicity is already in the model, the regression model with age is more likely than the model without age (p-val &lt; 0.001)."
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#likelihood-ratio-test",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#likelihood-ratio-test",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Likelihood ratio test",
    "text": "Likelihood ratio test\n\nLikelihood ratio test answers the question:\n\nFor a specific covariate, which model tell us more about the outcome variable: the model including the covariate (or set of covariates) or the model omitting the covariate (or set of covariates)?\nAka: Which model is more likely given our data: model including the covariate or the model omitting the covariate?\n\n\n \n\nTest a single coefficient by comparing different models\n\nVery similar to the F-test\n\n\n \n\nImportant: LRT can be used conduct hypothesis tests for multiple coefficients\n\nJust like F-test, we can test a single coefficient, continuous/binary covariate, multi-level covariate, or multiple covariates"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#likelihood-ratio-test-33",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#likelihood-ratio-test-33",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Likelihood ratio test (3/3)",
    "text": "Likelihood ratio test (3/3)\n\nIf testing single variable and it’s continuous or binary, still use this hypothesis test:\n\n\\(H_0\\): \\(\\beta_j = 0\\)\n\\(H_1\\): \\(\\beta_j \\neq 0\\)\n\n\n \n\nIf testing single variable and it’s categorical with mroe than 2 groups, use this hypothesis test:\n\n\\(H_0\\): \\(\\beta_j=\\beta_{j+1}=\\ldots=\\beta_{j+i-1}=0\\)\n\\(H_1\\): at least one of the above \\(\\beta\\)’s is not equal to 0\n\n\n \n\nIf testing a set of variables, use this hypothesis test:\n\n\\(H_0\\): \\(\\beta_1=\\beta_{2}=\\ldots=\\beta_{k}=0\\)\n\\(H_1\\): at least one of the above \\(\\beta\\)’s is not equal to 0"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#lrt-procedure",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#lrt-procedure",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "LRT procedure",
    "text": "LRT procedure\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic and p-value\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#likelihood-ratio-test-33-1",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#likelihood-ratio-test-33-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Likelihood ratio test (3/3)",
    "text": "Likelihood ratio test (3/3)\n\nFrom our multiple logistic regression model:\n\n\\[ \\begin{aligned}\n\\text{logit}\\left(\\pi(\\mathbf{X})\\right) = & \\beta_0\n+\\beta_1 \\cdot I \\left( R/E = H/L \\right)\n+\\beta_2 \\cdot I \\left( R/E = NH AIAN \\right) \\\\\n& +\\beta_3 \\cdot I \\left( R/E = NH API \\right)\n+\\beta_4 \\cdot I \\left( R/E = NH B \\right)\n+\\beta_5 \\cdot Age\n\\end{aligned}\\]\n\nWe can test a single coefficient or multiple coefficients\n \n\nExample 1: Single, continuous variable: Age\n\n \n\nExample 2: Single, &gt;2 categorical variable: Race and Ethnicity\n\n \n\nExample 3: Set of variables: Race and Ethnicity, and Age"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#reminder-on-nested-models",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#reminder-on-nested-models",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Reminder on nested models",
    "text": "Reminder on nested models\n\nLikelihood ratio test is only suitable to test “nested” models\n“Nested” models means the bigger model (full model) contains all the independent variables of the smaller model (reduced model)\nWe cannot compare the following two models using LRT:\n\nModel 1: \\[ \\begin{aligned} \\text{logit}\\left(\\pi(\\mathbf{X})\\right) = & \\beta_0 +\\beta_1 \\cdot I \\left( R/E = H/L \\right) +\\beta_2 \\cdot I \\left( R/E = NH AIAN \\right) \\\\ & +\\beta_3 \\cdot I \\left( R/E = NH API \\right) +\\beta_4 \\cdot I \\left( R/E = NH B \\right) \\end{aligned}\\]\nModel 2: \\[\\begin{aligned} \\text{logit}\\left(\\pi(Age)\\right) = & \\beta_0+\\beta_1 \\cdot Age  \\end{aligned}\\]\n\nIf the two models to be compared are not nested, likelihood ratio test should not be used"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-5",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-5",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\nNeeded steps:\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\nCalculate the test statistic and p-value\nWrite a conclusion to the hypothesis test"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-6",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-6",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\n\nSet the level of significance \\(\\alpha\\)\n\n\\(\\alpha = 0.05\\)\n\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\n\\(H_0: \\beta_5 = 0\\) or model without age is more likely\n\\(H_1: \\beta_5 \\neq 0\\) or model with age is more likely"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-7",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-7",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\n\nCalculate the test statistic and p-value\n\n\nmulti_bc = glm(Late_stage_diag ~ Race_Ethnicity + Age_c, data = bc, family = binomial)\nre_bc = glm(Late_stage_diag ~ Race_Ethnicity, data = bc, family = binomial)\n\nlmtest::lrtest(multi_bc, re_bc)\n\nLikelihood ratio test\n\nModel 1: Late_stage_diag ~ Race_Ethnicity + Age_c\nModel 2: Late_stage_diag ~ Race_Ethnicity\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n1   6 -5741.8                         \n2   5 -5918.1 -1 352.63  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-8",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-8",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\n\nWrite a conclusion to the hypothesis test\n\nGiven race and ethnicity is already in the model, the regression model with age is more likely than the model without age (p-val &lt; \\(2.2\\cdot10^{-16}\\))."
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-2-single-2-categorical-variable-race-and-ethnicity",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-2-single-2-categorical-variable-race-and-ethnicity",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 2: Single, >2 categorical variable: Race and Ethnicity",
    "text": "Example 2: Single, &gt;2 categorical variable: Race and Ethnicity\n\n\nSingle, &gt;2 categorical variable: Race and Ethnicity\n\n\nGiven age is already in the model, is the regression model with race and ethnicity more likely than the model without race and ethnicity?\n\n\nNeeded steps:\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\nCalculate the test statistic and p-value\nWrite a conclusion to the hypothesis test"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-2-single-2-categorical-variable-race-and-ethnicity-1",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-2-single-2-categorical-variable-race-and-ethnicity-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 2: Single, >2 categorical variable: Race and Ethnicity",
    "text": "Example 2: Single, &gt;2 categorical variable: Race and Ethnicity\n\n\nSingle, &gt;2 categorical variable: Race and Ethnicity\n\n\nGiven age is already in the model, is the regression model with race and ethnicity more likely than the model without race and ethnicity?\n\n\n\nSet the level of significance \\(\\alpha\\)\n\n\\(\\alpha = 0.05\\)\n\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\n\\(H_0: \\beta_1 = \\beta_2 = \\beta_3 = \\beta_4 = 0\\) or model without race and ethnicity is more likely\n$H_1: at least one \\(\\beta\\) is not 0 or model with race and ethnicity is more likely"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-2-single-2-categorical-variable-race-and-ethnicity-2",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-2-single-2-categorical-variable-race-and-ethnicity-2",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 2: Single, >2 categorical variable: Race and Ethnicity",
    "text": "Example 2: Single, &gt;2 categorical variable: Race and Ethnicity\n\n\nSingle, &gt;2 categorical variable: Race and Ethnicity\n\n\nGiven age is already in the model, is the regression model with race and ethnicity more likely than the model without race and ethnicity?\n\n\n\nCalculate the test statistic and p-value\n\n\nmulti_bc = glm(Late_stage_diag ~ Race_Ethnicity + Age_c, data = bc, family = binomial)\nage_bc = glm(Late_stage_diag ~ Age_c, data = bc, family = binomial)\n\nlmtest::lrtest(multi_bc, age_bc)\n\nLikelihood ratio test\n\nModel 1: Late_stage_diag ~ Race_Ethnicity + Age_c\nModel 2: Late_stage_diag ~ Age_c\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n1   6 -5741.8                         \n2   2 -5754.8 -4 26.053  3.087e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-2-single-2-categorical-variable-race-and-ethnicity-3",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-2-single-2-categorical-variable-race-and-ethnicity-3",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 2: Single, >2 categorical variable: Race and Ethnicity",
    "text": "Example 2: Single, &gt;2 categorical variable: Race and Ethnicity\n\n\nSingle, &gt;2 categorical variable: Race and Ethnicity\n\n\nGiven age is already in the model, is the regression model with race and ethnicity more likely than the model without race and ethnicity?\n\n\n\nWrite a conclusion to the hypothesis test\n\nGiven age is already in the model, the regression model with race and ethnicity is more likely than the model without race and ethnicity (p-val = \\(3.1\\cdot10^{-5}\\) &lt; 0.05)."
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-3-set-of-variables-race-and-ethnicity-and-age",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-3-set-of-variables-race-and-ethnicity-and-age",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 3: Set of variables: Race and Ethnicity, and Age",
    "text": "Example 3: Set of variables: Race and Ethnicity, and Age\n\n\nSet of variables: Race and Ethnicity, and Age\n\n\nIs the regression model with race and ethnicity and age more likely than the model without race and ethnicity nor age?\n\n\nNeeded steps:\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\nCalculate the test statistic and p-value\nWrite a conclusion to the hypothesis test"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-3-set-of-variables-race-and-ethnicity-and-age-1",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-3-set-of-variables-race-and-ethnicity-and-age-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 3: Set of variables: Race and Ethnicity, and Age",
    "text": "Example 3: Set of variables: Race and Ethnicity, and Age\n\n\nSet of variables: Race and Ethnicity, and Age\n\n\nIs the regression model with race and ethnicity and age more likely than the model without race and ethnicity nor age?\n\n\n\nSet the level of significance \\(\\alpha\\)\n\n\\(\\alpha = 0.05\\)\n\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\n\\(H_0: \\beta_1 = \\beta_2 = \\beta_3 = \\beta_4 = \\beta_5 = 0\\) or model without race and ethnicity and age is more likely\n$H_1: at least one \\(\\beta\\) is not 0 or model with race and ethnicity and age is more likely"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-3-set-of-variables-race-and-ethnicity-and-age-2",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-3-set-of-variables-race-and-ethnicity-and-age-2",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 3: Set of variables: Race and Ethnicity, and Age",
    "text": "Example 3: Set of variables: Race and Ethnicity, and Age\n\n\nSet of variables: Race and Ethnicity, and Age\n\n\nIs the regression model with race and ethnicity and age more likely than the model without race and ethnicity nor age?\n\n\n\nCalculate the test statistic and p-value\n\n\nmulti_bc = glm(Late_stage_diag ~ Race_Ethnicity + Age_c, data = bc, family = binomial)\nintercept_bc = glm(Late_stage_diag ~ 1, data = bc, family = binomial)\n\nlmtest::lrtest(multi_bc, intercept_bc)\n\nLikelihood ratio test\n\nModel 1: Late_stage_diag ~ Race_Ethnicity + Age_c\nModel 2: Late_stage_diag ~ 1\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n1   6 -5741.8                         \n2   1 -5930.5 -5 377.32  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-3-set-of-variables-race-and-ethnicity-and-age-3",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-3-set-of-variables-race-and-ethnicity-and-age-3",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 3: Set of variables: Race and Ethnicity, and Age",
    "text": "Example 3: Set of variables: Race and Ethnicity, and Age\n\n\nSet of variables: Race and Ethnicity, and Age\n\n\nIs the regression model with race and ethnicity and age more likely than the model without race and ethnicity nor age?\n\n\n\nWrite a conclusion to the hypothesis test\n\nThe regression model with race and ethnicity and age is more likely than the model omitting race and ethnicity and age (p-val &lt; \\(2.2\\cdot10^{-16}\\))."
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#estimatedpredicted-probability-for-mlr",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#estimatedpredicted-probability-for-mlr",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Estimated/Predicted Probability for MLR",
    "text": "Estimated/Predicted Probability for MLR\n\nBasic idea for predicting/estimating probability stays the same\n\n \n\nCalculations will be slightly different\n\nEspecially for the confidence interval\n\n\n \n\nRecall our fitted model for late stage breast cancer diagnosis: \\[ \\begin{aligned}\n\\text{logit}\\left(\\widehat{\\pi}(\\mathbf{X})\\right) = &-4.56\n-0.02 \\cdot I \\left( R/E = H/L \\right)\n-0.09 \\cdot I \\left( R/E = NH AIAN \\right) \\\\\n& +0.13 \\cdot I \\left( R/E = NH API \\right)\n+0.36 \\cdot I \\left( R/E = NH B \\right)\n+0.06 \\cdot Age\n\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicted-probability",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicted-probability",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Predicted Probability",
    "text": "Predicted Probability\n\nWe may be interested in predicting probability of having a late stage breast cancer diagnosis for a specific age.\nThe predicted probability is the estimated probability of having the event for given values of covariate(s)\nRecall our fitted model for late stage breast cancer diagnosis: \\[ \\begin{aligned}\n\\text{logit}\\left(\\widehat{\\pi}(\\mathbf{X})\\right) = &-4.56\n-0.02 \\cdot I \\left( R/E = H/L \\right)\n-0.09 \\cdot I \\left( R/E = NH AIAN \\right) \\\\\n& +0.13 \\cdot I \\left( R/E = NH API \\right)\n+0.36 \\cdot I \\left( R/E = NH B \\right)\n+0.06 \\cdot Age\n\\end{aligned}\\]\nWe can convert it to the predicted probability: \\[\\hat{\\pi}(\\mathbf{X})=\\dfrac{\\exp \\left( \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot I \\left( R/E = H/L \\right) + ... + \\widehat{\\beta}_5 \\cdot Age \\right)}  {1+\\exp \\left(\\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot I \\left( R/E = H/L \\right) + ... + \\widehat{\\beta}_5 \\cdot Age \\right)}\\]\n\nThis is an inverse logit calculation\n\nWe can calculate this using the the predict() function like in BSTA 512\n\nAnother option: taking inverse logit of fitted values from augment() function"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Predicting probability in R",
    "text": "Predicting probability in R\n\n\nPredicting probability of late stage breast cancer diagnosis\n\n\nFor someone who is 60 years old and Non-Hispanic Asian/Pacific Islander, what is the predicted probability for late stage breast cancer diagnosis (with confidence intervals)?\n\n\nNeeded steps:\n\nCalculate probability prediction\nCheck if we can use Normal approximation\nCalculate confidence interval\n\nUsing logit scale then converting\nUsing Normal approximation\n\nInterpret results"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r-1",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Predicting probability in R",
    "text": "Predicting probability in R\n\n\nPredicting probability of late stage breast cancer diagnosis\n\n\nFor someone who is 60 years old and Non-Hispanic Asian/Pacific Islander, what is the predicted probability for late stage breast cancer diagnosis (with confidence intervals)?\n\n\n\nCalculate probability prediction\n\n\nnewdata = data.frame(Age_c = 60 - mean_age, \n                     Race_Ethnicity = \"NH Asian/Pacific Islander\")\npred1 = predict(multi_bc, newdata, se.fit = T, type=\"response\")\npred1\n\n$fit\n        1 \n0.2685667 \n\n$se.fit\n         1 \n0.01572695 \n\n$residual.scale\n[1] 1"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r-2",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r-2",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Predicting probability in R",
    "text": "Predicting probability in R\n\n\nPredicting probability of late stage breast cancer diagnosis\n\n\nFor someone who is 60 years old and Non-Hispanic Asian/Pacific Islander, what is the predicted probability for late stage breast cancer diagnosis (with confidence intervals)?\n\n\n\nCheck if we can use Normal approximation\n\nWe can use the Normal approximation if: \\(\\widehat{p}n = \\widehat{\\pi}(X)\\cdot n &gt; 10\\) and \\((1-\\widehat{p})n = (1-\\widehat{\\pi}(X))\\cdot n &gt; 10\\).\n\nn = nobs(multi_bc)\np = pred1$fit\nn*p\n\n       1 \n2685.667 \n\nn*(1-p)\n\n       1 \n7314.333 \n\n\nWe can use the Normal approximation!"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r-3",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r-3",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Predicting probability in R",
    "text": "Predicting probability in R\n\n\nPredicting probability of late stage breast cancer diagnosis\n\n\nFor someone who is 60 years old and Non-Hispanic Asian/Pacific Islander, what is the predicted probability for late stage breast cancer diagnosis (with confidence intervals)?\n\n\n3b. Calculate confidence interval (Option 2: with Normal approximation)\n\npred = predict(multi_bc, newdata, se.fit = T, type = \"response\")\n\nLL_CI = pred$fit - qnorm(1-0.05/2) * pred$se.fit\nUL_CI = pred$fit + qnorm(1-0.05/2) * pred$se.fit\n\nc(Pred = pred$fit, LL_CI, UL_CI) %&gt;% round(digits=3)\n\nPred.1      1      1 \n 0.269  0.238  0.299"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r-4",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r-4",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Predicting probability in R",
    "text": "Predicting probability in R\n\n\nPredicting probability of late stage breast cancer diagnosis\n\n\nFor someone who is 60 years old and Non-Hispanic Asian/Pacific Islander, what is the predicted probability for late stage breast cancer diagnosis (with confidence intervals)?\n\n\n\nInterpret results\n\nFor someone who is 60 years old and Non-Hispanic Asian/Pacific Islander, the predicted probability of late stage breast cancer diagnosis is 0.269 (95% CI: 0.238, 0.299)."
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-table",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-table",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "How to present odds ratios: Table",
    "text": "How to present odds ratios: Table\n\ntbl_regression() in the gtsummary package is helpful for presenting the odds ratios in a clean way\n\n\nlibrary(gtsummary)\ntbl_regression(multi_bc, exponentiate = TRUE) %&gt;% \n  as_gt() %&gt;% # allows us to use tab_options()\n  tab_options(table.font.size = 38)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\n\nRace_Ethnicity\n\n\n\n\n\n\n\n\n    NH White\n—\n—\n\n\n\n\n    Hispanic-Latino\n0.98\n0.83, 1.16\n0.9\n\n\n    NH American Indian/Alaskan Native\n0.92\n0.33, 2.25\n0.9\n\n\n    NH Asian/Pacific Islander\n1.14\n0.97, 1.35\n0.11\n\n\n    NH Black\n1.43\n1.24, 1.65\n&lt;0.001\n\n\nAge_c\n1.06\n1.05, 1.07\n&lt;0.001\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-forest-plot-setup",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-forest-plot-setup",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "How to present odds ratios: Forest Plot Setup",
    "text": "How to present odds ratios: Forest Plot Setup\n\nlibrary(broom.helpers)\nMLR_tidy = tidy_and_attach(multi_bc, conf.int=T, exponentiate = T) %&gt;%\n  tidy_remove_intercept() %&gt;%\n  tidy_add_reference_rows() %&gt;%\n  tidy_add_estimate_to_reference_rows() %&gt;%\n  tidy_add_term_labels()\nglimpse(MLR_tidy)\n\nRows: 6\nColumns: 16\n$ term           &lt;chr&gt; \"Race_EthnicityNH White\", \"Race_EthnicityHispanic-Latin…\n$ variable       &lt;chr&gt; \"Race_Ethnicity\", \"Race_Ethnicity\", \"Race_Ethnicity\", \"…\n$ var_label      &lt;chr&gt; \"Race_Ethnicity\", \"Race_Ethnicity\", \"Race_Ethnicity\", \"…\n$ var_class      &lt;chr&gt; \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"nume…\n$ var_type       &lt;chr&gt; \"categorical\", \"categorical\", \"categorical\", \"categoric…\n$ var_nlevels    &lt;int&gt; 5, 5, 5, 5, 5, NA\n$ contrasts      &lt;chr&gt; \"contr.treatment\", \"contr.treatment\", \"contr.treatment\"…\n$ contrasts_type &lt;chr&gt; \"treatment\", \"treatment\", \"treatment\", \"treatment\", \"tr…\n$ reference_row  &lt;lgl&gt; TRUE, FALSE, FALSE, FALSE, FALSE, NA\n$ label          &lt;chr&gt; \"NH White\", \"Hispanic-Latino\", \"NH American Indian/Alas…\n$ estimate       &lt;dbl&gt; 1.0000000, 0.9846940, 0.9178662, 1.1433526, 1.4300256, …\n$ std.error      &lt;dbl&gt; NA, 0.083653090, 0.484110085, 0.083796726, 0.071788616,…\n$ statistic      &lt;dbl&gt; NA, -0.1843845, -0.1770333, 1.5986877, 4.9825778, 17.81…\n$ p.value        &lt;dbl&gt; NA, 8.537118e-01, 8.594822e-01, 1.098900e-01, 6.274274e…\n$ conf.low       &lt;dbl&gt; NA, 0.8344282, 0.3262638, 0.9688184, 1.2414629, 1.05221…\n$ conf.high      &lt;dbl&gt; NA, 1.158411, 2.254643, 1.345732, 1.645053, 1.065538"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-forest-plot-setup-1",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-forest-plot-setup-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "How to present odds ratios: Forest Plot Setup",
    "text": "How to present odds ratios: Forest Plot Setup\n\nMLR_tidy = MLR_tidy %&gt;%\n  mutate(var_label = case_match(var_label, \n                               \"Race_Ethnicity\" ~ \"Race and ethnicity\", \n                               \"Age_c\" ~ \"\"), \n         label = case_match(label, \n                            \"NH White\" ~ \"Non-Hispanic White\", \n                            \"Hispanic-Latino\" ~ \"Hispanic-Latinx\", \n                            \"NH American Indian/Alaskan Native\" ~ \"Non-Hispanic American \\n Indian/Alaskan Native\", \n                            \"NH Asian/Pacific Islander\" ~ \"Non-Hispanic \\n Asian/Pacific Islander\",\n                            \"NH Black\" ~ \"Non-Hispanic Black\", \n                               \"Age_c\" ~ \"Age (yrs)\"))\n  # %&gt;%\n  #                       fct_relevel(\"Age (yrs)\", \"Non-Hispanic \\n Asian/Pacific Islander\", \"Non-Hispanic American \\n Indian/Alaskan Native\", \"Non-Hispanic White\", \"Hispanic-Latinx\", \"Non-Hispanic Black\"))"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-forest-plot",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-forest-plot",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "How to present odds ratios: Forest Plot",
    "text": "How to present odds ratios: Forest Plot\n\nMLR_tidy = MLR_tidy %&gt;% mutate(label = fct_reorder(label, term))\n\nplot_MLR = ggplot(data=MLR_tidy, \n       aes(y=label, x=estimate, xmin=conf.low, xmax=conf.high)) + \n  geom_point(size = 3) +  geom_errorbarh(height=.2) + \n  \n  geom_vline(xintercept=1, color='#C2352F', linetype='dashed', alpha=1) +\n  theme_classic() +\n  \n  facet_grid(rows = vars(var_label), scales = \"free\",\n             space='free_y', switch = \"y\") + \n  \n  labs(x = \"OR (95% CI)\", \n       title = \"Odds ratios of Late Stage Breast Cancer Diagnosis\") +\n  theme(axis.title = element_text(size = 25), \n        axis.text = element_text(size = 25), \n        title = element_text(size = 25), \n        axis.title.y=element_blank(), \n        strip.text = element_text(size = 25), \n        strip.placement = \"outside\", \n        strip.background = element_blank())\n# plot_MLR"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-forest-plot-1",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-forest-plot-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "How to present odds ratios: Forest Plot",
    "text": "How to present odds ratios: Forest Plot"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#adding-odds-ratios",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#adding-odds-ratios",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Adding odds ratios",
    "text": "Adding odds ratios\n\nMLR_tidy = MLR_tidy %&gt;% \n  mutate(estimate_r = round(estimate, 2), \n         conf.low_r = round(conf.low, 2), \n         conf.high_r = round(conf.high, 2), \n         OR_char = paste0(estimate_r, \" (\", conf.low_r, \", \", conf.high_r, \")\"), \n         OR_char = ifelse(reference_row == F | is.na(reference_row), OR_char, NA))\n\n \n\n“Plot” of the text for odds ratios estimates\n\n\nOR_labs = ggplot(data=MLR_tidy, aes(y=label)) +\n  geom_text(aes(x = -1, label = OR_char), hjust = 0, size=8) +   \n  xlim(-1, 1) +\n  facet_grid(rows = vars(var_label), scales = \"free\",\n             space='free_y') +\n  theme_void() + \n    theme(strip.text = element_blank())"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#combine-them",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#combine-them",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Combine them!!",
    "text": "Combine them!!\n\nlibrary(cowplot)\nplot_grid(plot_MLR, OR_labs, ncol=2, align = \"h\", rel_widths = c(4, 1))"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#multivariable-logistic-regression-model",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#multivariable-logistic-regression-model",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Multivariable Logistic Regression Model",
    "text": "Multivariable Logistic Regression Model\n\nThe multivariable model of logistic regression (called multiple logistic regression) is useful in that it statistically adjusts the estimated effect of each variable in the model\n\n \n\nEach estimated coefficient provides an estimate of the log odds adjusting for all other variables included in the model\n \n\nThe adjusted odds ratio can be different from or similar to the unadjusted odds ratio\n\n \n\nComparing adjusted vs. unadjusted odds ratios can be a useful activity"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#interpretation-of-coefficients-in-mlr",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#interpretation-of-coefficients-in-mlr",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Interpretation of Coefficients in MLR",
    "text": "Interpretation of Coefficients in MLR\n\nThe interpretation of coefficients in multiple logistic regression is essentially the same as the interpretation of coefficients in simple logistic regression\n\n \n\nFor interpretation, we need to\n\npoint out that these are adjusted estimates\nprovide a list of other variables in the model"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-race-and-ethnicity-and-age-model-fit-fixed",
    "href": "lessons/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-race-and-ethnicity-and-age-model-fit-fixed",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example: Race and Ethnicity and Age model fit (FIXED)",
    "text": "Example: Race and Ethnicity and Age model fit (FIXED)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\n\nRace_Ethnicity\n\n\n\n\n\n\n\n\n    NH White\n—\n—\n\n\n\n\n    Hispanic-Latino\n0.98\n0.83, 1.16\n0.9\n\n\n    NH American Indian/Alaskan Native\n0.92\n0.33, 2.25\n0.9\n\n\n    NH Asian/Pacific Islander\n1.14\n0.97, 1.35\n0.11\n\n\n    NH Black\n1.43\n1.24, 1.65\n&lt;0.001\n\n\nAge_c\n1.06\n1.05, 1.07\n&lt;0.001\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\n\n\nThe estimated odds of late stage breast cancer diagnosis for Hispanic-Latinx individuals is 0.98 times that of Non-Hispanic White individuals, controlling for age (95% CI: 0.83, 1.16).\nThe estimated odds of late stage breast cancer diagnosis for Non-Hispanic American Indian/Alaskan Natives is 0.92 times that of Non-Hispanic White individuals, controlling for age (95% CI: 0.33, 2.25).\nThe estimated odds of late stage breast cancer diagnosis for Non-Hispanic Asian/Pacific Islanders is 1.14 times that of Non-Hispanic White individuals, controlling for age (95% CI: 0.97, 1.35).\nThe estimated odds of late stage breast cancer diagnosis for Non-Hispanic Black individuals is 1.43 times that of Non-Hispanic White individuals, controlling for age (95% CI: 1.24, 1.65).\nFor every one year increase in age, there is an 6% increase in the estimated odds of late stage breast cancer diagnosis, adjusting for race and ethnicity (95% CI: 5%, 7%)."
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#review-of-model-assessment-so-far-12",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#review-of-model-assessment-so-far-12",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Review of model assessment so far (1/2)",
    "text": "Review of model assessment so far (1/2)\n\nOverall measurements of fit\n\nHow well does the fitted logistic regression model predict the outcome?\nDifferent ways to measure the answer to this question\n\n\n\n\n\n\n\n\n\n\n\nMeasure of fit\nHypothesis tested?\nEquation\nR code\n\n\n\n\nPearson residual\nYes\n\\(X^2=\\sum_{j=1}^{J}{r\\left(Y_j,{\\hat{\\pi}}_j\\right)^2}\\)\nNot given\n\n\nHosmer-Lemeshow test\nYes\n\\(\\hat{C}=\\sum_{k=1}^{g}\\frac{\\left(o_k-n_k^\\prime{\\bar{\\pi}}_k\\right)^2}{n_k^\\prime{\\bar{\\pi}}_k(1-{\\bar{\\pi}}_k)}\\)\nhoslem.test()\n\n\nAUC-ROC\nKinda\nNot given\nauc(observed, predicted)\n\n\nAIC\nOnly to compare models\n\\(AIC = -2 \\cdot \\text{log-likelihood} + 2q\\)\nAIC(model_name)\n\n\nBIC\nOnly to compare models\n\\(BIC = -2 \\cdot \\text{log-likelihood} + q\\text{log}(n)\\)\nBIC(model_name)"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#review-of-model-assessment-so-far-22",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#review-of-model-assessment-so-far-22",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Review of model assessment so far (2/2)",
    "text": "Review of model assessment so far (2/2)\n\nNumerical problems\n\nAssess pre and post model fit\nNumerical problems often depend on the final model (which variables and interactions are included)\n\nDifferent numerical problems to look out for\n\nZero cell count\nComplete separation\nMulticollinearity"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#today",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#today",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Today",
    "text": "Today\n\nWe now use model diagnostics to identify any observations that the model does not fit well"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#review-of-number-of-covariate-patterns",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#review-of-number-of-covariate-patterns",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Review of Number of Covariate Patterns",
    "text": "Review of Number of Covariate Patterns\n\nCovariate patterns are the unique covariate combinations that are observed\n\n \n\nFor example: model contains two binary covariates (history of fracture and smoking status), there will be 4 unique combination of these factors\n\nThis model has 4 covariate patterns\nSubjects can be divided into 4 groups based on the covariates’ values\n\n\n \n\nWhen we have continuous covariates, the number of covariate patterns will be close to the number of individuals in the dataset"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#from-overall-measure-to-diagnostics",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#from-overall-measure-to-diagnostics",
    "title": "Lesson 14: Model Diagnostics",
    "section": "From overall measure to diagnostics",
    "text": "From overall measure to diagnostics\n\nNow we need to investigate diagnostics looking at individual data or covariate pattern data\n\nMake sure the overall measure has not been influenced by certain observations\n\n\n \n\nThe key quantities from logistic regression diagnostics are the components of “residual sum-of-squares”\n\nThe same idea as in the linear regression\nAssessed for each covariate pattern \\(j\\), by computing standardized Pearson residuals and Deviance residuals\n\nStandardization using \\(h_j\\), the leverage values"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#hat-matrix-and-leverage-values-linear-regression",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#hat-matrix-and-leverage-values-linear-regression",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Hat Matrix and Leverage Values: Linear regression",
    "text": "Hat Matrix and Leverage Values: Linear regression\n\nWe have learned “hat” matrix and leverage values from linear regression diagnostics\n\n \n\nIn linear regression, the hat matrix projects the outcome variable onto the covariate space:\n \n\n\\(H=X\\left(X^\\prime X\\right)^{-1}X^\\prime\\) and \\(\\hat{y}=Hy\\)\n\n \n\nThe linear regression residuals is thus \\(y - \\widehat{y}\\), or \\((I-H)y\\)\n\n\n \n\nThe leverage is just the diagonal elements of the hat matrix, which is proportional to the distance of \\(x_j\\) to the mean of the data \\(\\overline{x}\\)"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#hat-matrix-and-leverage-values-logistic-regression",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#hat-matrix-and-leverage-values-logistic-regression",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Hat Matrix and Leverage Values: Logistic regression",
    "text": "Hat Matrix and Leverage Values: Logistic regression\n\nIn logistic regression model, the hat matrix is: \\[H=V^\\frac{1}{2}X\\left(X^\\prime V\\ X\\right)^{-1}X^\\prime V^\\frac{1}{2}\\]\nThe leverage is \\[h_j=m_j\\cdot\\hat{\\pi}\\left(\\textbf{x}_j\\right)\\left[1-\\hat{\\pi}\\left(\\textbf{x}_j\\right)\\right]\\textbf{x}_j^\\prime\\left(\\textbf{X}^\\prime\\textbf{VX}\\right)^{-1}\\textbf{x}_j=v_j\\cdot b_j\\]\n\n\\(b\\): weighted distance of \\(x_j\\) from \\(\\overline{x}\\)\n\\(v_j\\): model based estimator of the variance of \\(y_j\\)\n\n\\(v_j=m_j\\cdot\\hat{\\pi}\\left(\\textbf{x}_j\\right)\\left[1-\\hat{\\pi}\\left(\\textbf{x}_j\\right)\\right]\\)\n\n\n\\(h_j\\) reflects the relative influence of each covariate pattern on the model’s fit"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#poll-everywhere-question-1",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#poll-everywhere-question-1",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#poll-everywhere-question-2",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#poll-everywhere-question-2",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#diagnostic-statistics-computation-12",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#diagnostic-statistics-computation-12",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Diagnostic Statistics Computation (1/2)",
    "text": "Diagnostic Statistics Computation (1/2)\n\nTwo diagnostic statistics computation approach\n\nApproach 1: computed by covariate pattern\n\nRecommendation of Hosmer-Lemeshow textbook\nR uses this approach\nIdentify outliers as group that shares the same covariate values (in the same covariate pattern)\n\nApproach 2: individual observation approach\n\nSAS uses this approach\nIdentify outliers as individual\n\n\nWhy prefer covariate patterns approach?\n\nWhen the number of covariate pattern is much smaller than n, there is risk that we may fail to identify influential and/or poorly fit covariate patterns using individual based on residual"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#diagnostic-statistics-computation-22",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#diagnostic-statistics-computation-22",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Diagnostic Statistics Computation (2/2)",
    "text": "Diagnostic Statistics Computation (2/2)\nConsider a covariate pattern with \\(m_j\\) subjects, all did not have event (some \\(y_i = 0\\)). So the estimated logistic probability is \\(\\widehat\\pi_j\\)\n\nPearson residual computed by individual \\[r_i=-\\sqrt{\\frac{{\\hat{\\pi}}_j}{(1-{\\hat{\\pi}}_j)}}\\]\nPearson residual computed by covariate pattern \\[r_i=-\\sqrt{m_j}\\sqrt{\\frac{{\\hat{\\pi}}_j}{(1-{\\hat{\\pi}}_j)}}\\]\nDifference between aboveresiduals will be large if \\(m_j\\) is large: usually a problem if less covariate patterns\n\nResidual from covariate pattern will identify poorly fit covariate patterns"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#diagnostics-of-logistic-regression",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#diagnostics-of-logistic-regression",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Diagnostics of Logistic Regression",
    "text": "Diagnostics of Logistic Regression\n\nModel diagnostics of logistic regression can be assessed by checking how influential a covariate pattern is:\n \n\nLook at change in residuals if a covariate pattern is excluded\n\nStandardized Pearson residual\nStandardized Deviance residual\n\n\n \n\nLook at change in coefficients if a covariate pattern is excluded"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#change-of-standardized-residuals",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#change-of-standardized-residuals",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Change of Standardized Residuals",
    "text": "Change of Standardized Residuals\n\nChange in standardized Pearson Chi-square statistic due to deletion of subjects with covariate pattern \\(x_j\\): \\[\\Delta X_j^2 = r_{sj}^2 = \\dfrac{r_j^2}{1-h_j}\\]\nDon’t need to know this: change in standardized deviance statistic due to deletion of subjects with covariate pattern \\(x_j\\): \\[\\Delta D_j = \\dfrac{d_j^2}{1-h_j}\\]\nRefer to Lesson 12: Assessing Model Fit for expression of Pearson residual"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#change-of-estimated-coefficients",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#change-of-estimated-coefficients",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Change of Estimated Coefficients",
    "text": "Change of Estimated Coefficients\n\nChange in estimated coefficients due to deletion of subjects with covariate pattern \\(x_j\\): \\[\\Delta \\widehat{\\beta}_j = \\dfrac{r_j^2 h_j}{(1-h_j)^2}\\]\n\n \n\nThis is the logistic regression analog of Cook’s influence statistic (in linear regression)"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#visual-assessment-for-diagnostics-of-logistic-regression-i",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#visual-assessment-for-diagnostics-of-logistic-regression-i",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Visual Assessment for Diagnostics of Logistic Regression (I)",
    "text": "Visual Assessment for Diagnostics of Logistic Regression (I)\n\nIn logistic regression, we mainly rely on graphical methods\n\nBecause the distribution of diagnostic measures under null hypothesis (that the model fits) is only known in certain limited settings\n\n\n \n\nFour plots for analysis of diagnostics in logistic regression:\n\n\\(\\Delta X_j^2\\) vs. \\({\\hat{\\pi}}_j\\)\n\\(\\Delta D_j\\) vs. \\({\\hat{\\pi}}_j\\)\n\\(\\Delta\\widehat{\\beta}_j\\) vs. \\({\\hat{\\pi}}_j\\)\n\\(h_j\\) vs. \\({\\hat{\\pi}}_j\\)"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#recall-the-model-we-fit-glow-study-with-interactions",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#recall-the-model-we-fit-glow-study-with-interactions",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Recall the model we fit: GLOW Study with interactions",
    "text": "Recall the model we fit: GLOW Study with interactions\n\nOutcome variable: any fracture in the first year of follow up (FRACTURE: 0 or 1)\nRisk factor/variable of interest: history of prior fracture (PRIORFRAC: 0 or 1)\nPotential confounder or effect modifier: age (AGE, a continuous variable)\nFitted model with interactions: \\[\\begin{aligned} \\text{logit}\\left(\\widehat\\pi(\\mathbf{X})\\right) & = \\widehat\\beta_0 &+ &\\widehat\\beta_1\\cdot I(\\text{PF}) & + &\\widehat\\beta_2\\cdot Age& + &\\widehat\\beta_3 \\cdot I(\\text{PF}) \\cdot Age \\\\  \n\\text{logit}\\left(\\widehat\\pi(\\mathbf{X})\\right) & = -1.376 &+ &1.002\\cdot I(\\text{PF})& + &0.063\\cdot Age& -&0.057 \\cdot I(\\text{PF}) \\cdot Age\n\\end{aligned}\\]\n\n \n\nLesson 12: determined the overall fit of this model\nToday: determine the if any observations/covariate patterns that model does not fit well"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#how-do-we-get-these-values-in-r",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#how-do-we-get-these-values-in-r",
    "title": "Lesson 14: Model Diagnostics",
    "section": "How do we get these values in R?",
    "text": "How do we get these values in R?\n\nNice function in the R script Logistic_Dx_Functions.R\n\nHighly suggest you save this R script for future use!!\n\n\n\nsource(here(\"lessons\", \"14_Model_diagnostics\", \"Logistic_Dx_Functions.R\"))\ndx_glow = dx(glow_m3)\nglimpse(dx_glow)\n\nRows: 71\nColumns: 16\n$ `(Intercept)`        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ priorfracYes         &lt;dbl&gt; 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0…\n$ age_c                &lt;dbl&gt; 1, -7, 7, -2, 10, 20, 1, -2, 2, 8, 18, -8, 11, 10…\n$ `priorfracYes:age_c` &lt;dbl&gt; 1, 0, 7, 0, 10, 0, 0, -2, 2, 0, 0, 0, 0, 0, -3, 0…\n$ y                    &lt;dbl&gt; 2, 2, 3, 2, 2, 1, 3, 3, 1, 5, 1, 3, 2, 1, 1, 4, 1…\n$ P                    &lt;dbl&gt; 0.4088354, 0.1402159, 0.4162991, 0.1822879, 0.420…\n$ n                    &lt;int&gt; 5, 15, 7, 10, 5, 2, 12, 8, 3, 15, 2, 18, 7, 4, 3,…\n$ yhat                 &lt;dbl&gt; 2.0441770, 2.1032389, 2.9140936, 1.8228786, 2.100…\n$ Pr                   &lt;dbl&gt; -0.04018670, -0.07677228, 0.06586860, 0.14507476,…\n$ dr                   &lt;dbl&gt; -0.04023255, -0.07730975, 0.06577949, 0.14332786,…\n$ h                    &lt;dbl&gt; 0.008844090, 0.003811004, 0.008725450, 0.00290085…\n$ sPr                  &lt;dbl&gt; -0.04036559, -0.07691899, 0.06615786, 0.14528564,…\n$ sdr                  &lt;dbl&gt; -0.04041165, -0.07745749, 0.06606836, 0.14353620,…\n$ dChisq               &lt;dbl&gt; 0.001629381, 0.005916530, 0.004376863, 0.02110791…\n$ dDev                 &lt;dbl&gt; 0.001633102, 0.005999662, 0.004365028, 0.02060264…\n$ dBhat                &lt;dbl&gt; 1.453897e-05, 2.263418e-05, 3.852626e-05, 6.14091…"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#key-to-the-values",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#key-to-the-values",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Key to the values",
    "text": "Key to the values\n\n\n\ncolnames(dx_glow)\n\n [1] \"(Intercept)\"        \"priorfracYes\"       \"age_c\"             \n [4] \"priorfracYes:age_c\" \"y\"                  \"P\"                 \n [7] \"n\"                  \"yhat\"               \"Pr\"                \n[10] \"dr\"                 \"h\"                  \"sPr\"               \n[13] \"sdr\"                \"dChisq\"             \"dDev\"              \n[16] \"dBhat\"             \n\n\n\nFor each covariate pattern (which is each row) …\n\ny: Number of events\nP: Estimated probability of events\nn: Number of observations\nyhat: Estimated number of events\nPr: Pearson residual\ndr: Deviance\nh: leverage\nsPr: Standardized Pearson residual\nsdr: Standardized deviance\ndChisq: Change in standardized Pearson residual\ndDev: Change in standardized deviance\ndBhat: Change in coefficient estimates"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#poll-everywhere-question-3",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#poll-everywhere-question-3",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#visual-assessment-for-diagnostics-of-logistic-regression",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#visual-assessment-for-diagnostics-of-logistic-regression",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Visual Assessment for Diagnostics of Logistic Regression",
    "text": "Visual Assessment for Diagnostics of Logistic Regression\n\nThe plots allow us to identify those covariate patterns that are…\n\nPoorly fit\n\nLarge values of \\(\\Delta X_j^2\\) (and/or \\(\\Delta D_j\\) if we looked at those)\n\nInfluential on estimated coefficients\n\nLarge values of \\(\\Delta\\widehat{\\beta}_j\\)\n\n\nIf you are interested to look at the contribution of leverage (ℎ_𝑗) to the values of the diagnostic statistic, you may also look at plots of:\n\n\\(\\Delta X_j^2\\) vs. \\({\\hat{\\pi}}_j\\)\n\\(\\Delta D_j\\) vs. \\({\\hat{\\pi}}_j\\)\n\\(\\Delta\\widehat{\\beta}_j\\) vs. \\({\\hat{\\pi}}_j\\)"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-standardized-pearson-residuals",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-standardized-pearson-residuals",
    "title": "Lesson 14: Model Diagnostics",
    "section": "GLOW study: standardized Pearson residuals",
    "text": "GLOW study: standardized Pearson residuals\n\n\n\nGenerally, the points that curve from top left to bottom right of plot correspond to covariate patterns with \\(y_j = 1\\)\n\nOpposite corresponds to \\(y_j = 0\\)\n\n\n\n\n\nTo make the plot\nggplot(dx_glow) + geom_point(aes(x=P, y=dChisq), size = 3) + \n  xlab(\"Estimated/Predicted Probability of Fracture\") +\n  ylab(\"Change in Std. Pearson Residual\") +\n  theme(text = element_text(size = 26))"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-standardized-pearson-residuals-1",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-standardized-pearson-residuals-1",
    "title": "Lesson 14: Model Diagnostics",
    "section": "GLOW study: standardized Pearson residuals",
    "text": "GLOW study: standardized Pearson residuals\n\n\n\nGenerally, the points that curve from top left to bottom right of plot correspond to covariate patterns with \\(y_j = 1\\)\n\nOpposite corresponds to \\(y_j = 0\\)\n\nPoints in the top left or top right corners identify the covariate patterns that are poorly fit\nWe may use 4 as a crude approximation to the upper 95th percentile for \\(\\Delta X_j^2\\)\n\n95th percentile of chi-squared distribution is 3.84\n\n\n\n\n\nTo make the plot\nggplot(dx_glow) + geom_point(aes(x=P, y=dChisq), size = 3) + \n  xlab(\"Estimated/Predicted Probability of Fracture\") +\n  ylab(\"Change in Std. Pearson Residual\") +\n  theme(text = element_text(size = 26))"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-standardized-pearson-residuals-2",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-standardized-pearson-residuals-2",
    "title": "Lesson 14: Model Diagnostics",
    "section": "GLOW study: standardized Pearson residuals",
    "text": "GLOW study: standardized Pearson residuals\n\n\n\nGenerally, the points that curve from top left to bottom right of plot correspond to covariate patterns with \\(y_j = 1\\)\n\nOpposite corresponds to \\(y_j = 0\\)\n\nPoints in the top left or top right corners identify the covariate patterns that are poorly fit\nWe may use 4 as a crude approximation to the upper 95th percentile for \\(\\Delta X_j^2\\)\n\n95th percentile of chi-squared distribution is 3.84\n\nWhich point is over 4?\n\n\ndx_glow %&gt;% filter(dChisq &gt; 4) %&gt;% select(priorfracYes, age_c, P, dChisq)\n\n   priorfracYes age_c         P   dChisq\n          &lt;num&gt; &lt;num&gt;     &lt;num&gt;    &lt;num&gt;\n1:            0    -4 0.1643855 4.413937\n\n\n\n\n\nTo make the plot\nggplot(dx_glow) + geom_point(aes(x=P, y=dChisq), size = 3) + \n  xlab(\"Estimated/Predicted Probability of Fracture\") +\n  ylab(\"Change in Std. Pearson Residual\") +\n  theme(text = element_text(size = 26))"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-standardized-deviance-residuals",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-standardized-deviance-residuals",
    "title": "Lesson 14: Model Diagnostics",
    "section": "GLOW study: standardized Deviance residuals",
    "text": "GLOW study: standardized Deviance residuals\n\n\n\nSame investigation as Pearson residuals\nPoints in the top left or top right corners identify the covariate patterns that are poorly fit\nUse 4 as a crude approximation to the upper 95th percentile\nWhich point is over 4?\n\n\ndx_glow %&gt;% filter(dDev &gt; 4) %&gt;% \n  select(priorfracYes, age_c, P, dDev)\n\n   priorfracYes age_c         P     dDev\n          &lt;num&gt; &lt;num&gt;     &lt;num&gt;    &lt;num&gt;\n1:            0   -10 0.1190935 4.841217\n2:            0     7 0.2812460 5.313540\n3:            1     6 0.4150524 4.325664\n\n\n\n\n\nTo make the plot\nggplot(dx_glow) + geom_point(aes(x=P, y=dDev), size = 3) + \n  xlab(\"Estimated/Predicted Probability of Fracture\") +\n  ylab(\"Change in Std. Deviance Residual\") +\n  theme(text = element_text(size = 26))"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-change-in-coefficient-estimates",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-change-in-coefficient-estimates",
    "title": "Lesson 14: Model Diagnostics",
    "section": "GLOW Study: Change in coefficient estimates",
    "text": "GLOW Study: Change in coefficient estimates\n\n\n\nBook recommends flagging certain covariate patterns if change in coefficient estimates are greater than 1\nAll values of \\(\\Delta\\widehat{\\beta}_j\\) are below 0.09\n\n\ndx_glow %&gt;% filter(dBhat &gt; 0.075) %&gt;% \n  select(priorfracYes, age_c, P, dBhat)\n\n   priorfracYes age_c         P      dBhat\n          &lt;num&gt; &lt;num&gt;     &lt;num&gt;      &lt;num&gt;\n1:            1    20 0.4325984 0.08926472\n\n\n\n\n\nTo make the plot\nggplot(dx_glow) + geom_point(aes(x=P, y=dBhat), size = 3) + \n  xlab(\"Estimated/Predicted Probability of Fracture\") +\n  ylab(\"Change in Coefficient Estimates\") +\n  theme(text = element_text(size = 26))"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-leverage",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-leverage",
    "title": "Lesson 14: Model Diagnostics",
    "section": "GLOW Study: Leverage",
    "text": "GLOW Study: Leverage\n\n\n\nWe can use the same rule as linear regression: \\(h_j &gt; 3p/n\\)\n\nFlag these points as high leverage\n\nPoints with high leverage\n\n\\(p=4\\): four regression coefficients\n\\(n=500\\): 500 total observations\nLook for \\(h_j &gt; 3p/n = 3\\cdot4 /500 = 0.024\\)\n\n\n\ndx_glow %&gt;% filter(h &gt; 3*4/500) %&gt;% \n  select(priorfracYes, age_c, P, h) %&gt;% \n  head()\n\n   priorfracYes age_c         P          h\n          &lt;num&gt; &lt;num&gt;     &lt;num&gt;      &lt;num&gt;\n1:            0    20 0.4686423 0.02688958\n2:            1   -12 0.3928116 0.03186122\n3:            0    19 0.4531105 0.02451738\n4:            1   -11 0.3940365 0.02900675\n5:            1    19 0.4313389 0.02895824\n6:            1    18 0.4300804 0.02621708\n\n\n\n\n\nTo make the plot\nggplot(dx_glow) + geom_point(aes(x=P, y=h), size=3) + \n  xlab(\"Estimated/Predicted Probability of Fracture\") +\n  ylab(\"Leverage\") +\n  theme(text = element_text(size = 26))"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#find-out-the-influential-observation-from-the-data-set",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#find-out-the-influential-observation-from-the-data-set",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Find Out the “Influential” Observation From the Data Set",
    "text": "Find Out the “Influential” Observation From the Data Set\n\n\n\nWe identified covariate patterns that may be poorly fit or influential\n\n \n\nLet’s identify the covariate patterns that were not fit well\n\n\n\ndx_glow %&gt;% mutate(Cov_patt = 1:nrow(.)) %&gt;%\n  filter(dChisq &gt; 4 | dDev &gt; 4 | dBhat &gt; 1 | \n          h &gt; 3*4/500) %&gt;%\n  select(Cov_patt, y, P, h, dChisq, dDev, dBhat, h) %&gt;%\n  round(., 3)\n\n    Cov_patt     y     P     h dChisq  dDev dBhat\n       &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;  &lt;num&gt; &lt;num&gt; &lt;num&gt;\n 1:        6     1 0.469 0.027  0.008 0.008 0.000\n 2:       22     1 0.393 0.032  0.046 0.047 0.002\n 3:       36     1 0.453 0.025  0.178 0.183 0.004\n 4:       43     0 0.119 0.005  2.581 4.841 0.012\n 5:       45     6 0.164 0.003  4.414 3.554 0.014\n 6:       47     0 0.281 0.006  3.148 5.314 0.018\n 7:       48     0 0.394 0.029  0.670 1.032 0.020\n 8:       49     2 0.431 0.029  0.698 0.693 0.021\n 9:       50     0 0.430 0.026  0.775 1.155 0.021\n10:       53     0 0.415 0.008  2.862 4.326 0.024\n11:       57     2 0.395 0.026  0.949 0.924 0.026\n12:       63     0 0.484 0.029  0.967 1.364 0.029\n13:       69     0 0.434 0.035  1.588 2.358 0.058\n14:       70     1 0.392 0.035  1.610 1.943 0.058\n15:       71     2 0.433 0.032  2.710 3.462 0.089"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#after-identifying-points",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#after-identifying-points",
    "title": "Lesson 14: Model Diagnostics",
    "section": "After identifying points",
    "text": "After identifying points\n\nDo a data quality check\n\nUnless you have a very good reason to believe the data are not measured correctly, then we leave it in\nCommon to do nothing\n\n\n \n\nIf only a few covariate pattern does not fit well (\\(y_j\\) differs from \\(m_j\\widehat\\pi_j\\) ), we are not too worried\n\nWe had 15 out of 71 covariate patterns\n\n\n \n\nIf quite a few covariate patterns do not fit well, potential reasons can be considered:\n\nThe link used in logistic regression model is not appropriate for outcome\n\nThis is usually unlikely, since logistic regression model is very flexible (think back to why we transformed our outcome from binary form)\n\nOne or more important covariates missing in the model\n\nAt least one of the covariates in the model has been entered in the wrong scale (think age-squared vs. age)"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#how-would-i-report-this-combining-all-model-assessment",
    "href": "lessons/14_Model_diagnostics/14_Model_diagnostics.html#how-would-i-report-this-combining-all-model-assessment",
    "title": "Lesson 14: Model Diagnostics",
    "section": "How would I report this? (Combining all model assessment)",
    "text": "How would I report this? (Combining all model assessment)\n\nAssuming I have not checked other final models (no other models to compare AIC/BIC or AUC with)\n\nMethods: To assess the overall model fit, we calculated the AUC-ROC. We also calculated several model diagnostics including standardized Pearson residual, standardized deviance, change in coefficient estimates, and leverage. We identified covariate patterns with high standardized Pearson residual (greater than 4), standardized deviance (greater than 4), change in coefficient estimates (greater than 1), and leverage (greater than 0.024).\n \nResults: Our final logistic regression model consisted of the outcome, fracture, and predictors including prior fracture, age, and their interaction. The AUC-ROC was 0.68. We identified 11 covariate patterns with high leverage and 4 with high standardized Pearson residual, standardized deviance, or change in coefficient estimates. No identified observations were omitted.\n \nDiscussion:\n\nAUC-ROC low: Included covariates were pre-determined\nInfluential points were kept in because all observations were within feasible range of the predictors and outcome. (we could try age-sqaured and see if that helps AUC and/or diagnostics)"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#last-class-glow-study-with-interactions",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#last-class-glow-study-with-interactions",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Last Class: GLOW Study with interactions",
    "text": "Last Class: GLOW Study with interactions\n\nOutcome variable: any fracture in the first year of follow up (FRACTURE: 0 or 1)\nRisk factor/variable of interest: history of prior fracture (PRIORFRAC: 0 or 1)\nPotential confounder or effect modifier: age (AGE, a continuous variable)\nFitted model with interactions: \\[\\begin{aligned} \\text{logit}\\left(\\widehat\\pi(\\mathbf{X})\\right) & = \\widehat\\beta_0 &+ &\\widehat\\beta_1\\cdot I(\\text{PF}) & + &\\widehat\\beta_2\\cdot Age& + &\\widehat\\beta_3 \\cdot I(\\text{PF}) \\cdot Age \\\\  \n\\text{logit}\\left(\\widehat\\pi(\\mathbf{X})\\right) & = -1.376 &+ &1.002\\cdot I(\\text{PF})& + &0.063\\cdot Age& -&0.057 \\cdot I(\\text{PF}) \\cdot Age\n\\end{aligned}\\]\n\n \n\nToday: determine the overall fit of this model"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#last-class-reporting-results-of-glow-study-with-interactions",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#last-class-reporting-results-of-glow-study-with-interactions",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Last Class: Reporting results of GLOW Study with interactions",
    "text": "Last Class: Reporting results of GLOW Study with interactions\n\nRemember our main covariate is prior fracture, so we want to focuse on how age changes the relationship between prior fracture and a new fracture!\n\n\n\nFor individuals 69 years old, the estimated odds of a new fracture for individuals with prior fracture is 2.72 times the estimated odds of a new fracture for individuals with no prior fracture (95% CI: 1.70, 4.35). As seen in Figure 1 (a), the odds ratio of a new fracture when comparing prior fracture status decreases with age, indicating that the effect of prior fractures on new fractures decreases as individuals get older. In Figure 1 (b), it is evident that for both prior fracture statuses, the predict probability of a new fracture increases as age increases. However, the predicted probability of new fracture for those without a prior fracture increases at a higher rate than that of individuals with a prior fracture. Thus, the predicted probabilities of a new fracture converge at age [insert age here].\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Odds ratio of fracture outcome comparing prior fracture to no prior fracture\n\n\n\n\n\n\n\n\n\n\n\n(b) Predicted probability of fracture\n\n\n\n\n\n\n\nFigure 1: Plots of odds ratio and predicted probability from fitted interaction model"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#overview-12",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#overview-12",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Overview (1/2)",
    "text": "Overview (1/2)\n\nOnce a potential final model has been determined, we need to assess the fit of the model\n\n \n\nVariable selection is no longer our focus at this stage\n\nWe want to find answer to whether the model fits the data adequately\n\n\n \n\nAssessing the Goodness of Fit or Assessing model fit\n\nAssess how well our fitted logistic regression model predicts/estimates the observed outcomes\nComparison: fitted/estimated outcome vs. observed outcome"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#some-good-measurements-for-our-final-models",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#some-good-measurements-for-our-final-models",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Some good measurements for our final model(s)",
    "text": "Some good measurements for our final model(s)\n\nPearson residual statistic\n\n \n\nHosmer-Lemeshaw goodness-of-fit statistic\n\n \n\nAUC-ROC (area under the curve of the receiver operating characteristic)\n\n \n\nAIC/BIC"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#overview-22",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#overview-22",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Overview (2/2)",
    "text": "Overview (2/2)\n\nTo assess the fit of the model, it is good to have a mixture of measurements\n\n \n\nWe want to measure the absolute fit: not comparing to any models, but determining if the model fits the data well\n\nPearson residual statistic\nHosmer-Lemeshaw goodness-of-fit statistic\nAUC-ROC (kind of, often do not use a hypothesis test but you can!)\n\n\n \n\nWe want comparable measures of fit: if we have candidate models that are not nested\n\nAUC-ROC\nAIC/BIC"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#poll-everywhere-question-1",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#poll-everywhere-question-1",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#components-to-assess-model-fit",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#components-to-assess-model-fit",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Components to Assess Model Fit",
    "text": "Components to Assess Model Fit\n\nThe model fits the data well if\n\nSummary measures of the distance between the predicted/estimated/fitted and observed Y are small\n\nToday’s lecture!!\n\nThe contribution of each pair (predicted and observed) to these summary measures is unsystematic and is small relative to the error structure of the model\n\nModel Diagnostics that will be covered in another lecture!\n\n\n\n \n\nNeed both components\n\nIt is possible to see a “good” summary measure of the distance between predicted and observed Y with some substantial deviation from fit for a few subjects"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#summary-measures-of-goodness-of-fit",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#summary-measures-of-goodness-of-fit",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Summary Measures of Goodness of Fit",
    "text": "Summary Measures of Goodness of Fit\n\nAka overall measure of fit\n\n \n\nWhat do we need to calculate them?\n\nNeed to define what the fitted outcome is\nNeed to calculate how close the fitted outcome is to the observed outcome\nSummarize across all observations (or individuals’ data)\n\n\n \n\nTwo tests of goodness-of-fit\n\nPearson residual statistic\nHosmer-Lemeshaw goodness-of-fit statistic"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#comparing-fitted-outcome-to-observed-outcome",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#comparing-fitted-outcome-to-observed-outcome",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Comparing fitted outcome to observed outcome",
    "text": "Comparing fitted outcome to observed outcome\n\nIn logistic regression model, we estimate \\(\\pi(\\mathbf{X}) = P(Y=1|\\mathbf{X})\\)\n\nPredicted value, \\(\\widehat\\pi(\\mathbf{X})\\), is between 0 and 1 for each subject\n\nHowever, we always observe \\(Y=1\\) or \\(Y=0\\)\n\nNot an observed \\(\\pi(\\mathbf{X})\\)\n\n\n \n\nWe can deterimine the fitted outcome by sampling Y’s from a Bernoulli distribution with the fitted probability\n\n\\(\\widehat{Y} \\sim \\text{Bernoulli}(\\widehat\\pi(\\mathbf{X}))\\)\n\nIf there are groups of individuals that share the same covariate observations, then we can use the same \\(\\widehat\\pi(\\mathbf{X})\\)\n\n\\(\\sum_j \\widehat{Y} \\sim \\text{Binomial}(\\sum_j, \\widehat\\pi(\\mathbf{X}))\\)\n\n\n \n\nInstead of comparing the expected vs. observed at individual level, we can compare them at “group” level"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#number-of-covariate-patterns",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#number-of-covariate-patterns",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Number of Covariate Patterns",
    "text": "Number of Covariate Patterns\n\nWhen the logistic regression model contains only categorical covariates, we can think of the number of covariate patterns\nFor example: model contains two binary covariates (history of fracture and smoking status), there will be 4 unique combination of these factors\n\nThis model has 4 covariate patterns\nSubjects can be divided into 4 groups based on the covariates’ values\n\nWe can then compute the predicted number of individuals with Y=1 in each group, and compare that with the actual observed number of individuals with Y=1 in that group\n\nWe don’t need to sample this\nWe use the expected value (mean) of the Binomial to determine the \\(\\widehat{Y}\\) for each covariate pattern\nFor covariate pattern \\(j\\) with \\(m_j\\) observations: \\[\\widehat{Y}_j = m_j \\widehat\\pi(\\mathbf{X_j}) = m_j{\\hat{\\pi}}_j\\]"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#pearson-residual",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#pearson-residual",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Pearson Residual",
    "text": "Pearson Residual\n\nIn logistic regression model, can use Pearson residual for summary measure of goodness-of-fit Uses the \\(\\widehat{Y}_j\\) fitted value from previous slide\nPearson residual for jth covariate pattern is: \\[r\\left(Y_j,{\\hat{\\pi}}_j\\right)=\\frac{(Y_j-m_j{\\hat{\\pi}}_j)}{\\sqrt{m_j{\\hat{\\pi}}_j(1-{\\hat{\\pi}}_j)}}=\\frac{(Y_j-{\\hat{Y}}_i)}{\\sqrt{{\\hat{Y}}_i(1-{\\hat{\\pi}}_j)}}\\]\nThe summary statistics of Pearson residual is thus: \\[X^2=\\sum_{j=1}^{J}{r\\left(Y_j,{\\hat{\\pi}}_j\\right)^2}\\]"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#pearson-residual-procedure",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#pearson-residual-procedure",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Pearson Residual procedure",
    "text": "Pearson Residual procedure\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses: same for all data\n\n\\(H_0\\): model fits well\n\\(H_1\\): model does not fits well\n\nCalculate the test statistic and p-value\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#not-going-to-bother-going-through-an-example",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#not-going-to-bother-going-through-an-example",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Not going to bother going through an example",
    "text": "Not going to bother going through an example\n\nWe can calculate this by hand and test against a chi-squared distribution\n\n \n\nNo set R code to do this\n\n \n\nI do not see this as the main way to determine goodness of fit… for a binary outcome!\n\nOften because of the bigger issues with it…"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#issues-with-pearson-residuals",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#issues-with-pearson-residuals",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Issues with Pearson Residuals",
    "text": "Issues with Pearson Residuals\n\nAssume current model has p covariates…\n\nthen \\(X^2\\) (Pearson residual) follows a chi-squared distribution\n\nunder the null hypothesis based on large sample theory\n\nOnly appropriate if the number of covariate patterns is less than the number of observations\n\n\n \n\nWhen the logistic regression model contains one or more continuous covariates, it is likely that the number of covariate patterns equals to the sample size n\n\n \n\nWe should not use Pearson Residuals to evaluate goodness-of-fit test when the fitted model contains one or more continuous variables"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#hosmer-lemeshow-test",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#hosmer-lemeshow-test",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Hosmer-Lemeshow test",
    "text": "Hosmer-Lemeshow test\n\nIf number of covariate patterns is roughly same as the number of observations\n\nWhenever you include a continuous variable in your model\nHosmer-Lemeshow (HL) goodness-of-fit test should be used instead\n\n\n \n\nHowever, HL test does not work well if the number of covariate patterns is small\n\nHL test should not be used if the number of covariate patterns ≤ 6\n\nFor reference: 3 binary predictors makes 8 covariate patterns\n\nPearson residuals \\(X^2\\) should be used when the number of covariate patterns is small\n\n\n \n\nA large p-value from HL test suggests the model fits well"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#poll-everwhere-question-2",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#poll-everwhere-question-2",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Poll Everwhere question 2",
    "text": "Poll Everwhere question 2"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#hosmer-lemeshow-test-1",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#hosmer-lemeshow-test-1",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Hosmer-Lemeshow test",
    "text": "Hosmer-Lemeshow test\n\nHL test uses groupings from percentiles to basically measure what Pearson residual measures\n\n \n\nSteps to compute HL test statistic:\n\nCompute estimated probability \\(\\widehat\\pi(\\mathbf{X}))\\) for all n subjects (\\(n=1, 2, ..., n\\))\nOrder \\(\\widehat\\pi(\\mathbf{X}))\\) from largest to smallest values\nDivide ordered values into g percentile grouping (usually \\(g = 10\\) based on H-L’s suggestion)\nForm table of observed and expected counts\nCalculate HL test statistic from table\nCompare HL test statistic to chi-squared distribution (\\(\\chi^2_{g-2}\\))"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#hosmer-lemeshow-test-statistic",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#hosmer-lemeshow-test-statistic",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Hosmer-Lemeshow test statistic",
    "text": "Hosmer-Lemeshow test statistic\n\nThe test statistic of Hosmer-Lemeshow goodness-of-fit test is denoted by \\(\\widehat{C}\\), which is obtained by calculating the Pearson chi-squared statistic from the \\(g \\times 2\\) table of observed and estimated expected frequencies \\[\\hat{C}=\\sum_{k=1}^{g}\\frac{\\left(o_k-n_k^\\prime{\\bar{\\pi}}_k\\right)^2}{n_k^\\prime{\\bar{\\pi}}_k(1-{\\bar{\\pi}}_k)}\\]\n\nwhere \\(n'_k\\) is the total number of subjects in the \\(k\\)th group\n\nLet \\(c_k\\) be the number of covariate patterns in the \\(k\\)th decile: \\[o_k=\\sum_{j=1}^{c_k}y_j\\] and \\[{\\bar{\\pi}}_k=\\sum_{j=1}^{c_k}\\frac{m_j{\\hat{\\pi}}_j}{n_k^\\prime}\\]"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#hosmer-lemeshow-test-procedure",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#hosmer-lemeshow-test-procedure",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Hosmer-Lemeshow test procedure",
    "text": "Hosmer-Lemeshow test procedure\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses: same for all data\n\n\\(H_0\\): model fits well\n\\(H_1\\): model does not fits well\n\nCalculate the test statistic and p-value\n\nNote: \\(\\widehat{C} \\sim \\chi^2_{df=g-2}\\)\n\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#glow-study-hosmer-lemeshow-test",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#glow-study-hosmer-lemeshow-test",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "GLOW Study: Hosmer-Lemeshow test",
    "text": "GLOW Study: Hosmer-Lemeshow test\n\nOkay, so let’s look at the interaction model from last class \\[\\text{logit}\\left(\\pi(\\mathbf{X})\\right) = \\beta_0 + \\beta_1\\cdot I(\\text{PF}) +\\beta_2\\cdot Age + \\beta_3 \\cdot I(\\text{PF}) \\cdot Age\\]\nWe need to fit the model and use a new command:\n\n\nglow_m3 = glm(fracture ~ priorfrac + age_c + priorfrac*age_c, \n              data = glow, family = binomial)\nlibrary(ResourceSelection)\nobs_vals = as.numeric(glow$fracture) -1\nfit_vals = fitted(glow_m3)\nhoslem.test(obs_vals, fit_vals, g = 10)\n\n\n    Hosmer and Lemeshow goodness of fit (GOF) test\n\ndata:  obs_vals, fit_vals\nX-squared = 6.778, df = 8, p-value = 0.5608\n\n\nNote to Nicky: do NOT make conclusion yet! In the poll everywhere!"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#poll-everywhere-question-3",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#poll-everywhere-question-3",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Poll Everywhere question 3",
    "text": "Poll Everywhere question 3"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#glow-study-hosmer-lemeshow-test-1",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#glow-study-hosmer-lemeshow-test-1",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "GLOW Study: Hosmer-Lemeshow test",
    "text": "GLOW Study: Hosmer-Lemeshow test\n\nConclusion: The p-value is 0.5608, so we fail to reject the null hypothesis that the model fits the data well. Thus, the preliminary final model for the GLOW dataset fits the data well\n\n \n\nDon’t forget that we still need to check individual observations (Model Diagnostics!)\n\n \n\nR may give results for the HL test even if it is not appropriate to use it!\n\nIf number of covariate patterns ≤ 6, do not use HL test"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#big-data-issue-in-goodness-of-fit-test",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#big-data-issue-in-goodness-of-fit-test",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Big Data Issue in Goodness-of-fit Test",
    "text": "Big Data Issue in Goodness-of-fit Test\n\nWhen the sample size is really big (&gt; 1000), it is much more likely to find the H-L reject the model fit (even when the expected vs. observed in each decile categories looks pretty similar)\n\n \n\nThis is due to “too much” power in hypothesis testing.\n\nPaul et al. (2012) for samples sizes from 1000 to 25,000, the number of groups g should be equal to \\[g=\\max{\\left(10,\\min{\\left\\{\\frac{n_1}{2},\\ \\frac{n-n_1}{2},\\ 2+8\\left(\\frac{n}{1000}\\right)^2\\right\\}}\\right)}\\]\n\n\n \n\nFor example, if one has a sample with \\(n=10, 000\\) (sample size) and \\(n_1=1,000\\) (number of events) then \\(g=500\\) groups are suggested\nFor n &gt; 25000, other methods, such as partitioning data into a developmental data set (with smaller n) and a validation set"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#final-notes-on-goodness-of-fit-test",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#final-notes-on-goodness-of-fit-test",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Final Notes on Goodness-of-fit Test",
    "text": "Final Notes on Goodness-of-fit Test\n\nThey should not be used for variable selection\n\nThe likelihood ratio tests for significance of coefficients are much more powerful and appropriate (when nested)\n\n\n \n\nThey are not for model comparison\n\nOne should not use the p-value from goodness of fit tests of different models to decide which model is better than the other\nSomething like AUC-ROC, AIC, or BIC can be used"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#roc-curve-and-auc-12",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#roc-curve-and-auc-12",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "ROC Curve and AUC (1/2)",
    "text": "ROC Curve and AUC (1/2)\n\n\n\nReceiver Operating Characteristics (ROC) curve is useful tool to quantify how good is our model predicting binary outcome\n\n \n\nIt is a plot of sensitivity (true positive rate) versus (1-specificity) or false positive rate of fitted binary values\n\nTrue Positive Rate \\(= \\dfrac{TP}{TP + FN}\\)\nFalse Positive Rate \\(=  \\dfrac{FP}{FP + TN}\\)\n\n\n \n\nThe ROC curve shows the tradeoff between sensitivity and specificity"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#roc-curve-and-auc-22",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#roc-curve-and-auc-22",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "ROC Curve and AUC (2/2)",
    "text": "ROC Curve and AUC (2/2)\n\n\n\nArea under the ROC curve (AUC ROC) is a reasonable summary of the overall predictive accuracy of the test\n\nAccuracy means how well the predicted value matches the observed value\n\n\n \n\nThe closer the curve follows the left-hand border and top border of the ROC space, the more accurate the test\n\nAn AUC =1 represents 100% accuracy\n\n\n \n\nThe closer the curve comes to the 45-degree diagonal line, the less accurate the test\n\nAn AUC = 0.5 represents an unhelpful model\n\nRandom predictions"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#poll-everywhere-question-4",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#poll-everywhere-question-4",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#roc-curve-and-auc-33",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#roc-curve-and-auc-33",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "ROC Curve and AUC (3/3)",
    "text": "ROC Curve and AUC (3/3)\n\nOften only report the AUC\n\n \n\nSuggestions of how to interpret model fit through AUC values:"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#glow-study-roc-of-interaction-model",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#glow-study-roc-of-interaction-model",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "GLOW Study: ROC of interaction model",
    "text": "GLOW Study: ROC of interaction model\n\n\n\nlibrary(pROC)\npredicted &lt;- predict(glow_m3, glow, type=\"response\")\n\n# define object to plot and calculate AUC\nrocobj &lt;- roc(glow$fracture, predicted)\nauc &lt;- round(auc(glow$fracture, predicted),4)\n\n#create ROC plot\nggroc(rocobj, colour = 'steelblue', \n      size = 2, legacy.axes = TRUE) +\n  ggtitle(paste0('ROC Curve ','(AUC = ',auc,')')) +\n  theme(text = element_text(size = 23)) +\n  xlab(\"False Positive Rate (1 - Specificity)\") +\n  ylab(\"True Positive Rate (Sensitivity)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe have a poorly fitting model\nWe can take auc and compare it to other models: good way to pick a model based on predictive power"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#another-way-to-think-about-auc",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#another-way-to-think-about-auc",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Another way to think about AUC",
    "text": "Another way to think about AUC\n\nGLOW Study: Consider the situation in which the fracture status of each individual is known\n\n \n\nRandomly pick one individual from fractured group and one from non-fractured outcome group\n\nBased on their age, height, prior fracture, and all other covariates, we will correctly predict which is from fractured group\n\n\n \n\nThe AUC is the percentage of randomly drawn pairs for which we predict the pair correctly\n\n \n\nTherefore, AUC represents the ability of our covariates to discriminate between individuals with the outcome (fracture) and those without the outcome"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#aic-and-bic",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#aic-and-bic",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "AIC and BIC",
    "text": "AIC and BIC\n\nTwo widely used non-hypothesis testing based measurements that helps select a good model\n\nAkaike Information Criterion (AIC)\nBayesian Information Criterion (BIC)\n\n\n \n\nUnlike likelihood ratio test which is only suitable for nested model, AIC and BIC are suitable for both nested and non-nested model\n\n \n\nThere is no hypothesis/conclusion testing for the comparison between two models\n\nSo not the best for selecting covariates to include in model\nBUT helpful if you have a few preliminary final models that you want to compare"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#poll-everywhere-question-5",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#poll-everywhere-question-5",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Poll Everywhere Question 5",
    "text": "Poll Everywhere Question 5"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#aic-and-bic-1",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#aic-and-bic-1",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "AIC and BIC",
    "text": "AIC and BIC\n\nBoth AIC and BIC penalize a model for having many parameters\n\n \n\n\n\n\n\n\n\n\nMeasure of fit\nEquation\nR code\n\n\n\n\nAkaike information criterion (AIC)\n\\(AIC = -2 \\cdot \\text{log-likelihood} + 2q\\)\nAIC(model_name)\n\n\nBayesian information criterion (BIC)\n\\(AIC = -2 \\cdot \\text{log-likelihood} + q\\text{log}(n)\\)\nBIC(model_name)\n\n\n\n \n\nWhere q is the number of parameters in the model and n is the sample size\nBoth AIC and BIC can only be used to compare models fitting the same data set\nIn comparing two models, the model with smaller AIC and/or BIC is preferred\n\nWhen the difference in AIC between two models exceeds 3, the difference is viewed as “meaningful”"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#aic-and-bic-in-r",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#aic-and-bic-in-r",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "AIC and BIC in R",
    "text": "AIC and BIC in R\n\nAfter fitting the logistic regression model, can calculate AIC and BIC\nLet’s look at the AIC and BIC of our interaction model:\n\n\nAIC(glow_m3)\n\n[1] 531.2716\n\nBIC(glow_m3)\n\n[1] 548.13"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#summary-12",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#summary-12",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Summary (1/2)",
    "text": "Summary (1/2)\n\n\n\n\n\n\n\n\n\nMeasure of fit\nHypothesis tested?\nEquation\nR code\n\n\n\n\nPearson residual\nYes\n\\(X^2=\\sum_{j=1}^{J}{r\\left(Y_j,{\\hat{\\pi}}_j\\right)^2}\\)\nNot given\n\n\nHosmer-Lemeshow test\nYes\n\\(\\hat{C}=\\sum_{k=1}^{g}\\frac{\\left(o_k-n_k^\\prime{\\bar{\\pi}}_k\\right)^2}{n_k^\\prime{\\bar{\\pi}}_k(1-{\\bar{\\pi}}_k)}\\)\nhoslem.test()\n\n\nAUC-ROC\nKinda\nNot given\nauc(observed, predicted)\n\n\nAIC\nOnly to compare models\n\\(AIC = -2 \\cdot \\text{log-likelihood} + 2q\\)\nAIC(model_name)\n\n\nBIC\nOnly to compare models\n\\(AIC = -2 \\cdot \\text{log-likelihood} + q\\text{log}(n)\\)\nBIC(model_name)\n\n\n\nSpecial notes:\n\nUse Hosmer-Lemshow test over Pearson residual unless number of covariate patterns is less than 6\nCannot use Pearson residual when there is a continuous variable in the model"
  },
  {
    "objectID": "lessons/12_Assessing_fit/12_Assessing_fit.html#summary-22",
    "href": "lessons/12_Assessing_fit/12_Assessing_fit.html#summary-22",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Summary (2/2)",
    "text": "Summary (2/2)\n\nFor our interaction model: \\[\\begin{aligned} \\text{logit}\\left(\\widehat\\pi(\\mathbf{X})\\right) & = \\widehat\\beta_0 &+ &\\widehat\\beta_1\\cdot I(\\text{PF}) & + &\\widehat\\beta_2\\cdot Age& + &\\widehat\\beta_3 \\cdot I(\\text{PF}) \\cdot Age \\\\  \n\\text{logit}\\left(\\widehat\\pi(\\mathbf{X})\\right) & = -1.376 &+ &1.002\\cdot I(\\text{PF})& + &0.063\\cdot Age& -&0.057 \\cdot I(\\text{PF}) \\cdot Age\n\\end{aligned}\\]\nWe can examine the overall model fit using:\n\nNot comparing to any other models:\n\nPearson residual: Not appropriate for this model\nHosmer-Lemeshow: \\(\\hat{C}=6.778\\), p-value = 0.56\nAUC-ROC: 0.6819\n\nCan be used to compare to other models:\n\nAUC-ROC: 0.6819\nAIC: 531.27\nBIC: 548.13"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#three-numerical-problems",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#three-numerical-problems",
    "title": "Lesson 13: Numerical Problems",
    "section": "Three Numerical Problems",
    "text": "Three Numerical Problems\n\nIssues that may cause numerical problems:\n \n\nZero cell count\n\n \n\nComplete separation\n\n \n\nMulticollinearity\n\n\n \n\nAll may cause large estimated coefficients and/or large estimated standard errors"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-count-in-a-contingency-table",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-count-in-a-contingency-table",
    "title": "Lesson 13: Numerical Problems",
    "section": "Zero cell count in a contingency table",
    "text": "Zero cell count in a contingency table\n\nIf no observations at any intersection of the covariate and outcome\n\n \n\nZero cell in a contingency table should be detected in descriptive statistical analysis stage\n\n \n\nExample of one covariate with outcome:"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-count-example-13",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-count-example-13",
    "title": "Lesson 13: Numerical Problems",
    "section": "Zero cell count: example (1/3)",
    "text": "Zero cell count: example (1/3)\n\n\n\nExample of logistic regression with one covariate:\n\n\nex1_m = glm(outcome ~ x, data = ex1, \n              family = binomial)"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-count-example-23",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-count-example-23",
    "title": "Lesson 13: Numerical Problems",
    "section": "Zero cell count: example (2/3)",
    "text": "Zero cell count: example (2/3)\n\n\n\nExample of logistic regression with one covariate:\n\n\nex1_m = glm(outcome ~ x, data = ex1, \n              family = binomial())\n\n\n\n\n\n\n\n\nCoefficient estimates\ntidy(ex1_m, conf.int=T) %&gt;% gt() %&gt;% \n  tab_options(table.font.size = 32) %&gt;%\n  fmt_number(decimals = 2)\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n−0.62\n0.47\n−1.32\n0.19\n−1.60\n0.27\n\n\nxTwo\n1.02\n0.65\n1.57\n0.12\n−0.23\n2.35\n\n\nxThree\n20.19\n2,404.67\n0.01\n0.99\n−119.00\nNA\n\n\n\n\n\n\n\n\n\n\nOdds ratio\ntbl_regression(ex1_m, exponentiate=T) %&gt;%\n  as_gt() %&gt;% # allows us to use tab_options()\n  tab_options(table.font.size = 38)\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\n\nx\n\n\n\n\n\n\n\n\n    One\n—\n—\n\n\n\n\n    Two\n2.79\n0.79, 10.5\n0.12\n\n\n    Three\n583,822,601\n0.00,\n\n&gt;0.9\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-count-example-33",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-count-example-33",
    "title": "Lesson 13: Numerical Problems",
    "section": "Zero cell count: example (3/3)",
    "text": "Zero cell count: example (3/3)\n\n\n\nExample of logistic regression with one covariate:\n\n\nex1_m = glm(outcome ~ x, data = ex1, \n              family = binomial())\n\n\n\n\n\n\n\n\nCoefficient estimates\ntidy(ex1_m, conf.int=T) %&gt;% gt() %&gt;% \n  tab_options(table.font.size = 32) %&gt;%\n  fmt_number(decimals = 2)\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n−0.62\n0.47\n−1.32\n0.19\n−1.60\n0.27\n\n\nxTwo\n1.02\n0.65\n1.57\n0.12\n−0.23\n2.35\n\n\nxThree\n20.19\n2,404.67\n0.01\n0.99\n−119.00\nNA\n\n\n\n\n\n\n\n \n\nCoefficient estimate is large and standard error is large! Estimated odds ratio is very large and confidence interval cannot be computed.\n\n\n\n\nOdds ratio\ntbl_regression(ex1_m, exponentiate=T) %&gt;%\n  as_gt() %&gt;% # allows us to use tab_options()\n  tab_options(table.font.size = 38)\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\n\nx\n\n\n\n\n\n\n\n\n    One\n—\n—\n\n\n\n\n    Two\n2.79\n0.79, 10.5\n0.12\n\n\n    Three\n583,822,601\n0.00,\n\n&gt;0.9\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#ways-to-address-zero-cell",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#ways-to-address-zero-cell",
    "title": "Lesson 13: Numerical Problems",
    "section": "Ways to address zero cell",
    "text": "Ways to address zero cell\n\nAdd one-half to each of the cell counts\n\nTechnically works, but not the best option\nRarely useful with a more complex analysis: may work for simple logistic regression\nNicky would say worst option because manipulating the data that does not work on individual level\n\nCollapse the categories to remove the 0 cells\n\nWe could collapse groups 2 and 3 together if it makes clinical sense\nGood idea if this makes clinical sense OR there is no difference between groups\n\nRemove the category with 0 cells\n\nThis would mean we reduce the total sample size as well\nNot a good idea: we would remove people from our dataset. Why would we do that?\n\nIf the variable is in ordinal scale, treat it as continuous\n\nGood idea if you have seen evidence that there is a linear trend on log-odds scale"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#decide-on-how-to-address-zero-cell-12",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#decide-on-how-to-address-zero-cell-12",
    "title": "Lesson 13: Numerical Problems",
    "section": "Decide on how to address zero cell (1/2)",
    "text": "Decide on how to address zero cell (1/2)\n\nLook at the proportions across the predictor, X:\n\n\nggplot(data = ex1, aes(x = x, fill = outcome)) + \n      geom_bar(stat = \"count\", position = \"fill\") +\n      labs(y = \"Proportion of Outcome\")"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#decide-on-how-to-address-zero-cell-22",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#decide-on-how-to-address-zero-cell-22",
    "title": "Lesson 13: Numerical Problems",
    "section": "Decide on how to address zero cell (2/2)",
    "text": "Decide on how to address zero cell (2/2)\n\nLook at the proportions across the predictor, X:\n\n\nggplot(data = ex1, aes(x = x, fill = outcome)) + \n      geom_bar(stat = \"count\", position = \"fill\") +\n      labs(y = \"Proportion of Outcome\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\nCombining groups 2 and 3 together may not be a good idea.\nTheir proportions of the outcome do not look similar.\nThe predictor has an ordinal quality, so this is making me think a continuous approach might be good."
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#collapse-the-categories-of-predictor",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#collapse-the-categories-of-predictor",
    "title": "Lesson 13: Numerical Problems",
    "section": "Collapse the categories of predictor",
    "text": "Collapse the categories of predictor\nCombine groups 2 and 3:\n\nex1_23 = ex1 %&gt;% \n            mutate(x = factor(x, levels = c(\"One\", \"Two\", \"Three\"), \n                                 labels = c(\"One\", \"Two-Three\", \"Two-Three\")))\nex1_23_glm = glm(outcome ~ x, data = ex1_23, family = binomial)\ntbl_regression(ex1_23_glm, exponentiate=T) %&gt;% as_gt() %&gt;% \n  tab_options(table.font.size = 38)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\n\nx\n\n\n\n\n\n\n\n\n    One\n—\n—\n\n\n\n\n    Two-Three\n7.43\n2.32, 26.3\n0.001\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\n\nBased on our previous visual, I don’t think this is a good idea\nLook at the estimated OR comparing group 2 to group 1 from our original model: 2.79 (95% CI: 0.79, 10.5)\n\nLooks different than the estimated OR in the above table"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#remove-the-category-with-0-cells",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#remove-the-category-with-0-cells",
    "title": "Lesson 13: Numerical Problems",
    "section": "Remove the category with 0 cells",
    "text": "Remove the category with 0 cells\nRemove group 3 from the data:\n\nex1_two = ex1 %&gt;% filter(x != \"Three\")\nex1_two_glm = glm(outcome ~ x, data = ex1_two, family = binomial())\ntbl_regression(ex1_two_glm, exponentiate=T) %&gt;% as_gt() %&gt;% \n  tab_options(table.font.size = 38)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\n\nx\n\n\n\n\n\n\n\n\n    One\n—\n—\n\n\n\n\n    Two\n2.79\n0.79, 10.5\n0.12\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval\n\n\n\n\n\n\n\n\n \n\nNot a good idea because we lose information (sample size goes down!)\nAnd really bad when we have other predictors!!"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#treat-predictor-as-continuous",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#treat-predictor-as-continuous",
    "title": "Lesson 13: Numerical Problems",
    "section": "Treat predictor as continuous",
    "text": "Treat predictor as continuous\n\nWhen we treat a predictor as continuous, we need to make sure we have linearty between continuous predictor and log-odds\nCannot test this before fitting the logistic regression with the continuous predictor\n\nTry taking the logit of a probability of 1… it’s infinity!\n\n\n\nex1_cont = ex1 %&gt;% mutate(x = as.numeric(x))\nex1_cont_glm = glm(outcome ~ x, data = ex1_cont, family = binomial())\ntbl_regression(ex1_cont_glm, exponentiate=T) %&gt;% as_gt() %&gt;% \n  tab_options(table.font.size = 38)\n\n\n\n\n\n\n\nCharacteristic\nOR1\n95% CI1\np-value\n\n\n\n\nx\n6.22\n2.63, 18.0\n&lt;0.001\n\n\n\n1 OR = Odds Ratio, CI = Confidence Interval"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#treat-predictor-as-continuous-check-linearity-assumption",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#treat-predictor-as-continuous-check-linearity-assumption",
    "title": "Lesson 13: Numerical Problems",
    "section": "Treat predictor as continuous: check linearity assumption",
    "text": "Treat predictor as continuous: check linearity assumption\n\nnewdata = data.frame(x = c(1, 2, 3)) \npred = predict(ex1_cont_glm, newdata, se.fit=T, type = \"link\")\nLL_CI1 = pred$fit - qnorm(1-0.05/2) * pred$se.fit\nUL_CI1 = pred$fit + qnorm(1-0.05/2) * pred$se.fit\n\npred_link = cbind(Pred = pred$fit, LL_CI1, UL_CI1) %&gt;% inv.logit()\npred_prob = as.data.frame(pred_link) %&gt;% mutate(x = c(\"One\", \"Two\", \"Three\"))\n\n\n\n\n\nPlotting sample and predicted probabilities\nggplot() + \n      geom_bar(data = ex1, aes(x = x, fill = outcome), stat = \"count\", position = \"fill\") +\n      labs(y = \"Proportion of Outcome\") +\n      scale_fill_manual(values=c(\"#D6295E\", \"#ED7D31\")) +\n      geom_point(data = pred_prob, aes(x = x, y=Pred), size=3) +\n      geom_errorbar(data = pred_prob, aes(x = x, y=Pred, ymin = LL_CI1, ymax = UL_CI1), width = 0.25)\n\n\n\n\n\n\n\n\n\n\n \n\nThis looks pretty good. We’ve mostly captured the trend of the outcome proportion!"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-count-when-we-have-multiple-predictors",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-count-when-we-have-multiple-predictors",
    "title": "Lesson 13: Numerical Problems",
    "section": "Zero cell count when we have multiple predictors",
    "text": "Zero cell count when we have multiple predictors\n\nNote that we may not see the zero count cells in a single predictor\n \n\nBut we may have issues if there is an interaction!\n\n \n\nThis is why I suggested we keep an eye out for cell counts below 10 in our lab!\n\n\n \n\nIf you see a big coefficient estimate with a big standard deviation for a specific category or interaction…\n \n\n…this may mean that a low cell count for that category is causing you issues!"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-count-summary",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-count-summary",
    "title": "Lesson 13: Numerical Problems",
    "section": "Zero cell count: summary",
    "text": "Zero cell count: summary\nMy suggestion is to try possible solutions in this order\n\nFor group with zero cell count, see if there is an adjacent group that makes sense to combine it with\n\n \n\nIf that does not make sense (or obscures your data) AND your data has an inherent order, then you can try treating it as continuous.\n\n \n\nRemove the zero count group and all the observations in it (not a very good solution)\n\n \n\nAdd a half count to each cell (only works for a single predictor)"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#poll-everywhere-question-1",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#poll-everywhere-question-1",
    "title": "Lesson 13: Numerical Problems",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#complete-separation",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#complete-separation",
    "title": "Lesson 13: Numerical Problems",
    "section": "Complete Separation",
    "text": "Complete Separation\n\nComplete separation: occurs when a collection of the covariates completely separates the outcome groups\n\nExample: Outcome is “gets senior discount at iHop” and the only covariate you measure is age\nAge will completely separate the outcome\nNo overlap in distribution of covariates between two outcome groups\n\n\n \n\nProblem: the maximum likelihood estimates do not exist\n\nLikelihood function is monotone\nIn order to have finite maximum likelihood estimates we must have some overlap in the distribution of the covariates in the model"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#poll-everywhere-question-2",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#poll-everywhere-question-2",
    "title": "Lesson 13: Numerical Problems",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-example-13",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-example-13",
    "title": "Lesson 13: Numerical Problems",
    "section": "Complete Separation: example (1/3)",
    "text": "Complete Separation: example (1/3)\n\n\n\nWe get a warning when we have complete separation\n\n\ny = c(0,0,0,0,1,1,1,1)\nx1 = c(1,2,3,3,5,6,10,11)\nx2 = c(3,2,-1,-1,2,4,1,0)\nex3 = data.frame(outcome = y, x1 = x1, x2= x2)\nex3\n\n\n\n\n  outcome x1 x2\n1       0  1  3\n2       0  2  2\n3       0  3 -1\n4       0  3 -1\n5       1  5  2\n6       1  6  4\n7       1 10  1\n8       1 11  0\n\n\n\n \n\nm1 = glm(outcome ~ x1 + x2, data = ex3, family=binomial)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\n\nOutcomes of 0 and 1 are completely separated by x2\n\nIf x2 &gt; 4 then outcome is 1\nIf x2 &lt; 4 then outcome is 0"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-example-23",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-example-23",
    "title": "Lesson 13: Numerical Problems",
    "section": "Complete Separation: example (2/3)",
    "text": "Complete Separation: example (2/3)\n\n\n \nCoefficient estimates:\n\ntidy(m1, conf.int=T) %&gt;% gt() %&gt;% \n  tab_options(table.font.size = 35) %&gt;%\n  fmt_number(decimals = 2)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n−66.10\n183,471.72\n0.00\n1.00\n−10,644.72\n10,512.52\n\n\nx1\n15.29\n27,362.84\n0.00\n1.00\n−3,122.69\nNA\n\n\nx2\n6.24\n81,543.72\n0.00\n1.00\n−12,797.28\nNA"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-example-33",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-example-33",
    "title": "Lesson 13: Numerical Problems",
    "section": "Complete Separation: example (3/3)",
    "text": "Complete Separation: example (3/3)\n\n\n \nCoefficient estimates:\n\ntidy(m1, conf.int=T) %&gt;% gt() %&gt;% \n  tab_options(table.font.size = 35) %&gt;%\n  fmt_number(decimals = 2)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n−66.10\n183,471.72\n0.00\n1.00\n−10,644.72\n10,512.52\n\n\nx1\n15.29\n27,362.84\n0.00\n1.00\n−3,122.69\nNA\n\n\nx2\n6.24\n81,543.72\n0.00\n1.00\n−12,797.28\nNA\n\n\n\n\n\n\n\n\n \n\n\nCoefficient estimate of x1 is large\nStandard error of x1’s coefficient is large\nBut also the coefficients and standard errors for the intercept and x2 are large!"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-1",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-1",
    "title": "Lesson 13: Numerical Problems",
    "section": "Complete Separation",
    "text": "Complete Separation\n\nThe occurrence of complete separation in practice depends on\n\nSample size\nNumber of subjects with the outcome present\nNumber of variables included in the model\n\n\n \n\nExample: 25 observations and only 5 have “success” outcome\n\n1 variable in model may not lead to complete separation\nMore variables = more dimensions that can completely separate the observations\n\n\n \n\nIn most cases, the occurrence of complete separation is not bad for clinical importance\n\nBut rather a numerical coincidence that causing problem for model fitting"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#poll-everywhere-question-3",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#poll-everywhere-question-3",
    "title": "Lesson 13: Numerical Problems",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-ways-to-address-issue",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-ways-to-address-issue",
    "title": "Lesson 13: Numerical Problems",
    "section": "Complete Separation: Ways to address issue",
    "text": "Complete Separation: Ways to address issue\n\nCollapse categorical variables in a meaningful way\n\nEasiest and best if stat methods are restricted (common for collaborations)\n\n\n \n\nExclude x1 from the model\n\nNot ideal because this could lead to biased estimates for the other predicted variables in the model\n\n\n \n\nFirth logistic regression\n\nUses penalized likelihood estimation method\nBasically takes the likelihood (that has no maximum) and adds a penalty that makes the MLE estimatable"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-firth-logistic-regression",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-firth-logistic-regression",
    "title": "Lesson 13: Numerical Problems",
    "section": "Complete Separation: Firth logistic regression",
    "text": "Complete Separation: Firth logistic regression\n\nlibrary(logistf)\nm1_f = logistf(outcome ~ x1 + x2, data = ex3, family=binomial)\nsummary(m1_f) # Cannot use tidy on this :(\n\nlogistf(formula = outcome ~ x1 + x2, data = ex3, family = binomial)\n\nModel fitted by Penalized ML\nCoefficients:\n                  coef  se(coef)   lower 0.95 upper 0.95     Chisq          p\n(Intercept) -2.9748898 1.7244237 -15.47721665 -0.1208883 4.2179522 0.03999841\nx1           0.4908484 0.2745754   0.05268216  2.1275832 5.0225056 0.02501994\nx2           0.4313732 0.4988396  -0.65793078  4.4758930 0.7807099 0.37692411\n            method\n(Intercept)      2\nx1               2\nx2               2\n\nMethod: 1-Wald, 2-Profile penalized log-likelihood, 3-None\n\nLikelihood ratio test=5.505687 on 2 df, p=0.06374636, n=8\nWald test = 3.624899 on 2 df, p = 0.1632538"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity",
    "title": "Lesson 13: Numerical Problems",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nMulticollinearity happens when one or more of the covariates in a model can be predicted from other covariates in the same model\n\n \n\nThis will cause unreliable coefficient estimates for some covariates in logistic regression, as in an ordinary linear regression\n\n \n\nLooking at correlations among pairs of variables is helpful but not enough to identify multicollinearity problem\n\nBecause multicollinearity problems may involve relationships among more than two covariates"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-example-14",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-example-14",
    "title": "Lesson 13: Numerical Problems",
    "section": "Multicollinearity: example (1/4)",
    "text": "Multicollinearity: example (1/4)\n\nTable below is a simulated data with\n\n\\(x_1 \\sim \\text{Normal}(0,1)\\)\n\\(x_2 = x_1 + \\text{Uniform}(0,0.1)\\)\n\\(x_3 = 1 + \\text{Uniform}(0, 0.01)\\)\n\nTherefore, \\(x_1\\) and \\(x_2\\) are highly correlated, and \\(x_3\\) is nearly collinear with the constant term"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-example-24",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-example-24",
    "title": "Lesson 13: Numerical Problems",
    "section": "Multicollinearity: example (2/4)",
    "text": "Multicollinearity: example (2/4)\n\nFour logistic regression models using data in the previous slide\nConsequence of multicollinearity: large coefficient estimates and/or standard errors"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-example-34",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-example-34",
    "title": "Lesson 13: Numerical Problems",
    "section": "Multicollinearity: example (3/4)",
    "text": "Multicollinearity: example (3/4)\n\nFour logistic regression models using data in the previous slide\nConsequence of multicollinearity: large coefficient estimates and/or standard errors"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-example-44",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-example-44",
    "title": "Lesson 13: Numerical Problems",
    "section": "Multicollinearity: example (4/4)",
    "text": "Multicollinearity: example (4/4)\n\nFour logistic regression models using data in the previous slide\nConsequence of multicollinearity: large coefficient estimates and/or standard errors"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-how-to-detect",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-how-to-detect",
    "title": "Lesson 13: Numerical Problems",
    "section": "Multicollinearity: how to detect",
    "text": "Multicollinearity: how to detect\n\nMulticollinearity only involves the covariates\n\nNo specific issues to logistic regression (vs. linear regression)\nTechniques from 512/612 work well for logistic regression model\n\n\n \n\nIn more complicated dataset/analysis, we may not be able to detect multicollinearity using the coefficient estimates/SE\n\n \n\nVariance inflation factor (VIF) approach: well-known approach to detect multicollinearity"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#variance-inflation-factor-vif-approach",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#variance-inflation-factor-vif-approach",
    "title": "Lesson 13: Numerical Problems",
    "section": "Variance Inflation Factor (VIF) Approach",
    "text": "Variance Inflation Factor (VIF) Approach\n\nComputed by regressing each variable on all the other explanatory variables\n\nFor example: \\(E(x_1│x_2,x_3,…)=\\alpha_0+\\alpha_1 x_2+\\alpha_2 x_3\\)\n\nCalculate the coefficient of determination, \\(R^2\\)\n\nProportion of the variation in \\(x_1\\) that is predicted from \\(x_2\\), \\(x_3\\),… \\[VIF = \\dfrac{1}{1=R^2}\\]\n\nEach covariate has its own VIF computed\nGet worried for multicollinearity if VIF &gt; 10\nSometimes VIF approach may miss serious multicollinearity\n\nSame multicollinearity we wish to detect using VIF can cause numerical problems in reliably estimating \\(R^2\\)"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-ways-to-address-the-issue",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-ways-to-address-the-issue",
    "title": "Lesson 13: Numerical Problems",
    "section": "Multicollinearity: Ways to address the issue",
    "text": "Multicollinearity: Ways to address the issue\n\nExclude the redundant variable from the model\nScaling and centering variables\n\nWhen you have transformed a continuous variable\n\nOther modeling approach (outside scope of this class)\n\nRidge regression\nPrinciple component analysis\n\n\n \n\nPlease take a look at the BSTA 512/612 lesson that included multicollinearity"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/13_Numeric_problems.html#poll-everywhere-question-4",
    "href": "lessons/13_Numeric_Problems/13_Numeric_problems.html#poll-everywhere-question-4",
    "title": "Lesson 13: Numerical Problems",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lessons/01_Intro/01_Intro.html#course-learning-objectives",
    "href": "lessons/01_Intro/01_Intro.html#course-learning-objectives",
    "title": "Lesson 1: Welcome!",
    "section": "Course Learning Objectives",
    "text": "Course Learning Objectives\nAt the end of this course, students should be able to…\n\nApply and interpret some hypothesis-testing procedures for two-way and three-way contingency tables\nCompute and interpret measures of association for binary and ordinal data.\nCalculate and correctly interpret odds ratios using logistic regression, make comparison across groups and examine relationship between binary outcome and predictor variables.\nApply appropriate model-building strategies for logistic regression. Effectively use statistical computing packages for contingency table and logistic regression procedures.\nPerform Poisson regression analysis using count data and interpret model estimates, make comparison across groups and examine relationship between outcome and predictor variables.\nCoherently summarize methods and results of data analyses, and discuss in context of original health-related research questions to audiences with varied statistical background."
  },
  {
    "objectID": "lessons/01_Intro/01_Intro.html#new-way-to-recordview-lectures",
    "href": "lessons/01_Intro/01_Intro.html#new-way-to-recordview-lectures",
    "title": "Lesson 1: Welcome!",
    "section": "New way to record/view lectures",
    "text": "New way to record/view lectures\n\nI will be using Echo360 to automatically record time from 1-3pm in this room!\nIt will be LIVE\n\nSo you can watch in real time, but won’t be able to interact with us\nMight have a 10 second lag\nNo need to tell me if you cannot make class\n\nHere is the link to the Echo site! I’ll keep posting it on the weekly pages\n\nEveryone should be added! Please check that you have access.\n\nI don’t have to post anything after class, so no issues with me forgetting to post it"
  },
  {
    "objectID": "lessons/01_Intro/01_Intro.html#new-exit-ticket-style",
    "href": "lessons/01_Intro/01_Intro.html#new-exit-ticket-style",
    "title": "Lesson 1: Welcome!",
    "section": "New Exit ticket style",
    "text": "New Exit ticket style\n\nAll the questions are optional\nBut still open the link and submit!\n4 of the exit tickets are dropped!"
  },
  {
    "objectID": "lessons/01_Intro/01_Intro.html#some-important-tasks",
    "href": "lessons/01_Intro/01_Intro.html#some-important-tasks",
    "title": "Lesson 1: Welcome!",
    "section": "Some important tasks",
    "text": "Some important tasks\n\nStar the class website: https://nwakim.github.io/S25_BSTA_513/\nComplete the when2meet for office hours\n\nIf your calendar feels set, take 5 minutes to fill this out now!\nComplete by Thursday at 11pm!!!\n\nHighly suggest that you make an appointment with a learning specialist through Student Academic Support Services!\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "lessons/01_Intro/01_Intro.html#syllabus",
    "href": "lessons/01_Intro/01_Intro.html#syllabus",
    "title": "Lesson 1: Welcome!",
    "section": "Syllabus",
    "text": "Syllabus\n\nNot many changes from last quarters syllabus\nChanges from last quarter\n\nLab discussion days\nQuizzes\nNo midterm feedback"
  },
  {
    "objectID": "lessons/01_Intro/01_Intro.html#quizzes",
    "href": "lessons/01_Intro/01_Intro.html#quizzes",
    "title": "Lesson 1: Welcome!",
    "section": "Quizzes",
    "text": "Quizzes\n\nIn BSTA 512/612, I removed the quizzes that I implemented in W24\nI noticed there were some key learning objectives that many of us were not meeting in the project\n\nNot your fault\n\nLeading me to implement some new assessment practices (quizzes and lab discussions)\nI think quizzes are a helpful way to embed a checkpoint for understanding\nQuizzes will NOT be in class\n\nQuizzes will open at 3pm on Wednesday and close at 11pm on Sunday\nI’m open to changing the exact timing of this, so let me know in the exit ticket!"
  },
  {
    "objectID": "lessons/01_Intro/01_Intro.html#homework-grading",
    "href": "lessons/01_Intro/01_Intro.html#homework-grading",
    "title": "Lesson 1: Welcome!",
    "section": "Homework grading",
    "text": "Homework grading\n\nSlightly new grading\nNow, need to turn in 50% of the homework completed to get check mark\nNoticed that demonstrating understanding in the project was correlated with completing the homework*\n\nNo formal analysis was done on this\nAnd there may be confounders like time available to commit to this class in general\n\nEither way, I think practice is the most important tool for learning\n\nSo I want to us to practice the work, but I’m trying to balance this with added stress"
  },
  {
    "objectID": "lessons/01_Intro/01_Intro.html#homework-grading-in-syllabus",
    "href": "lessons/01_Intro/01_Intro.html#homework-grading-in-syllabus",
    "title": "Lesson 1: Welcome!",
    "section": "Homework grading in syllabus",
    "text": "Homework grading in syllabus"
  },
  {
    "objectID": "lessons/01_Intro/01_Intro.html#how-to-print-slides",
    "href": "lessons/01_Intro/01_Intro.html#how-to-print-slides",
    "title": "Lesson 1: Welcome!",
    "section": "How to print slides",
    "text": "How to print slides\n\nAnyone have issues with this?\n\nI can show how to do it in Chrome and Safari\n\nInstructions on Quarto page"
  },
  {
    "objectID": "lessons/01_Intro/01_Intro.html#lets-take-10-minutes-for-student-survey",
    "href": "lessons/01_Intro/01_Intro.html#lets-take-10-minutes-for-student-survey",
    "title": "Lesson 1: Welcome!",
    "section": "Let’s take 10 minutes for Student Survey",
    "text": "Let’s take 10 minutes for Student Survey\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n−+\n11:00"
  },
  {
    "objectID": "lessons/01_Intro/01_Intro.html#now-we-take-a-10-minute-break",
    "href": "lessons/01_Intro/01_Intro.html#now-we-take-a-10-minute-break",
    "title": "Lesson 1: Welcome!",
    "section": "Now we take a 10 minute break!",
    "text": "Now we take a 10 minute break!\n\n\n\n−+\n10:00"
  },
  {
    "objectID": "lessons/01_Intro/01_Intro.html#assessment-breakdown",
    "href": "lessons/01_Intro/01_Intro.html#assessment-breakdown",
    "title": "Lesson 1: Welcome!",
    "section": "Assessment Breakdown",
    "text": "Assessment Breakdown\n\nWe will have quizzes and required in-person lab discussions (on schedule)\n\n\n\n\n\n\n\n\n\n\n\nCourse activity\nType of Assessment\nDue Date\nPercentage of final grade (BSTA 513)\nPercentage of final grade (BSTA 613)\n\n\nHomework\nFormative\nEvery 1-2 weeks\n30%\n25%\n\n\nQuizzes\nSummative\n4/27, 5/18, 6/1\n25%\n25%\n\n\nProject Labs\nFormative/Summative\nEvery 2-3 weeks\n25%\n25%\n\n\nLab Discussion + Poster Day Participation\nN/A\nEvery 2-3 weeks\n5%\n5%\n\n\nProject Poster\nSummative\n6/13\n10%\n10%\n\n\nExit tickets (Attendance)\nN/A\nTwice Weekly\n5%\n5%\n\n\n613 Readings\nFormative\nApprox. every other week\n0%\n5%"
  },
  {
    "objectID": "lessons/01_Intro/01_Intro.html#now-we-take-a-5-minute-break",
    "href": "lessons/01_Intro/01_Intro.html#now-we-take-a-5-minute-break",
    "title": "Lesson 1: Welcome!",
    "section": "Now we take a 5 minute break!",
    "text": "Now we take a 5 minute break!\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "lessons/01_Intro/01_Intro.html#lab-discussion-poster-day-participation",
    "href": "lessons/01_Intro/01_Intro.html#lab-discussion-poster-day-participation",
    "title": "Lesson 1: Welcome!",
    "section": "Lab Discussion + Poster Day Participation",
    "text": "Lab Discussion + Poster Day Participation\n\nWe will have 5 classes with required in-person attendance\n\n4 lab discussion days\n1 poster day\n\nLab discussions will either take the full or half class time\n\nThey will always follow the due date for a lab\n\nYou will spend the time in a pair or group to discuss what you did in your lab\nIf you have extenuating circumstances, you can complete a make-up assignment for credit (Syllabus)"
  },
  {
    "objectID": "lessons/01_Intro/01_Intro_key_info.html",
    "href": "lessons/01_Intro/01_Intro_key_info.html",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "For National Public Health Week, join us for an exclusive tour of the School of Public Health’s only clinical laboratory, the Center for Infectious Disease (CIDS) on Tuesday, April 8 from 3:00 - 3:45pm.\nGetting a peek into the work of CIDS, you’ll witness first hand how applied public health research shapes real-world solutions and directly supports community well-being. Their expert-led research delves into the depths of pharmacoepidemiology and rheumatologic diseases, enhancing drug safety and efficacy for the broader population. Join us for a tour to expand your academic horizons, ask questions of the research team on what their day-to-day work and career path looks like, and to see what options you have as a pivotal researcher who is advancing health equity and enhancing community health outcomes."
  },
  {
    "objectID": "lessons/01_Intro/01_Intro_key_info.html#announcements",
    "href": "lessons/01_Intro/01_Intro_key_info.html#announcements",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "For National Public Health Week, join us for an exclusive tour of the School of Public Health’s only clinical laboratory, the Center for Infectious Disease (CIDS) on Tuesday, April 8 from 3:00 - 3:45pm.\nGetting a peek into the work of CIDS, you’ll witness first hand how applied public health research shapes real-world solutions and directly supports community well-being. Their expert-led research delves into the depths of pharmacoepidemiology and rheumatologic diseases, enhancing drug safety and efficacy for the broader population. Join us for a tour to expand your academic horizons, ask questions of the research team on what their day-to-day work and career path looks like, and to see what options you have as a pivotal researcher who is advancing health equity and enhancing community health outcomes."
  },
  {
    "objectID": "lessons/01_Intro/01_Intro_key_info.html#key-dates",
    "href": "lessons/01_Intro/01_Intro_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates\n\nLab 1 due 4/11"
  },
  {
    "objectID": "lessons/01_Intro/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "href": "lessons/01_Intro/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "title": "Muddy Points",
    "section": "Muddy Points from Spring 2024",
    "text": "Muddy Points from Spring 2024"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA_muddy_points.html#muddy-points-from-spring-2024",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA_muddy_points.html#muddy-points-from-spring-2024",
    "title": "Muddy Points",
    "section": "Muddy Points from Spring 2024",
    "text": "Muddy Points from Spring 2024\n\n1. Why does the test of trend treat ordinal variables as quantitative rather than qualitative?\nWhen we treat something as qualitative, we can only look at differences between groups. This means we cannot rank the groups and look at the change across groups. By treating the ordinal variables as quantitative, we can look at the change as we move from one group to another (and over all the ranked categories).\n\n\n2. Organizing the tests in a tree\nHere’s a organizational tree that I took from Meike and expanded:"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/02_Intro_CDA_key_info.html#key-dates",
    "href": "lessons/14_Model_diagnostics/02_Intro_CDA_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/14_Model_diagnostics/02_Intro_CDA_key_info.html#last-class",
    "href": "lessons/14_Model_diagnostics/02_Intro_CDA_key_info.html#last-class",
    "title": "Key Info and Announcements",
    "section": "Last class",
    "text": "Last class"
  },
  {
    "objectID": "lessons/09_Missing_data/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "href": "lessons/09_Missing_data/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "title": "Muddy Points",
    "section": "Muddy Points from Spring 2024",
    "text": "Muddy Points from Spring 2024"
  },
  {
    "objectID": "lessons/05_Simple_logistic_reg/week_03_sched.html#resources",
    "href": "lessons/05_Simple_logistic_reg/week_03_sched.html#resources",
    "title": "Week 3",
    "section": "Resources",
    "text": "Resources\n\n\n\nLesson\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n5\nSimple Logistic Regression\n\n\n\n\n\n6\nTests for GLMs using Likelihood function\n\n\n\n\n\n\nIf you ever have trouble with the above links to the videos, this Echo360 link should take you to our class page."
  },
  {
    "objectID": "lessons/05_Simple_logistic_reg/week_03_sched.html#on-the-horizon",
    "href": "lessons/05_Simple_logistic_reg/week_03_sched.html#on-the-horizon",
    "title": "Week 3",
    "section": "On the Horizon",
    "text": "On the Horizon\n\nLab 1 due this Thursday (4/18)\nQuiz 1 opens on Monday, 4/22, at 2pm and will close on Wednesday, 4/24, at 1pm"
  },
  {
    "objectID": "lessons/05_Simple_logistic_reg/week_03_sched.html#class-exit-tickets",
    "href": "lessons/05_Simple_logistic_reg/week_03_sched.html#class-exit-tickets",
    "title": "Week 3",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (4/15)\n Wednesday (4/17)"
  },
  {
    "objectID": "lessons/05_Simple_logistic_reg/week_03_sched.html#announcements",
    "href": "lessons/05_Simple_logistic_reg/week_03_sched.html#announcements",
    "title": "Week 3",
    "section": "Announcements",
    "text": "Announcements\n\nMonday 4/15\n\nI am trying to stay on track of the Exit tickets this quarter\n\nThat may mean you have a 0 in your gradebook\nAs long as you complete the exit ticket within the 7 days, I will change the 0 to a 1\nI plan to have a scheduled block on Fridays to check them\n\n\n\n\nWednesday 4/17\n\nQuiz 1 info"
  },
  {
    "objectID": "lessons/05_Simple_logistic_reg/week_03_sched.html#muddiest-points",
    "href": "lessons/05_Simple_logistic_reg/week_03_sched.html#muddiest-points",
    "title": "Week 3",
    "section": "Muddiest Points",
    "text": "Muddiest Points\n\n1. Not entirely sure I understand what IRLS is about\nFair enough. It’s a little confusing. IWLS is an iterative solving technique that let’s us solve the coefficient estimates ( \\(\\beta_0\\) , \\(\\beta_1\\)) without solving the equations theoretically.\nWe start with an educated guess of the estimates, put them into the likelihood, and calculate the likelihood. Then we update the estimates using some complicated math, put them into the likelihood, and calculate the likelihood again. We compare the two likelihoods, and if the likelihood increases, then we keep going. We stop when the increase in likelihood between iterations is small. This means we are at or very close to the maximum likelihood.\n\n\n2. Link functions\nYes! Link functions are the important transformations we need to make to our outcome in order to connect them to our perdictors/covariates. Specifically, it’s the transformation we make to our mean/expected value.\nThe same link function can be used different types of outcomes. And here’s a few examples:\n\nContinuous data: identity\nBinary: logit, log\nCount/Poisson: log\n\nOur goal with link functions is to put our outcome on a flexible range so that any range of covariates can be mapped to it with coefficients. So think about trying to map age onto a 0 or 1… We can’t come up with an equation like \\(\\beta_0 + \\beta_1 Age\\) that perfectly maps to only 0’s and 1’s.\n\n\n3. Is GLM the umbrella over the other functions? The 4 functions all use different distributions, yes?\nGLM is the umbrella term for different types of regression! Not all types of regression have different outcome distributions. For example, a binary outcome can be used in logistic regression with the logit link or log-binomial regression with the log link.\n\n\n4. What would you need to change in your model to reduce a high IRLS number? As I understand it from the lecture, a high number suggests convergence but it appeared like something unfavorable even though a model that converges might be closer to maximum likelihood or maybe the distance to maximum likelihood\nA high number suggests that the model did NOT converge! Thus, we did not land on an estimate close to our maximum likelihood. You can think of the IRLS number as the number of iterations it is taking to find the maximum likelihood estimate (MLE). If it takes too many iterations, then it just stops without finding the MLE.\n\n\n5. We’re using linear vs logistic, but which are we focusing on? Regarding linear, how does linear used in categorical differ from continuous?\nWe are focusing on logistic! We cannot use linear regression on our binary outcomes anymore. When I say “linear” mapping I mean the mapping between our covariates and the transformed mean outcome using the link function.\n\n\n6. By the end of class (Lesson 6) my understanding is that the saturated model likelihood is the same between the two models being compared, right?\nYep!!\n\n\n7. The differences between each test and when to use them.\nIn terms of what each test is measuring:\n\nThe Wald test measures the distance between two potential values of \\(\\beta\\). One under the null and one under the alternative. The further they are from each other, the more evidence we have that they are different.\n\nThe Wald test approximates the differences in the likelihood function, but we do not actually compare the likelihoods under the null vs. alternative. We are only comparing the difference in the \\(\\beta\\) value, that is a reasonable approximation of the difference in the likelihood.\n\nThe Score test measures how close the tangent line of the likelihood function is to 0 (under the null). If it is close to 0 under the null, this indicates that our MLE of \\(\\beta\\) is not far from 0. Again, this is no a direct comparison of the likelihoods, but only an approximation of the difference.\nThe likelihood ratio test measures the difference in the log-likelihoods. This is a direct comparison of likelihoods, and is not an approximation!\n\nThus, we compare the likelihoods (horizontally, as someone asked) because we are making direct comparisons between the likelihood under the null and under the alternative."
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA_key_info.html#key-dates",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA_key_info.html#last-class",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA_key_info.html#last-class",
    "title": "Key Info and Announcements",
    "section": "Last class",
    "text": "Last class"
  },
  {
    "objectID": "lessons/06_Tests_GLMs/02_Intro_CDA_key_info.html#key-dates",
    "href": "lessons/06_Tests_GLMs/02_Intro_CDA_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/06_Tests_GLMs/02_Intro_CDA_key_info.html#last-class",
    "href": "lessons/06_Tests_GLMs/02_Intro_CDA_key_info.html#last-class",
    "title": "Key Info and Announcements",
    "section": "Last class",
    "text": "Last class"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/02_Intro_CDA_key_info.html#key-dates",
    "href": "lessons/13_Numeric_Problems/02_Intro_CDA_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/02_Intro_CDA_key_info.html#last-class",
    "href": "lessons/13_Numeric_Problems/02_Intro_CDA_key_info.html#last-class",
    "title": "Key Info and Announcements",
    "section": "Last class",
    "text": "Last class"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "href": "lessons/13_Numeric_Problems/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "title": "Muddy Points",
    "section": "Muddy Points from Spring 2024",
    "text": "Muddy Points from Spring 2024"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/02_Intro_CDA_key_info.html#key-dates",
    "href": "lessons/10_Multiple_logistic_regression/02_Intro_CDA_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/02_Intro_CDA_key_info.html#last-class",
    "href": "lessons/10_Multiple_logistic_regression/02_Intro_CDA_key_info.html#last-class",
    "title": "Key Info and Announcements",
    "section": "Last class",
    "text": "Last class"
  },
  {
    "objectID": "lessons/08_Interpretations_SLR/week_04_sched.html#resources",
    "href": "lessons/08_Interpretations_SLR/week_04_sched.html#resources",
    "title": "Week 4",
    "section": "Resources",
    "text": "Resources\n\n\n\nLesson\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n7\nPredictions and Visualizations in Simple Logistic Regression\n\n\n\n\n\n8\nInterpretations and Visualizations of Odds Ratios"
  },
  {
    "objectID": "lessons/08_Interpretations_SLR/week_04_sched.html#on-the-horizon",
    "href": "lessons/08_Interpretations_SLR/week_04_sched.html#on-the-horizon",
    "title": "Week 4",
    "section": "On the Horizon",
    "text": "On the Horizon\n\nHomework 2 due this Thursday"
  },
  {
    "objectID": "lessons/08_Interpretations_SLR/week_04_sched.html#class-exit-tickets",
    "href": "lessons/08_Interpretations_SLR/week_04_sched.html#class-exit-tickets",
    "title": "Week 4",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (4/22)\n\nMonday Exit ticket will not be graded bc of quiz\n\n Wednesday (4/24)"
  },
  {
    "objectID": "lessons/08_Interpretations_SLR/week_04_sched.html#announcements",
    "href": "lessons/08_Interpretations_SLR/week_04_sched.html#announcements",
    "title": "Week 4",
    "section": "Announcements",
    "text": "Announcements\nWednesday 4/24\n\nHave you all seen this??? Page on Basic Needs for students\n\nSPH Emergency funds\nCARE program\nCommittee for Improving Student Food Security\n\nLab 2 is up!\n\nFrom Lab 1:\n\nDO NOT USE ANY_HARDSHIP or MULT_HARDSHIP as your main variable\n\nThese are constructed from food insecurity variable\nSee the User Guide in the downloaded ICPSR folder\n\n\n\nQuiz 1 should be in!\nLab 1 feedback still in progress\nReview last quarter’s project\n\nOn Monday we will take 15 minutes to discuss changes to the project report instructions\nI will bring the learning objectives that I want to assess\nWe can rework or scrap parts of the report that do not assess these learning objectives\nAs part of the exit ticket, I will ask about your preferences as well"
  },
  {
    "objectID": "lessons/08_Interpretations_SLR/week_04_sched.html#muddiest-points",
    "href": "lessons/08_Interpretations_SLR/week_04_sched.html#muddiest-points",
    "title": "Week 4",
    "section": "Muddiest Points",
    "text": "Muddiest Points\nNone?? Wowza!"
  },
  {
    "objectID": "lessons/08_Interpretations_SLR/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "href": "lessons/08_Interpretations_SLR/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "title": "Muddy Points",
    "section": "Muddy Points from Spring 2024",
    "text": "Muddy Points from Spring 2024"
  },
  {
    "objectID": "lessons/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree_key_info.html#key-dates",
    "href": "lessons/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree_key_info.html#last-class",
    "href": "lessons/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree_key_info.html#last-class",
    "title": "Key Info and Announcements",
    "section": "Last class",
    "text": "Last class"
  },
  {
    "objectID": "lessons/07_Pred_Viz/week_04_sched.html#resources",
    "href": "lessons/07_Pred_Viz/week_04_sched.html#resources",
    "title": "Week 4",
    "section": "Resources",
    "text": "Resources\n\n\n\nLesson\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n7\nPredictions and Visualizations in Simple Logistic Regression\n\n\n\n\n\n8\nInterpretations and Visualizations of Odds Ratios"
  },
  {
    "objectID": "lessons/07_Pred_Viz/week_04_sched.html#on-the-horizon",
    "href": "lessons/07_Pred_Viz/week_04_sched.html#on-the-horizon",
    "title": "Week 4",
    "section": "On the Horizon",
    "text": "On the Horizon\n\nHomework 2 due this Thursday"
  },
  {
    "objectID": "lessons/07_Pred_Viz/week_04_sched.html#class-exit-tickets",
    "href": "lessons/07_Pred_Viz/week_04_sched.html#class-exit-tickets",
    "title": "Week 4",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (4/22)\n\nMonday Exit ticket will not be graded bc of quiz\n\n Wednesday (4/24)"
  },
  {
    "objectID": "lessons/07_Pred_Viz/week_04_sched.html#announcements",
    "href": "lessons/07_Pred_Viz/week_04_sched.html#announcements",
    "title": "Week 4",
    "section": "Announcements",
    "text": "Announcements\nWednesday 4/24\n\nHave you all seen this??? Page on Basic Needs for students\n\nSPH Emergency funds\nCARE program\nCommittee for Improving Student Food Security\n\nLab 2 is up!\n\nFrom Lab 1:\n\nDO NOT USE ANY_HARDSHIP or MULT_HARDSHIP as your main variable\n\nThese are constructed from food insecurity variable\nSee the User Guide in the downloaded ICPSR folder\n\n\n\nQuiz 1 should be in!\nLab 1 feedback still in progress\nReview last quarter’s project\n\nOn Monday we will take 15 minutes to discuss changes to the project report instructions\nI will bring the learning objectives that I want to assess\nWe can rework or scrap parts of the report that do not assess these learning objectives\nAs part of the exit ticket, I will ask about your preferences as well"
  },
  {
    "objectID": "lessons/07_Pred_Viz/week_04_sched.html#muddiest-points",
    "href": "lessons/07_Pred_Viz/week_04_sched.html#muddiest-points",
    "title": "Week 4",
    "section": "Muddiest Points",
    "text": "Muddiest Points\nNone?? Wowza!"
  },
  {
    "objectID": "lessons/07_Pred_Viz/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "href": "lessons/07_Pred_Viz/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "title": "Muddy Points",
    "section": "Muddy Points from Spring 2024",
    "text": "Muddy Points from Spring 2024"
  },
  {
    "objectID": "lessons/15_Model_building/02_Intro_CDA_key_info.html#key-dates",
    "href": "lessons/15_Model_building/02_Intro_CDA_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/15_Model_building/02_Intro_CDA_key_info.html#last-class",
    "href": "lessons/15_Model_building/02_Intro_CDA_key_info.html#last-class",
    "title": "Key Info and Announcements",
    "section": "Last class",
    "text": "Last class"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/week_02_sched.html#resources",
    "href": "lessons/03_Meas_Assoc_CT/week_02_sched.html#resources",
    "title": "Week 2",
    "section": "Resources",
    "text": "Resources\n\n\n\nLesson\n3\nTopic\nMeasurement of Association for Contingency Tables\nSlides | Annotated Slides | Recording | :==============================================================================================================================:+:=================================================================================================================================================:+:====================================================================================================================================================================================================================================================:+  |  | \n\n\n\n\n\n\n\n4\nMeasurements of Association and Agreement\n |  |  | | | | |  |\n\n\n\nIf you ever have trouble with the above links to the videos, this Echo360 link should take you to our class page.\nFor the slides, once they are opened, if you would like to print or save them as a PDF, the best way to do this is from a computer internet browser:\n\nClick on the icon with three horizontal bars on the bottom left of the browser.\nClick on “Tools” with the gear icon at the top of the sidebar.\nClick on “PDF Export Mode.”\nFrom there, you can print or save the PDF as you would normally from your internet browser.\n\nNote: this process does not work very well on an iPad."
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/week_02_sched.html#on-the-horizon",
    "href": "lessons/03_Meas_Assoc_CT/week_02_sched.html#on-the-horizon",
    "title": "Week 2",
    "section": "On the Horizon",
    "text": "On the Horizon\n\nHomework 1 due this Thursday!"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/week_02_sched.html#announcements",
    "href": "lessons/03_Meas_Assoc_CT/week_02_sched.html#announcements",
    "title": "Week 2",
    "section": "Announcements",
    "text": "Announcements\n\nMonday 4/8\n\nOffice hours!!\n\nTuesdays 5:30-7pm with Antara\nThursdays 3:30 - 5pm with Ariel\nFridays 2 - 3:30pm with Nicky\n\n\n\n\nWednesday 4/10\n\nEcho360: Let’s all double check that we can see the recordings\n\nLink to class site\n\nHomework question 5: no need to do LRT in the table\nLab 1 is up!!\nQuiz 1 decision\n\nOnline in Sakai\nWill open up on Monday at 2pm. You can chose to take it in the classroom or wait\nQuiz will close before class on Wednesday\nOpen book still\nPlease do not cheat\n\nIf I notice any unusual changes to quiz performance compared to last quarter then we will go back to the old way of giving quizzes\n\nMultiple choice with potentially some free response\n\nFor example: interpreting an OR would be divided into a multiple choice for the estimate, CI, and then writing a sentence to interpret the estimate"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/week_02_sched.html#class-exit-tickets",
    "href": "lessons/03_Meas_Assoc_CT/week_02_sched.html#class-exit-tickets",
    "title": "Week 2",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (4/8)\n Wednesday (4/10)"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/week_02_sched.html#muddiest-points",
    "href": "lessons/03_Meas_Assoc_CT/week_02_sched.html#muddiest-points",
    "title": "Week 2",
    "section": "Muddiest Points",
    "text": "Muddiest Points\n\n1. “times greater than” vs just “times” in interpretation\nI’ve seen it both ways. It comes down to more of an English language nuance, with what seems to be a long battle between viewpoints. Or maybe more accurately, I grammatically correct way to contruct the sentence, but with people understanding the meaning the “incorrect” way. I tend to be more lenient when it comes to grammar in this way, but maybe that’s because I have a general distaste when languages are rigid and don’t accommodate how people currently speak and write.\n\n\n2. For the relative risks poll everywhere question #2, how were they derived?\n\nFor #1 with Trt A’s risk as 0.01 (aka \\(risk_A=0.01\\)) and Trt B’s risk as 0.001 (aka \\(risk_B=0.01\\))\n\nThe risk ratio is: \\(\\widehat{RR} = \\dfrac{\\widehat{p}_1}{\\widehat{p}_2} = \\dfrac{risk_A}{risk_B} = \\dfrac{0.01}{0.001} = 10\\)\n\nFor #2 with Trt A’s risk as 0.41 (aka \\(risk_A=0.41\\)) and Trt B’s risk as 0.401 (aka \\(risk_B=0.401\\))\n\nThe risk ratio is: \\(\\widehat{RR} = \\dfrac{\\widehat{p}_1}{\\widehat{p}_2} = \\dfrac{risk_A}{risk_B} = \\dfrac{0.41}{0.401} = 1.02\\)\n\n\n\n\n3. Ranges that odds ratios can take (0, infinity) vs the ranges that risk ratios can take.\nYeah, so both can theoretically take on the range \\([0, \\infty)\\). Both are ratios, so we also have to think about the range of the denominator and numerator For relative risk, the numerator and denominator are probabilities that can only take values from 0 to 1. While for ORs, the denominator and numerator are odds that can be a range of values \\([0, \\infty)\\).\nThe main point I was trying to make was that once we observe one group’s proportion/probability, then RRs and ORs will differ in their potential range. Let’s say I observe the proportion fro group 1 and now know the numerator for RR and the odds in the numerator for OR. Because the RR has numerator and denominator that has ranges \\([0, 1]\\), if we know the proportion of group 1 (aka numerator value), then the ratio itself has a smaller range of values because the denominator can only be between 0 and 1. Because the OR has numerator and denominator that has ranges \\([0, \\infty)\\), if we know the proportion of group 1, then we do have a fixed numerator. However, the denominator can still be in \\([0, \\infty)\\).\n\n\n4. For the odds ratio equation that we reviewed today, is it different from ad/bc ? If they are different, when is it appropriate to use the equation we just reviewed over the other? p1/(1-p1) / p2/(1-p2)\nNope! These are the same! If you learned it that way, you can definitely use it when we are working with contingency tables. However, once we move into ORs from regression with multiple covariates, I think it’s better to understand the ORs and odds in terms of the probability/proportion.\n\n\n5. Not directly related to this class, but did we cover LRTs already? They’re mentioned in HW1 but aren’t in my notes.\nOops! Fixed it in the HW. No need to do anything with LRTs!\n\n\n6. In Epi, we were very strictly told that Odds Ratios were only to be used in one type of study. (I.e. we CAN NOT use them in cross-sectional and cohort studies) only case-control. So what is the application of attempting to utilize them, if each respective type of study already has a “pre-assigned” statistical method that suits it best?\nOdds ratios CAN be used in cross-sectional AND cohort studies. It is often an over-estimate of the relative risk in those situations, so it is important to interpret it ONLY as the odds ratio.\nEach respective study does not have a pre-assigned method. The only restriction is that relative risk cannot be used in case-control studies."
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT_key_info.html#key-dates",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates\n\nLab 1 due 4/11"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT_key_info.html#last-class",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT_key_info.html#last-class",
    "title": "Key Info and Announcements",
    "section": "Last class",
    "text": "Last class\n\nReviewed ways to estimate proportions (and differences) for categorical data\nReviewed different tests for comparing proportions and for association"
  },
  {
    "objectID": "lessons/11_Interactions/02_Intro_CDA_key_info.html#key-dates",
    "href": "lessons/11_Interactions/02_Intro_CDA_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/11_Interactions/02_Intro_CDA_key_info.html#last-class",
    "href": "lessons/11_Interactions/02_Intro_CDA_key_info.html#last-class",
    "title": "Key Info and Announcements",
    "section": "Last class",
    "text": "Last class"
  },
  {
    "objectID": "lessons/17_Wrap_up/week_10_sched.html",
    "href": "lessons/17_Wrap_up/week_10_sched.html",
    "title": "Week 10",
    "section": "",
    "text": "Room Locations for the week\n\n\n\nOn Monday, 6/3, we will be in RPV Room A (1217)"
  },
  {
    "objectID": "lessons/17_Wrap_up/week_10_sched.html#resources",
    "href": "lessons/17_Wrap_up/week_10_sched.html#resources",
    "title": "Week 10",
    "section": "Resources",
    "text": "Resources\n\n\n\nLesson\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n16\nPoisson Regression\n\n\n\n\n\n17\nOther types of categorical regressions!"
  },
  {
    "objectID": "lessons/17_Wrap_up/week_10_sched.html#on-the-horizon",
    "href": "lessons/17_Wrap_up/week_10_sched.html#on-the-horizon",
    "title": "Week 10",
    "section": "On the Horizon",
    "text": "On the Horizon\n\nQuiz 3 open 6/3 at 2pm\n\nCloses on 6/5 at 1pm\n\nLab 4 due yesterday!\nHW 5 due 6/6 at 11pm\nProject report due 6/13 at 11pm"
  },
  {
    "objectID": "lessons/17_Wrap_up/week_10_sched.html#announcements",
    "href": "lessons/17_Wrap_up/week_10_sched.html#announcements",
    "title": "Week 10",
    "section": "Announcements",
    "text": "Announcements\n\nMonday 6/3\n\nIn the last stretch of the project\n\nClass time next week dedicated to project report help\nI may have other time slots for project help next week. I just need to take a serious look at my calendar\n\nI’m attending a virtual stat ed conference, so I just need to balance meetings with conference attendance\n\n\nNo office hours this Wednesday 6/5\n#3 “The Last Bounce: Bunnies, Burritos, and DIY Bath Salts”\n\nWednesday, June 5th Noon-2pm\nStudent Success Center, 6th floor Vanport\n\n\n\n\nWednesday 6/5\n\nLast day of lecture!!\nQuiz 2: added 3 points to everyone’s grade for that one confusing question (I forget the number, maybe question 6?)\nLab 4\n\nYour starting variables should be the TEN from lab 2, not the 5 from Lab 3\ng-value for Hosmer-Lemeshow test\n\nPlease look at Lesson 12!! There is a note on how to pick the g-value when we have many samples!!\nSome of us are getting NA’s when we put the correct g\n\nDouble check that your observed values are in numeric form\n\n\nIf doing LASSO, make sure you describe the process!\n\nWe used LASSO regression with a penalty of 0.001 to identify important predictors. We used a single test and training split of 80% and 20%, respectively. Only main effects were considered in our LASSO regression.\nAlways think: what are the key pieces of information that someone else might need to recreate this method?\n\n\nThe class will end on June 14, 2024. All coursework is expected to be completed by June 16, 2024 at 11pm.\n\nI need to have grades in on June 21st. In order to grade fairly and thoroughly, I need the whole week to grade the project report and any late assignments.\n\nA word on project grading\n\nIn the final lab, I gave you the option to do LASSO regression and focus on prediction. I know this created some confusion since we mostly set up the project as a question of association in Lab 1-3. We can still interpret the odds ratios from LASSO regression. I will be fairly lenient if reports are confused between prediction and association aims. I will try to give feedback on it, but I will not penalize any minor confusions.\nAnother word: My process starts harsh. I want to give you as much feedback as possible, and this will also reflect in lower assigned scores. At this point, I put the report grades into Sakai. I check to see if anyone’s overall course letter grade goes down. If less than ~5 course grades go down, then I revisit their project reports. If their report fails to demonstrate the most important learning objectives from the course, then I will keep the lower grade. If they demonstrate an understanding of the most important learning objectives, then I will adjust their score to increase their course grade. If more than 5 grades go down, then I revisit everyone’s reports. I will make a class wide grade bump in all reports.\nThe most important learning objectives are: understanding when and what test is appropriate, and interpreting odds ratios (from main effects and interactions)\n\nProject: LASSO\n\nYou can take the finalized formula for LASSO and use it in glm()\n\nIn this case, use the test data to come up with predictive values (like AUC)\nYou can run it on the full dataset to get the coefficient estimates and other diagnostic information\n\n\nGuide on figures"
  },
  {
    "objectID": "lessons/17_Wrap_up/week_10_sched.html#class-exit-tickets",
    "href": "lessons/17_Wrap_up/week_10_sched.html#class-exit-tickets",
    "title": "Week 10",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (6/3)\n Wednesday (6/5)"
  },
  {
    "objectID": "lessons/17_Wrap_up/week_10_sched.html#muddiest-points",
    "href": "lessons/17_Wrap_up/week_10_sched.html#muddiest-points",
    "title": "Week 10",
    "section": "Muddiest Points",
    "text": "Muddiest Points"
  },
  {
    "objectID": "lessons/17_Wrap_up/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "href": "lessons/17_Wrap_up/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "title": "Muddy Points",
    "section": "Muddy Points from Spring 2024",
    "text": "Muddy Points from Spring 2024"
  },
  {
    "objectID": "lessons/16_Poisson_regression/02_Intro_CDA_key_info.html#key-dates",
    "href": "lessons/16_Poisson_regression/02_Intro_CDA_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/16_Poisson_regression/02_Intro_CDA_key_info.html#last-class",
    "href": "lessons/16_Poisson_regression/02_Intro_CDA_key_info.html#last-class",
    "title": "Key Info and Announcements",
    "section": "Last class",
    "text": "Last class"
  },
  {
    "objectID": "lessons/16_Poisson_regression/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "href": "lessons/16_Poisson_regression/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "title": "Muddy Points",
    "section": "Muddy Points from Spring 2024",
    "text": "Muddy Points from Spring 2024"
  },
  {
    "objectID": "lessons/16_Poisson_regression/week_10_sched.html",
    "href": "lessons/16_Poisson_regression/week_10_sched.html",
    "title": "Week 10",
    "section": "",
    "text": "Room Locations for the week\n\n\n\nOn Monday, 6/3, we will be in RPV Room A (1217)"
  },
  {
    "objectID": "lessons/16_Poisson_regression/week_10_sched.html#resources",
    "href": "lessons/16_Poisson_regression/week_10_sched.html#resources",
    "title": "Week 10",
    "section": "Resources",
    "text": "Resources\n\n\n\nLesson\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n16\nPoisson Regression\n\n\n\n\n\n17\nOther types of categorical regressions!"
  },
  {
    "objectID": "lessons/16_Poisson_regression/week_10_sched.html#on-the-horizon",
    "href": "lessons/16_Poisson_regression/week_10_sched.html#on-the-horizon",
    "title": "Week 10",
    "section": "On the Horizon",
    "text": "On the Horizon\n\nQuiz 3 open 6/3 at 2pm\n\nCloses on 6/5 at 1pm\n\nLab 4 due yesterday!\nHW 5 due 6/6 at 11pm\nProject report due 6/13 at 11pm"
  },
  {
    "objectID": "lessons/16_Poisson_regression/week_10_sched.html#announcements",
    "href": "lessons/16_Poisson_regression/week_10_sched.html#announcements",
    "title": "Week 10",
    "section": "Announcements",
    "text": "Announcements\n\nMonday 6/3\n\nIn the last stretch of the project\n\nClass time next week dedicated to project report help\nI may have other time slots for project help next week. I just need to take a serious look at my calendar\n\nI’m attending a virtual stat ed conference, so I just need to balance meetings with conference attendance\n\n\nNo office hours this Wednesday 6/5\n#3 “The Last Bounce: Bunnies, Burritos, and DIY Bath Salts”\n\nWednesday, June 5th Noon-2pm\nStudent Success Center, 6th floor Vanport\n\n\n\n\nWednesday 6/5\n\nLast day of lecture!!\nQuiz 2: added 3 points to everyone’s grade for that one confusing question (I forget the number, maybe question 6?)\nLab 4\n\nYour starting variables should be the TEN from lab 2, not the 5 from Lab 3\ng-value for Hosmer-Lemeshow test\n\nPlease look at Lesson 12!! There is a note on how to pick the g-value when we have many samples!!\nSome of us are getting NA’s when we put the correct g\n\nDouble check that your observed values are in numeric form\n\n\nIf doing LASSO, make sure you describe the process!\n\nWe used LASSO regression with a penalty of 0.001 to identify important predictors. We used a single test and training split of 80% and 20%, respectively. Only main effects were considered in our LASSO regression.\nAlways think: what are the key pieces of information that someone else might need to recreate this method?\n\n\nThe class will end on June 14, 2024. All coursework is expected to be completed by June 16, 2024 at 11pm.\n\nI need to have grades in on June 21st. In order to grade fairly and thoroughly, I need the whole week to grade the project report and any late assignments.\n\nA word on project grading\n\nIn the final lab, I gave you the option to do LASSO regression and focus on prediction. I know this created some confusion since we mostly set up the project as a question of association in Lab 1-3. We can still interpret the odds ratios from LASSO regression. I will be fairly lenient if reports are confused between prediction and association aims. I will try to give feedback on it, but I will not penalize any minor confusions.\nAnother word: My process starts harsh. I want to give you as much feedback as possible, and this will also reflect in lower assigned scores. At this point, I put the report grades into Sakai. I check to see if anyone’s overall course letter grade goes down. If less than ~5 course grades go down, then I revisit their project reports. If their report fails to demonstrate the most important learning objectives from the course, then I will keep the lower grade. If they demonstrate an understanding of the most important learning objectives, then I will adjust their score to increase their course grade. If more than 5 grades go down, then I revisit everyone’s reports. I will make a class wide grade bump in all reports.\nThe most important learning objectives are: understanding when and what test is appropriate, and interpreting odds ratios (from main effects and interactions)\n\nProject: LASSO\n\nYou can take the finalized formula for LASSO and use it in glm()\n\nIn this case, use the test data to come up with predictive values (like AUC)\nYou can run it on the full dataset to get the coefficient estimates and other diagnostic information\n\n\nGuide on figures"
  },
  {
    "objectID": "lessons/16_Poisson_regression/week_10_sched.html#class-exit-tickets",
    "href": "lessons/16_Poisson_regression/week_10_sched.html#class-exit-tickets",
    "title": "Week 10",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (6/3)\n Wednesday (6/5)"
  },
  {
    "objectID": "lessons/16_Poisson_regression/week_10_sched.html#muddiest-points",
    "href": "lessons/16_Poisson_regression/week_10_sched.html#muddiest-points",
    "title": "Week 10",
    "section": "Muddiest Points",
    "text": "Muddiest Points"
  },
  {
    "objectID": "lessons/17_Wrap_up/02_Intro_CDA_key_info.html#key-dates",
    "href": "lessons/17_Wrap_up/02_Intro_CDA_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/17_Wrap_up/02_Intro_CDA_key_info.html#last-class",
    "href": "lessons/17_Wrap_up/02_Intro_CDA_key_info.html#last-class",
    "title": "Key Info and Announcements",
    "section": "Last class",
    "text": "Last class"
  },
  {
    "objectID": "lessons/11_Interactions/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "href": "lessons/11_Interactions/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "title": "Muddy Points",
    "section": "Muddy Points from Spring 2024",
    "text": "Muddy Points from Spring 2024"
  },
  {
    "objectID": "lessons/11_Interactions/week_06_sched.html#resources",
    "href": "lessons/11_Interactions/week_06_sched.html#resources",
    "title": "Week 6",
    "section": "Resources",
    "text": "Resources\n\n\n\nLesson\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n10\nMultiple Logistic Regression\n\n\n\n\n\n11\nInteractions"
  },
  {
    "objectID": "lessons/11_Interactions/week_06_sched.html#on-the-horizon",
    "href": "lessons/11_Interactions/week_06_sched.html#on-the-horizon",
    "title": "Week 6",
    "section": "On the Horizon",
    "text": "On the Horizon\n\nHomework 3 due 5/9 at 11pm\nMid-quarter feedback due 5/9 at 11pm"
  },
  {
    "objectID": "lessons/11_Interactions/week_06_sched.html#class-exit-tickets",
    "href": "lessons/11_Interactions/week_06_sched.html#class-exit-tickets",
    "title": "Week 6",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (5/6)\n Wednesday (5/8)"
  },
  {
    "objectID": "lessons/11_Interactions/week_06_sched.html#announcements",
    "href": "lessons/11_Interactions/week_06_sched.html#announcements",
    "title": "Week 6",
    "section": "Announcements",
    "text": "Announcements\n\nMonday 5/6\n\nNicky’s office hours move to 3-4pm on Wednesdays\n\nI will stay after class and open Webex\n\nNotes on Homework 2\n\nContingency tables\n\nRev=”b”\n\nKnow the appropriate rev option\nriskratio() and oddsratio() reads first row as reference and first column as reference.\n\nQuestion 3 part h\n\nBoth a and b are correct\nMost ppl only marked one or the other\n\n\nWays to change outcome into factor or appropriate form for glm()\n\nicu$STA &lt;- ifelse(icu$STA==\"Died\",1,0)\nOR: icu = icu %&gt;% mutate(STA = as.factor(STA) %\\&gt;% relevel(ref = “Lived”))\n\nQuestion 4, part d\n\nTest for intercept wrong\nUsed coefficient for age\n\n\n\n\n\nWednesday 5/8\n\nLab 3 is up!"
  },
  {
    "objectID": "lessons/11_Interactions/week_06_sched.html#muddiest-points",
    "href": "lessons/11_Interactions/week_06_sched.html#muddiest-points",
    "title": "Week 6",
    "section": "Muddiest Points",
    "text": "Muddiest Points\n\n1. Why use logistic instead of linear regression?\nFrom waaay back in our slides from Lesson 5! We violate several of the assumptions for linear regression when our outcome has only two options. One of the most important reasons is that we cannot map our predictors (that can be continuous of categorical) onto a binary outcome. We need to transform our binary outcome so it is on a continuous and unbounded scale (logit does that!)\n\n\n2. Where can we find more resources for making forest plots?\nYou can either use the code I gave in Lesson 10 or use the reference links from Lab 4 in Linear Models.\n\n\n3. And how do we make a likelihood probability table instead of a plot?\nI’m not sure what this means. Please post on Slack so we can clarify! We have discussed plotting predicted probability in Lesson 7.\n\n\n4. I am confused on why we would do the odds ratio of prior fracture vs no prior fracture (in Lesson 11).\nThe odds ratio is mostly used to compare the two cases using one value. We want to compare the odds of a new fracture. We want to see how those odds of a new fracture change when we have a prior fracture or when we do not have a prior fracture. We can calculate the odds ratio for prior fracture to do that. When we only have main effects in our model, this is easier to calculate. We only have one odds ratio. However, when we have an interaction between prior fracture and age, we need a way to demonstrate how the odds ratio for prior fracture changes with age.\n\n\n5. I’m still confused about the difference between Model 3 and the estimated odd ratio table on slide 39 and what each is telling us.\nAh, yes! I’ll clarify a little more in the slide. The first table includes the coefficient estimates (\\(\\widehat\\beta_1\\),\\(\\widehat\\beta_2\\), \\(\\widehat\\beta_3\\)), and the second table includes the odds ratios (\\(\\exp(\\widehat\\beta_1)\\),\\(\\exp(\\widehat\\beta_2)\\), \\(\\exp(\\widehat\\beta_3)\\))"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT_muddy_points.html#muddy-points-from-spring-2024",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT_muddy_points.html#muddy-points-from-spring-2024",
    "title": "Muddy Points",
    "section": "Muddy Points from Spring 2024",
    "text": "Muddy Points from Spring 2024\n\n1. “times greater than” vs just “times” in interpretation\nI’ve seen it both ways. It comes down to more of an English language nuance, with what seems to be a long battle between viewpoints. Or maybe more accurately, there is a grammatically correct way to construct the sentence, but with people understanding the meaning the “incorrect” way. I tend to be more lenient when it comes to grammar in this way, but maybe that’s because I have a general distaste when languages are rigid and don’t accommodate how people currently speak and write.\n\n\n2. For the relative risks poll everywhere question #2, how were they derived?\n\nFor #1 with Trt A’s risk as 0.01 (aka \\(risk_A=0.01\\)) and Trt B’s risk as 0.001 (aka \\(risk_B=0.01\\))\n\nThe risk ratio is: \\(\\widehat{RR} = \\dfrac{\\widehat{p}_1}{\\widehat{p}_2} = \\dfrac{risk_A}{risk_B} = \\dfrac{0.01}{0.001} = 10\\)\n\nFor #2 with Trt A’s risk as 0.41 (aka \\(risk_A=0.41\\)) and Trt B’s risk as 0.401 (aka \\(risk_B=0.401\\))\n\nThe risk ratio is: \\(\\widehat{RR} = \\dfrac{\\widehat{p}_1}{\\widehat{p}_2} = \\dfrac{risk_A}{risk_B} = \\dfrac{0.41}{0.401} = 1.02\\)\n\n\n\n\n3. Ranges that odds ratios can take (0, infinity) vs the ranges that risk ratios can take.\nYeah, so both can theoretically take on the range \\([0, \\infty)\\). Both are ratios, so we also have to think about the range of the denominator and numerator For relative risk, the numerator and denominator are probabilities that can only take values from 0 to 1. While for ORs, the denominator and numerator are odds that can be a range of values \\([0, \\infty)\\).\nThe main point I was trying to make was that once we observe one group’s proportion/probability, then RRs and ORs will differ in their potential range. Let’s say I observe the proportion fro group 1 and now know the numerator for RR and the odds in the numerator for OR. Because the RR has numerator and denominator that has ranges \\([0, 1]\\), if we know the proportion of group 1 (aka numerator value), then the ratio itself has a smaller range of values because the denominator can only be between 0 and 1. Because the OR has numerator and denominator that has ranges \\([0, \\infty)\\), if we know the proportion of group 1, then we do have a fixed numerator. However, the denominator can still be in \\([0, \\infty)\\).\n\n\n4. For the odds ratio equation that we reviewed today, is it different from ad/bc ? If they are different, when is it appropriate to use the equation we just reviewed over the other? p1/(1-p1) / p2/(1-p2)\nNope! These are the same! If you learned it that way, you can definitely use it when we are working with contingency tables. However, once we move into ORs from regression with multiple covariates, I think it’s better to understand the ORs and odds in terms of the probability/proportion.\n\n\n5. Not directly related to this class, but did we cover LRTs already? They’re mentioned in HW1 but aren’t in my notes.\nOops! Fixed it in the HW. No need to do anything with LRTs!\n\n\n6. In Epi, we were very strictly told that Odds Ratios were only to be used in one type of study. (I.e. we CAN NOT use them in cross-sectional and cohort studies) only case-control. So what is the application of attempting to utilize them, if each respective type of study already has a “pre-assigned” statistical method that suits it best?\nOdds ratios CAN be used in cross-sectional AND cohort studies. It is often an over-estimate of the relative risk in those situations, so it is important to interpret it ONLY as the odds ratio.\nEach respective study does not have a pre-assigned method. The only restriction is that relative risk cannot be used in case-control studies."
  },
  {
    "objectID": "lessons/15_Model_building/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "href": "lessons/15_Model_building/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "title": "Muddy Points",
    "section": "Muddy Points from Spring 2024",
    "text": "Muddy Points from Spring 2024"
  },
  {
    "objectID": "lessons/15_Model_building/week_08_sched.html",
    "href": "lessons/15_Model_building/week_08_sched.html",
    "title": "Week 8",
    "section": "",
    "text": "Room Locations for the week\n\n\n\nOn Wednesday, 5/22, we will be in RPV Room A (1217)"
  },
  {
    "objectID": "lessons/15_Model_building/week_08_sched.html#resources",
    "href": "lessons/15_Model_building/week_08_sched.html#resources",
    "title": "Week 8",
    "section": "Resources",
    "text": "Resources\n\n\n\nLesson\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n14\nModel Diagnostics\n\n\n\n\n\n15\nModel Building"
  },
  {
    "objectID": "lessons/15_Model_building/week_08_sched.html#announcements",
    "href": "lessons/15_Model_building/week_08_sched.html#announcements",
    "title": "Week 8",
    "section": "Announcements",
    "text": "Announcements\n\nMonday 5/20\n\nHW 4 part d UPDATED!!\n\nHelp from Monday class on Part e\n\nHomework 3\n\nRemember to include the indicator function for different categories of your variables!!\nLOC has three levels: there should be two indicator functions and two coefficients for this variable!!\n\n\n\n\n\nWednesday 5/22\n\nLab 3\n\nWhen interpreting ORs…\n\nYou all are correct by including as much detail about the covariate as possible\n\nFor example: If I was using UNMETCARE_Y and I wrote “The estimated odds of food insecurity for individuals who needed medical care in the last 12 months but could not get it because they could not afford it…”\n\nThis is correct!\nBUT within our longer written report, we should define “unmet care” earlier on. Thus, once we get to interpreting ORs, we can just say “unmet care.”\n\n\nAlso, correct for including a list of the variables that you are adjusting for!\n\nBut again, we hopefully defined our final model and specifically mentioned the variables that we are adjusting for\nThus, once we get to our interpretation, we can say something more like “adjusting for the previously listed variables in our model”\n\n\nFor output of `tbl_regression()` make sure to edit the variable names into more common language\n\nset_variable_labels() from tibbleOne package might help!"
  },
  {
    "objectID": "lessons/15_Model_building/week_08_sched.html#class-exit-tickets",
    "href": "lessons/15_Model_building/week_08_sched.html#class-exit-tickets",
    "title": "Week 8",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (5/20)\n Wednesday (5/22)"
  },
  {
    "objectID": "lessons/15_Model_building/week_08_sched.html#muddiest-points-questions",
    "href": "lessons/15_Model_building/week_08_sched.html#muddiest-points-questions",
    "title": "Week 8",
    "section": "Muddiest Points / Questions",
    "text": "Muddiest Points / Questions\n\n1. How did you determine the ages for the R output on slide 24 (standardized deviance residuals)\nThe centered ages are centered around the mean age. A few classes ago I mentioned that the mean was 69 years old, might have gotten lost in this lesson. So calculating the actual ages is just adding the mean age and centered age. So centered age of 6 is 69+6 = 75. Also, very confusing because apparently I can’t add!\n\n\n2. From comment on shrinkage vs. regularization vs. penalized methods\nAll these terms are used intercahngeably!\nPenalized regression means that penalty is added to our likelihood function! This may feel like a more generic form of shrinkage or regularization. However, within statistics, I do not see penalized regression used for anything other than minimizing the coefficient values towards zero. I often see it defined as: form of regression that uses a penalty to shrink coefficients towards zero.\nDefinitions of regularized regression mirror the above for penalized regression.\nShrinkage is more the action of reducing coefficient values towards zero. Many people will refer to regularization and penalized regression as shrinkage methods.\n\nLASSO, ridge, and Elastic net are all types of penalized/regularization/shrinkage methods\n\n\n\n3. Sign column within vi() output\nThe sign column is in fact the sign of the coefficient within the model.\nSo within our interaction model, the sign for smoking status is negative. Since smoking status had many interactions, we cannot make claims about the association between smoking and fracture without considering all other variables that it interacts with. ALSO, remember that our goal here is prediction, NOT association."
  },
  {
    "objectID": "lessons/07_Pred_Viz/02_Intro_CDA_key_info.html#key-dates",
    "href": "lessons/07_Pred_Viz/02_Intro_CDA_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/07_Pred_Viz/02_Intro_CDA_key_info.html#last-class",
    "href": "lessons/07_Pred_Viz/02_Intro_CDA_key_info.html#last-class",
    "title": "Key Info and Announcements",
    "section": "Last class",
    "text": "Last class"
  },
  {
    "objectID": "lessons/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree_muddy_points.html#muddy-points-from-spring-2024",
    "href": "lessons/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree_muddy_points.html#muddy-points-from-spring-2024",
    "title": "Muddy Points",
    "section": "Muddy Points from Spring 2024",
    "text": "Muddy Points from Spring 2024"
  },
  {
    "objectID": "lessons/04_Meas_Assoc_Agree/week_02_sched.html#resources",
    "href": "lessons/04_Meas_Assoc_Agree/week_02_sched.html#resources",
    "title": "Week 2",
    "section": "Resources",
    "text": "Resources\n\n\n\nLesson\n3\nTopic\nMeasurement of Association for Contingency Tables\nSlides | Annotated Slides | Recording | :==============================================================================================================================:+:=================================================================================================================================================:+:====================================================================================================================================================================================================================================================:+  |  | \n\n\n\n\n\n\n\n4\nMeasurements of Association and Agreement\n |  |  | | | | |  |\n\n\n\nIf you ever have trouble with the above links to the videos, this Echo360 link should take you to our class page.\nFor the slides, once they are opened, if you would like to print or save them as a PDF, the best way to do this is from a computer internet browser:\n\nClick on the icon with three horizontal bars on the bottom left of the browser.\nClick on “Tools” with the gear icon at the top of the sidebar.\nClick on “PDF Export Mode.”\nFrom there, you can print or save the PDF as you would normally from your internet browser.\n\nNote: this process does not work very well on an iPad."
  },
  {
    "objectID": "lessons/04_Meas_Assoc_Agree/week_02_sched.html#on-the-horizon",
    "href": "lessons/04_Meas_Assoc_Agree/week_02_sched.html#on-the-horizon",
    "title": "Week 2",
    "section": "On the Horizon",
    "text": "On the Horizon\n\nHomework 1 due this Thursday!"
  },
  {
    "objectID": "lessons/04_Meas_Assoc_Agree/week_02_sched.html#announcements",
    "href": "lessons/04_Meas_Assoc_Agree/week_02_sched.html#announcements",
    "title": "Week 2",
    "section": "Announcements",
    "text": "Announcements\n\nMonday 4/8\n\nOffice hours!!\n\nTuesdays 5:30-7pm with Antara\nThursdays 3:30 - 5pm with Ariel\nFridays 2 - 3:30pm with Nicky\n\n\n\n\nWednesday 4/10\n\nEcho360: Let’s all double check that we can see the recordings\n\nLink to class site\n\nHomework question 5: no need to do LRT in the table\nLab 1 is up!!\nQuiz 1 decision\n\nOnline in Sakai\nWill open up on Monday at 2pm. You can chose to take it in the classroom or wait\nQuiz will close before class on Wednesday\nOpen book still\nPlease do not cheat\n\nIf I notice any unusual changes to quiz performance compared to last quarter then we will go back to the old way of giving quizzes\n\nMultiple choice with potentially some free response\n\nFor example: interpreting an OR would be divided into a multiple choice for the estimate, CI, and then writing a sentence to interpret the estimate"
  },
  {
    "objectID": "lessons/04_Meas_Assoc_Agree/week_02_sched.html#class-exit-tickets",
    "href": "lessons/04_Meas_Assoc_Agree/week_02_sched.html#class-exit-tickets",
    "title": "Week 2",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (4/8)\n Wednesday (4/10)"
  },
  {
    "objectID": "lessons/04_Meas_Assoc_Agree/week_02_sched.html#muddiest-points",
    "href": "lessons/04_Meas_Assoc_Agree/week_02_sched.html#muddiest-points",
    "title": "Week 2",
    "section": "Muddiest Points",
    "text": "Muddiest Points\n\n1. “times greater than” vs just “times” in interpretation\nI’ve seen it both ways. It comes down to more of an English language nuance, with what seems to be a long battle between viewpoints. Or maybe more accurately, I grammatically correct way to contruct the sentence, but with people understanding the meaning the “incorrect” way. I tend to be more lenient when it comes to grammar in this way, but maybe that’s because I have a general distaste when languages are rigid and don’t accommodate how people currently speak and write.\n\n\n2. For the relative risks poll everywhere question #2, how were they derived?\n\nFor #1 with Trt A’s risk as 0.01 (aka \\(risk_A=0.01\\)) and Trt B’s risk as 0.001 (aka \\(risk_B=0.01\\))\n\nThe risk ratio is: \\(\\widehat{RR} = \\dfrac{\\widehat{p}_1}{\\widehat{p}_2} = \\dfrac{risk_A}{risk_B} = \\dfrac{0.01}{0.001} = 10\\)\n\nFor #2 with Trt A’s risk as 0.41 (aka \\(risk_A=0.41\\)) and Trt B’s risk as 0.401 (aka \\(risk_B=0.401\\))\n\nThe risk ratio is: \\(\\widehat{RR} = \\dfrac{\\widehat{p}_1}{\\widehat{p}_2} = \\dfrac{risk_A}{risk_B} = \\dfrac{0.41}{0.401} = 1.02\\)\n\n\n\n\n3. Ranges that odds ratios can take (0, infinity) vs the ranges that risk ratios can take.\nYeah, so both can theoretically take on the range \\([0, \\infty)\\). Both are ratios, so we also have to think about the range of the denominator and numerator For relative risk, the numerator and denominator are probabilities that can only take values from 0 to 1. While for ORs, the denominator and numerator are odds that can be a range of values \\([0, \\infty)\\).\nThe main point I was trying to make was that once we observe one group’s proportion/probability, then RRs and ORs will differ in their potential range. Let’s say I observe the proportion fro group 1 and now know the numerator for RR and the odds in the numerator for OR. Because the RR has numerator and denominator that has ranges \\([0, 1]\\), if we know the proportion of group 1 (aka numerator value), then the ratio itself has a smaller range of values because the denominator can only be between 0 and 1. Because the OR has numerator and denominator that has ranges \\([0, \\infty)\\), if we know the proportion of group 1, then we do have a fixed numerator. However, the denominator can still be in \\([0, \\infty)\\).\n\n\n4. For the odds ratio equation that we reviewed today, is it different from ad/bc ? If they are different, when is it appropriate to use the equation we just reviewed over the other? p1/(1-p1) / p2/(1-p2)\nNope! These are the same! If you learned it that way, you can definitely use it when we are working with contingency tables. However, once we move into ORs from regression with multiple covariates, I think it’s better to understand the ORs and odds in terms of the probability/proportion.\n\n\n5. Not directly related to this class, but did we cover LRTs already? They’re mentioned in HW1 but aren’t in my notes.\nOops! Fixed it in the HW. No need to do anything with LRTs!\n\n\n6. In Epi, we were very strictly told that Odds Ratios were only to be used in one type of study. (I.e. we CAN NOT use them in cross-sectional and cohort studies) only case-control. So what is the application of attempting to utilize them, if each respective type of study already has a “pre-assigned” statistical method that suits it best?\nOdds ratios CAN be used in cross-sectional AND cohort studies. It is often an over-estimate of the relative risk in those situations, so it is important to interpret it ONLY as the odds ratio.\nEach respective study does not have a pre-assigned method. The only restriction is that relative risk cannot be used in case-control studies."
  },
  {
    "objectID": "lessons/08_Interpretations_SLR/02_Intro_CDA_key_info.html#key-dates",
    "href": "lessons/08_Interpretations_SLR/02_Intro_CDA_key_info.html#key-dates",
    "title": "Key Info and Announcements",
    "section": "Key Dates",
    "text": "Key Dates"
  },
  {
    "objectID": "lessons/08_Interpretations_SLR/02_Intro_CDA_key_info.html#last-class",
    "href": "lessons/08_Interpretations_SLR/02_Intro_CDA_key_info.html#last-class",
    "title": "Key Info and Announcements",
    "section": "Last class",
    "text": "Last class"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "href": "lessons/10_Multiple_logistic_regression/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "title": "Muddy Points",
    "section": "Muddy Points from Spring 2024",
    "text": "Muddy Points from Spring 2024"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/week_06_sched.html#resources",
    "href": "lessons/10_Multiple_logistic_regression/week_06_sched.html#resources",
    "title": "Week 6",
    "section": "Resources",
    "text": "Resources\n\n\n\nLesson\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n10\nMultiple Logistic Regression\n\n\n\n\n\n11\nInteractions"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/week_06_sched.html#on-the-horizon",
    "href": "lessons/10_Multiple_logistic_regression/week_06_sched.html#on-the-horizon",
    "title": "Week 6",
    "section": "On the Horizon",
    "text": "On the Horizon\n\nHomework 3 due 5/9 at 11pm\nMid-quarter feedback due 5/9 at 11pm"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/week_06_sched.html#class-exit-tickets",
    "href": "lessons/10_Multiple_logistic_regression/week_06_sched.html#class-exit-tickets",
    "title": "Week 6",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (5/6)\n Wednesday (5/8)"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/week_06_sched.html#announcements",
    "href": "lessons/10_Multiple_logistic_regression/week_06_sched.html#announcements",
    "title": "Week 6",
    "section": "Announcements",
    "text": "Announcements\n\nMonday 5/6\n\nNicky’s office hours move to 3-4pm on Wednesdays\n\nI will stay after class and open Webex\n\nNotes on Homework 2\n\nContingency tables\n\nRev=”b”\n\nKnow the appropriate rev option\nriskratio() and oddsratio() reads first row as reference and first column as reference.\n\nQuestion 3 part h\n\nBoth a and b are correct\nMost ppl only marked one or the other\n\n\nWays to change outcome into factor or appropriate form for glm()\n\nicu$STA &lt;- ifelse(icu$STA==\"Died\",1,0)\nOR: icu = icu %&gt;% mutate(STA = as.factor(STA) %\\&gt;% relevel(ref = “Lived”))\n\nQuestion 4, part d\n\nTest for intercept wrong\nUsed coefficient for age\n\n\n\n\n\nWednesday 5/8\n\nLab 3 is up!"
  },
  {
    "objectID": "lessons/10_Multiple_logistic_regression/week_06_sched.html#muddiest-points",
    "href": "lessons/10_Multiple_logistic_regression/week_06_sched.html#muddiest-points",
    "title": "Week 6",
    "section": "Muddiest Points",
    "text": "Muddiest Points\n\n1. Why use logistic instead of linear regression?\nFrom waaay back in our slides from Lesson 5! We violate several of the assumptions for linear regression when our outcome has only two options. One of the most important reasons is that we cannot map our predictors (that can be continuous of categorical) onto a binary outcome. We need to transform our binary outcome so it is on a continuous and unbounded scale (logit does that!)\n\n\n2. Where can we find more resources for making forest plots?\nYou can either use the code I gave in Lesson 10 or use the reference links from Lab 4 in Linear Models.\n\n\n3. And how do we make a likelihood probability table instead of a plot?\nI’m not sure what this means. Please post on Slack so we can clarify! We have discussed plotting predicted probability in Lesson 7.\n\n\n4. I am confused on why we would do the odds ratio of prior fracture vs no prior fracture (in Lesson 11).\nThe odds ratio is mostly used to compare the two cases using one value. We want to compare the odds of a new fracture. We want to see how those odds of a new fracture change when we have a prior fracture or when we do not have a prior fracture. We can calculate the odds ratio for prior fracture to do that. When we only have main effects in our model, this is easier to calculate. We only have one odds ratio. However, when we have an interaction between prior fracture and age, we need a way to demonstrate how the odds ratio for prior fracture changes with age.\n\n\n5. I’m still confused about the difference between Model 3 and the estimated odd ratio table on slide 39 and what each is telling us.\nAh, yes! I’ll clarify a little more in the slide. The first table includes the coefficient estimates (\\(\\widehat\\beta_1\\),\\(\\widehat\\beta_2\\), \\(\\widehat\\beta_3\\)), and the second table includes the odds ratios (\\(\\exp(\\widehat\\beta_1)\\),\\(\\exp(\\widehat\\beta_2)\\), \\(\\exp(\\widehat\\beta_3)\\))"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/week_07_sched.html#resources",
    "href": "lessons/13_Numeric_Problems/week_07_sched.html#resources",
    "title": "Week 7",
    "section": "Resources",
    "text": "Resources\n\n\n\nLesson\n12\nTopic\nAssessing Model Fit\nSlides | Annotated Slides | Recording | :==============================================================================================================================:+:=================================================================================================================================================:+:======================================================================================================================================:+  |  | \n\n\n\n\n\n\n\n13\nNumerical Problems\n |  |  | | | pdf on github | | |"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/week_07_sched.html#on-the-horizon",
    "href": "lessons/13_Numeric_Problems/week_07_sched.html#on-the-horizon",
    "title": "Week 7",
    "section": "On the Horizon",
    "text": "On the Horizon"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/week_07_sched.html#class-exit-tickets",
    "href": "lessons/13_Numeric_Problems/week_07_sched.html#class-exit-tickets",
    "title": "Week 7",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (5/13)\nQuiz starts on Wednesday (5/15)"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/week_07_sched.html#announcements",
    "href": "lessons/13_Numeric_Problems/week_07_sched.html#announcements",
    "title": "Week 7",
    "section": "Announcements",
    "text": "Announcements\n\nMonday 5/13\n\nI will review our interaction notes from last time\nQuiz 2 opens on Wednesday at 2pm!!\n\nWill cover Lessons 5-9!!\nMostly on Lessons 5-8, with one question about concepts covered in Lesson 9 (like types of missing data…)\n\nThink about how we fit logistic regression and GLMs\nThink about link functions!\nThink about our tests (Wald, Score, LRT)\nThink about the different transformations between Y, probability, and logit!\nThink about what our predictions mean\nThink about interpretations within logistic regression.\n\n\nLab 2\n\nI noticed that a lot of you did not comment on the trends from the bivariate analysis\n\nThis is why I didn’t have us use ggpairs() last quarter. It’s too easy to just blow past this.\nGetting to know your data and the trends you see in the sample is incredibly important!!\nThe best way to identify issues with your model is to have a good understanding of your data and their trends\n\nMany people noticed small cell counts for income levels\n\nWe will address this issues in lecture this week!\n\nUnless you are removing food insecurity from any hardship or multiple hardships, do NOT use these variables!"
  },
  {
    "objectID": "lessons/13_Numeric_Problems/week_07_sched.html#muddiest-points",
    "href": "lessons/13_Numeric_Problems/week_07_sched.html#muddiest-points",
    "title": "Week 7",
    "section": "Muddiest Points",
    "text": "Muddiest Points\n\n1. How do we determine the number of covariate patterns in R?\nTheoretically, all you need to do is count the number of groups in each categorical covariates. To find the total number of covariate patterns, you multiple those numbers by each other.\nIn R, we can take a dataframe with only the predictors in your model. You can use the distinct() function to create unique rows. The number of rows outputted will be the number of covariate patterns."
  },
  {
    "objectID": "lessons/06_Tests_GLMs/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "href": "lessons/06_Tests_GLMs/01_Intro_muddy_points.html#muddy-points-from-spring-2024",
    "title": "Muddy Points",
    "section": "Muddy Points from Spring 2024",
    "text": "Muddy Points from Spring 2024"
  },
  {
    "objectID": "lessons/06_Tests_GLMs/week_03_sched.html#resources",
    "href": "lessons/06_Tests_GLMs/week_03_sched.html#resources",
    "title": "Week 3",
    "section": "Resources",
    "text": "Resources\n\n\n\nLesson\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n5\nSimple Logistic Regression\n\n\n\n\n\n6\nTests for GLMs using Likelihood function\n\n\n\n\n\n\nIf you ever have trouble with the above links to the videos, this Echo360 link should take you to our class page."
  },
  {
    "objectID": "lessons/06_Tests_GLMs/week_03_sched.html#on-the-horizon",
    "href": "lessons/06_Tests_GLMs/week_03_sched.html#on-the-horizon",
    "title": "Week 3",
    "section": "On the Horizon",
    "text": "On the Horizon\n\nLab 1 due this Thursday (4/18)\nQuiz 1 opens on Monday, 4/22, at 2pm and will close on Wednesday, 4/24, at 1pm"
  },
  {
    "objectID": "lessons/06_Tests_GLMs/week_03_sched.html#class-exit-tickets",
    "href": "lessons/06_Tests_GLMs/week_03_sched.html#class-exit-tickets",
    "title": "Week 3",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (4/15)\n Wednesday (4/17)"
  },
  {
    "objectID": "lessons/06_Tests_GLMs/week_03_sched.html#announcements",
    "href": "lessons/06_Tests_GLMs/week_03_sched.html#announcements",
    "title": "Week 3",
    "section": "Announcements",
    "text": "Announcements\n\nMonday 4/15\n\nI am trying to stay on track of the Exit tickets this quarter\n\nThat may mean you have a 0 in your gradebook\nAs long as you complete the exit ticket within the 7 days, I will change the 0 to a 1\nI plan to have a scheduled block on Fridays to check them\n\n\n\n\nWednesday 4/17\n\nQuiz 1 info"
  },
  {
    "objectID": "lessons/06_Tests_GLMs/week_03_sched.html#muddiest-points",
    "href": "lessons/06_Tests_GLMs/week_03_sched.html#muddiest-points",
    "title": "Week 3",
    "section": "Muddiest Points",
    "text": "Muddiest Points\n\n1. Not entirely sure I understand what IRLS is about\nFair enough. It’s a little confusing. IWLS is an iterative solving technique that let’s us solve the coefficient estimates ( \\(\\beta_0\\) , \\(\\beta_1\\)) without solving the equations theoretically.\nWe start with an educated guess of the estimates, put them into the likelihood, and calculate the likelihood. Then we update the estimates using some complicated math, put them into the likelihood, and calculate the likelihood again. We compare the two likelihoods, and if the likelihood increases, then we keep going. We stop when the increase in likelihood between iterations is small. This means we are at or very close to the maximum likelihood.\n\n\n2. Link functions\nYes! Link functions are the important transformations we need to make to our outcome in order to connect them to our perdictors/covariates. Specifically, it’s the transformation we make to our mean/expected value.\nThe same link function can be used different types of outcomes. And here’s a few examples:\n\nContinuous data: identity\nBinary: logit, log\nCount/Poisson: log\n\nOur goal with link functions is to put our outcome on a flexible range so that any range of covariates can be mapped to it with coefficients. So think about trying to map age onto a 0 or 1… We can’t come up with an equation like \\(\\beta_0 + \\beta_1 Age\\) that perfectly maps to only 0’s and 1’s.\n\n\n3. Is GLM the umbrella over the other functions? The 4 functions all use different distributions, yes?\nGLM is the umbrella term for different types of regression! Not all types of regression have different outcome distributions. For example, a binary outcome can be used in logistic regression with the logit link or log-binomial regression with the log link.\n\n\n4. What would you need to change in your model to reduce a high IRLS number? As I understand it from the lecture, a high number suggests convergence but it appeared like something unfavorable even though a model that converges might be closer to maximum likelihood or maybe the distance to maximum likelihood\nA high number suggests that the model did NOT converge! Thus, we did not land on an estimate close to our maximum likelihood. You can think of the IRLS number as the number of iterations it is taking to find the maximum likelihood estimate (MLE). If it takes too many iterations, then it just stops without finding the MLE.\n\n\n5. We’re using linear vs logistic, but which are we focusing on? Regarding linear, how does linear used in categorical differ from continuous?\nWe are focusing on logistic! We cannot use linear regression on our binary outcomes anymore. When I say “linear” mapping I mean the mapping between our covariates and the transformed mean outcome using the link function.\n\n\n6. By the end of class (Lesson 6) my understanding is that the saturated model likelihood is the same between the two models being compared, right?\nYep!!\n\n\n7. The differences between each test and when to use them.\nIn terms of what each test is measuring:\n\nThe Wald test measures the distance between two potential values of \\(\\beta\\). One under the null and one under the alternative. The further they are from each other, the more evidence we have that they are different.\n\nThe Wald test approximates the differences in the likelihood function, but we do not actually compare the likelihoods under the null vs. alternative. We are only comparing the difference in the \\(\\beta\\) value, that is a reasonable approximation of the difference in the likelihood.\n\nThe Score test measures how close the tangent line of the likelihood function is to 0 (under the null). If it is close to 0 under the null, this indicates that our MLE of \\(\\beta\\) is not far from 0. Again, this is no a direct comparison of the likelihoods, but only an approximation of the difference.\nThe likelihood ratio test measures the difference in the log-likelihoods. This is a direct comparison of likelihoods, and is not an approximation!\n\nThus, we compare the likelihoods (horizontally, as someone asked) because we are making direct comparisons between the likelihood under the null and under the alternative."
  },
  {
    "objectID": "lessons/05_Simple_logistic_reg/05_Simple_logistic_reg_muddy_points.html#muddy-points-from-spring-2024",
    "href": "lessons/05_Simple_logistic_reg/05_Simple_logistic_reg_muddy_points.html#muddy-points-from-spring-2024",
    "title": "Muddy Points",
    "section": "Muddy Points from Spring 2024",
    "text": "Muddy Points from Spring 2024"
  },
  {
    "objectID": "lessons/12_Assessing_fit/week_07_sched.html#resources",
    "href": "lessons/12_Assessing_fit/week_07_sched.html#resources",
    "title": "Week 7",
    "section": "Resources",
    "text": "Resources\n\n\n\nLesson\n12\nTopic\nAssessing Model Fit\nSlides | Annotated Slides | Recording | :==============================================================================================================================:+:=================================================================================================================================================:+:======================================================================================================================================:+  |  | \n\n\n\n\n\n\n\n13\nNumerical Problems\n |  |  | | | pdf on github | | |"
  },
  {
    "objectID": "lessons/12_Assessing_fit/week_07_sched.html#on-the-horizon",
    "href": "lessons/12_Assessing_fit/week_07_sched.html#on-the-horizon",
    "title": "Week 7",
    "section": "On the Horizon",
    "text": "On the Horizon"
  },
  {
    "objectID": "lessons/12_Assessing_fit/week_07_sched.html#class-exit-tickets",
    "href": "lessons/12_Assessing_fit/week_07_sched.html#class-exit-tickets",
    "title": "Week 7",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (5/13)\nQuiz starts on Wednesday (5/15)"
  },
  {
    "objectID": "lessons/12_Assessing_fit/week_07_sched.html#announcements",
    "href": "lessons/12_Assessing_fit/week_07_sched.html#announcements",
    "title": "Week 7",
    "section": "Announcements",
    "text": "Announcements\n\nMonday 5/13\n\nI will review our interaction notes from last time\nQuiz 2 opens on Wednesday at 2pm!!\n\nWill cover Lessons 5-9!!\nMostly on Lessons 5-8, with one question about concepts covered in Lesson 9 (like types of missing data…)\n\nThink about how we fit logistic regression and GLMs\nThink about link functions!\nThink about our tests (Wald, Score, LRT)\nThink about the different transformations between Y, probability, and logit!\nThink about what our predictions mean\nThink about interpretations within logistic regression.\n\n\nLab 2\n\nI noticed that a lot of you did not comment on the trends from the bivariate analysis\n\nThis is why I didn’t have us use ggpairs() last quarter. It’s too easy to just blow past this.\nGetting to know your data and the trends you see in the sample is incredibly important!!\nThe best way to identify issues with your model is to have a good understanding of your data and their trends\n\nMany people noticed small cell counts for income levels\n\nWe will address this issues in lecture this week!\n\nUnless you are removing food insecurity from any hardship or multiple hardships, do NOT use these variables!"
  },
  {
    "objectID": "lessons/12_Assessing_fit/week_07_sched.html#muddiest-points",
    "href": "lessons/12_Assessing_fit/week_07_sched.html#muddiest-points",
    "title": "Week 7",
    "section": "Muddiest Points",
    "text": "Muddiest Points\n\n1. How do we determine the number of covariate patterns in R?\nTheoretically, all you need to do is count the number of groups in each categorical covariates. To find the total number of covariate patterns, you multiple those numbers by each other.\nIn R, we can take a dataframe with only the predictors in your model. You can use the distinct() function to create unique rows. The number of rows outputted will be the number of covariate patterns."
  },
  {
    "objectID": "lessons/09_Missing_data/week_05_sched.html",
    "href": "lessons/09_Missing_data/week_05_sched.html",
    "title": "Week 5",
    "section": "",
    "text": "MONDAY CLASS CANCELLED\n\n\n\nMonday’s class is cancelled! We will be back on Wednesday for a fun, random lecture on missing data!!"
  },
  {
    "objectID": "lessons/09_Missing_data/week_05_sched.html#resources",
    "href": "lessons/09_Missing_data/week_05_sched.html#resources",
    "title": "Week 5",
    "section": "Resources",
    "text": "Resources\n\n\n\nLesson\n9\nTopic\nMissing Data\nActivity:\nSlides | Annotated Slides | Recording | :==========================================================================================================================:+:=========================================================================================================================================:+:======================================================================================================================================:+  |  |  | | |  | | |"
  },
  {
    "objectID": "lessons/09_Missing_data/week_05_sched.html#on-the-horizon",
    "href": "lessons/09_Missing_data/week_05_sched.html#on-the-horizon",
    "title": "Week 5",
    "section": "On the Horizon",
    "text": "On the Horizon\n\nLab 2 due 5/2 at 11pm\nHW 3 is up and due 5/9 at 11pm!"
  },
  {
    "objectID": "lessons/09_Missing_data/week_05_sched.html#class-exit-tickets",
    "href": "lessons/09_Missing_data/week_05_sched.html#class-exit-tickets",
    "title": "Week 5",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Wednesday (5/1)"
  },
  {
    "objectID": "lessons/09_Missing_data/week_05_sched.html#announcements",
    "href": "lessons/09_Missing_data/week_05_sched.html#announcements",
    "title": "Week 5",
    "section": "Announcements",
    "text": "Announcements\n\nWednesday 5/1\n\nQuiz 1 grades are up!\nSPH Student survey still going until 5/3!!\n\nIf you already did it, then yay!\nRaffling 2 iPads!\n\nOur mid quarter survey is up!\n\nDUE 5/9 at 11pm!!"
  },
  {
    "objectID": "lessons/09_Missing_data/week_05_sched.html#muddiest-points",
    "href": "lessons/09_Missing_data/week_05_sched.html#muddiest-points",
    "title": "Week 5",
    "section": "Muddiest Points",
    "text": "Muddiest Points\nI couldn’t resist making a gif of me reacting to the fire alarm when I was editing the Echo360 recording."
  },
  {
    "objectID": "lessons/01_Intro/01_Intro.html#check-the-echo360-link",
    "href": "lessons/01_Intro/01_Intro.html#check-the-echo360-link",
    "title": "Lesson 1: Welcome!",
    "section": "Check the Echo360 link",
    "text": "Check the Echo360 link\n\nLet’s check the link and make sure we can see everything!"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#what-is-categorical-data-analysis",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#what-is-categorical-data-analysis",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "What is Categorical Data Analysis?",
    "text": "What is Categorical Data Analysis?\n\nIn BSTA 512/612 (linear regression), we focused on continuous responses/outcomes\n\nWe included categorical variables only as covariates (aka predictors, independent variables, explanatory variables)\nExamples from 512/612: life expectancy (in years), IAT score (ranging from -2 to 2)\n\n\n   \n\nCategorical data analysis focuses on the statistical methods for categorical responses/outcomes\n\nExplanatory (or ‘independent’) variable can be of any type (continuous or categorical)"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#types-of-variables",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#types-of-variables",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Types of Variables",
    "text": "Types of Variables"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#types-of-variables-outcomes-we-will-cover-in-this-course",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#types-of-variables-outcomes-we-will-cover-in-this-course",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Types of Variables: Outcomes we will cover in this course",
    "text": "Types of Variables: Outcomes we will cover in this course"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#what-does-this-course-cover",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#what-does-this-course-cover",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "What does this course cover?",
    "text": "What does this course cover?\n\nStrategies for assessing association between categorical response variable and a one explanatory variable\n\nHypothesis testing\nMeasure of association\nSimple logistic regression\n\n\n   \n\nStatistical modeling strategies for assessing association between the categorical response variable and a set of explanatory variables\n\nLogistic regression\n\nFor binary, ordinal, and multinomial outcomes\n\nPoisson regression\n\nFor counts outcomes"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#poll-everywhere-question-1",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#poll-everywhere-question-1",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Poll everywhere question 1",
    "text": "Poll everywhere question 1"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#binomial-distribution",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#binomial-distribution",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\nConsider a sample of \\(n\\) independent trials, each of which can have only two possible outcomes (“success” and “failure”)\nFor each trial: \\[\\begin{align} P( \\text{success}) & = p \\\\\n                  P( \\text{failure}) & = 1- p = q \\end{align}\\]\nBinomial distribution: distribution of the number of successes in n independent trials\nThe probability mass function for the binomial distribution is: \\[P(X=k) = {n \\choose k} p^k q^{n-k}, \\text{ for } k = 0, 1, ..., n\\]\n\n\\(E(X) = np\\)\n\\(Var(X) = npq = np(1-p)\\)"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#binomial-distribution-1",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#binomial-distribution-1",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\nConsider a sample of \\(n\\) independent trials, each of which can have only two possible outcomes (“success” and “failure”)\nFor each trial: \\[\\begin{align} P( \\text{success}) & = p \\\\\n                  P( \\text{failure}) & = 1- p = q \\end{align}\\]\nBinomial distribution: distribution of the number of successes in n independent trials\nThe probability mass function for the binomial distribution is: \\[P(X=k) = {n \\choose k} p^k q^{n-k}, \\text{ for } k = 0, 1, ..., n\\]\n\n\\(E(X) = np\\)\n\\(Var(X) = npq = np(1-p)\\)"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#binomial-distribution-r-commands",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#binomial-distribution-r-commands",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Binomial Distribution: R commands",
    "text": "Binomial Distribution: R commands\nR commands with their input and output:\n\n\n\n\n\n\n\nR code\nWhat does it return?\n\n\n\n\nrbinom()\nreturns sample of random variables with specified binomial distribution\n\n\ndbinom()\nreturns probability of getting certain number of successes\n\n\npbinom()\nreturns cumulative probability of getting certain number or less successes\n\n\nqbinom()\nreturns number of successes corresponding to desired quantile"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#binomial-distribution-example",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#binomial-distribution-example",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Binomial Distribution Example",
    "text": "Binomial Distribution Example\n\n\nExample\n\n\nIf the probability that one white blood cell is a lymphocyte is 0.2, compute the probability of 2 lymphocytes out of 10 white blood cells\n\n\n\\[P(X=2) = {10 \\choose 2} 0.2^2 (1-0.2)^{10-2}  = 0.3020\\]\n\ndbinom(2, 10, 0.2) %&gt;% round(4)\n\n[1] 0.302"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#normal-approximation-of-the-binomial-distribution",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#normal-approximation-of-the-binomial-distribution",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Normal Approximation of the Binomial Distribution",
    "text": "Normal Approximation of the Binomial Distribution\n\nAlso known as: Sampling distribution of \\(\\widehat{p}\\)\nIF \\(X\\sim \\text{Binomial}(n,p)\\) and \\(np&gt;10\\) and \\(nq = n(1-p) &gt; 10\\)\n\nEnsures sample size (\\(n\\)) is moderately large and the \\(p\\) is not too close to 0 or 1\nOther resources use other criteria (like \\(npq&gt;5\\) or \\(np&gt;5\\))\nWhen looking at a sample, we use \\(\\widehat{p}\\) instead of \\(p\\) to check this!!\n\n\n \n\nTHEN approximately \\(𝑋\\sim \\text{Normal}\\big(\\mu_X = np, \\sigma_X = \\sqrt{np(1-p)} \\big)\\)\n\nOr we often write this as the sampling distribution of \\(\\widehat{p}\\): \\[\\widehat{p} \\sim \\text{Normal}\\bigg(\\mu_{\\widehat{p}} = p, \\sigma_{\\widehat{p}} = \\sqrt{\\dfrac{p(1-p)}{n}}\\bigg)\\]\n\nPretty good video behind the intuition of this (Watch 00:00 - 05:40)"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#estimate-and-confidence-interval-of-single-proportion",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#estimate-and-confidence-interval-of-single-proportion",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Estimate and Confidence Interval of Single Proportion",
    "text": "Estimate and Confidence Interval of Single Proportion\n\nEstimate of proportion:\n\n\\[\n\\widehat{p} = \\dfrac{\\# \\text{successes}}{\\# \\text{successes} + \\# \\text{failures}}\n\\]\n\nUse the sampling distribution of \\(\\widehat{p}\\) to construct the confidence interval:\n\n\\((1-\\alpha)\\%\\) confidence interval for estimate proportion:\n\n\n\\[\\begin{align} \\widehat{p} &\\pm z^*_{(1-\\alpha/2)} \\cdot SE_{\\hat{p}} \\\\ \\widehat{p} &\\pm z^*_{(1-\\alpha/2)} \\cdot \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\end{align}\\]\n\nUsing \\(SE_{\\hat{p}} = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\) - instead of \\(\\sigma_{p} = \\sqrt{\\frac{p(1-p)}{n}}\\) - because we don’t know exactly what \\(p\\) is"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#section",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#section",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "",
    "text": "Smoking status example\n\n\nA cross-sectional study of 8681 patients was conducted to evaluate the nature of smoking status among people. Of the 8681 people, 4840 were nonsmokers and 3841 were smokers.\n\n\nNeeded steps:\n\nEstimate proportion \\(\\widehat{p}\\)\nCheck that \\(n\\widehat{p}&gt;10\\) and \\(n(1-\\widehat{p})&gt;10\\) in order to make normal approximation\nConstruct 95% confidence interval\nWrite interpretation"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#section-1",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#section-1",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "",
    "text": "Smoking status example\n\n\nA cross-sectional study of 8681 patients was conducted to evaluate the nature of smoking status among people. Of the 8681 people, 4840 were nonsmokers and 3841 were smokers.\n\n\n\n\n\nEstimate proportion \\(\\widehat{p}\\) \\[ \\widehat{p} = \\dfrac{3841}{3841 + 4840} = \\dfrac{3841}{8681} = 0.44246\\]\nCheck that \\(n\\widehat{p}&gt;10\\) and \\(n(1-\\widehat{p})&gt;10\\) in order to make normal approximation \\[ n\\widehat{p} = 8681\\cdot0.4425 = 3841 &gt; 10\\] \\[ n(1-\\widehat{p}) = 8681\\cdot(1-0.4425) = 4840 &gt; 10\\]\n\n\n\nConstruct 95% confidence interval\n\n\\[ \\widehat{p} \\pm z^*_{0.975} \\cdot \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\]\n\nprop.test(x = 3841, n = 8681, correct = T)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  3841 out of 8681, null probability 0.5\nX-squared = 114.73, df = 1, p-value &lt; 2.2e-16\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.4319827 0.4529896\nsample estimates:\n        p \n0.4424605 \n\n\n\nWrite interpretation of estimate\n\nThe estimated proportion of smokers is 0.442 (95% CI: 0.432, 0.453).\nAdditional interpretation of CI: We are 95% confident that the (population) proportion of smokers is between 0.432 and 0.453."
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#poll-everywhere-question-2",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#poll-everywhere-question-2",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Poll everywhere question 2",
    "text": "Poll everywhere question 2"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#estimate-and-confidence-interval-for-difference-in-proportions",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#estimate-and-confidence-interval-for-difference-in-proportions",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Estimate and Confidence Interval for Difference in Proportions",
    "text": "Estimate and Confidence Interval for Difference in Proportions\n\nUse the sampling distribution of \\(\\widehat{p}_1\\) and \\(\\widehat{p}_2\\) to construct the confidence interval:\n\n\\((1-\\alpha)\\%\\) confidence interval for estimate difference in proportions:\n\n\n\\[\\begin{align} \\widehat{p}_1 - \\widehat{p}_2 &\\pm z^*_{(1-\\alpha/2)} \\cdot SE_{\\hat{p}_1 - \\hat{p}_2} \\\\ \\widehat{p}_1 - \\widehat{p}_2 &\\pm z^*_{(1-\\alpha/2)} \\cdot \\sqrt{\n\\frac{\\hat{p}_1\\cdot(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2\\cdot(1-\\hat{p}_2)}{n_2}} \\end{align}\\]\n\nUsing \\(SE_{\\hat{p}_1 - \\hat{p}_2} = \\sqrt{\\frac{\\hat{p}_1\\cdot(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2\\cdot(1-\\hat{p}_2)}{n_2}}\\) because we don’t know exactly what \\(p_1\\) and \\(p_2\\) are"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#poll-everywhere-question-3",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#poll-everywhere-question-3",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#example-strong-heart-study-12",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#example-strong-heart-study-12",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Example: Strong Heart Study (1/2)",
    "text": "Example: Strong Heart Study (1/2)\n\n\nThe Strong Heart Study is an ongoing study of American Indians residing in 13 tribal communities in three geographic areas (AZ, OK, and SD/ND) to study prevalence and incidence of cardiovascular disease and to identify risk factors. We will be examining the 4-year cumulative incidence of diabetes with one risk factor, glucose tolerance.\n \n\nImpaired glucose: normal or impaired glucose tolerance at baseline visit (between 1988 and 1991)\n \nDiabetes: Indicator of diabetes at follow-up visit (roughly four years after baseline) according to two-hour oral glucose tolerance test"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#example-strong-heart-study-22",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#example-strong-heart-study-22",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Example: Strong Heart Study (2/2)",
    "text": "Example: Strong Heart Study (2/2)\nThere is a total of 1664 American Indians in the dataset, with the following distribution of folks with diabetes and glucose tolerance:\n \n\n\n\n#shs_data = read.csv(file = here(\"./data/SHS_data.csv\"))\n\n\nSHS = tibble(Diabetes = c(rep(\"Not diabetic\", \n                   1338), \n                   rep(\"Diabetic\", 326)),\n              Glucose = c(rep(\"Normal\", \n                  1004),#Not diabetic\n          rep(\"Impaired\", 334),\n          rep(\"Normal\", \n              128), #Diabetic\n          rep(\"Impaired\", 198)))\n\n\n\n\nDisplaying the contingency table in R\nSHS %&gt;% tabyl(Glucose, Diabetes) %&gt;% \n  adorn_totals(where = c(\"row\", \"col\")) %&gt;% \n  gt() %&gt;% \n  tab_stubhead(label = \"Glucose Impairment\") %&gt;%\n  tab_spanner(label = \"Diabetes\", \n              columns = c(\"Not diabetic\", \"Diabetic\")) %&gt;%\n  tab_options(table.font.size = 45)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose\n\nDiabetes\n\nTotal\n\n\nNot diabetic\nDiabetic\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#section-2",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#section-2",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "",
    "text": "Strong Heart Study\n\n\nWhat is the difference in proportions for American Indians that have diabetes comparing individuals with normal vs. impaired glucose?\n\n\nNeeded steps:\n\nEstimate the difference in proportions\nCheck that each cell has at least 10 individuals\nConstruct 95% confidence interval\nWrite interpretation of estimate"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#section-3",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#section-3",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "",
    "text": "Strong Heart Study\n\n\nWhat is the difference in proportions for American Indians that have diabetes comparing individuals with normal vs. impaired glucose?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose\n\nDiabetes\n\nTotal\n\n\nNot diabetic\nDiabetic\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\nEstimate the difference in proportions \\[ \\widehat{p}_1 -\\widehat{p}_2 = \\dfrac{198}{532} - \\dfrac{128}{1132} = 0.2591\\]\nCheck that each cell has at least 10 individuals\n\n\n\nConstruct 95% confidence interval\n\n\nprop.test(x = table(SHS$Glucose, SHS$Diabetes), \n          correct = T)\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  table(SHS$Glucose, SHS$Diabetes)\nX-squared = 152.6, df = 1, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n95 percent confidence interval:\n 0.2126963 0.3055162\nsample estimates:\n   prop 1    prop 2 \n0.3721805 0.1130742 \n\n\n\nWrite interpretation of estimate\n\nThe estimated difference in proportion of diabetic American Indians comparing is 0.259 (95% CI: 0.213, 0.306).\nAdditional interpretation of CI: We are 95% confident that the difference in (population) proportions of American Indians who have normal glucose tolerance and impaired glucose tolerance that developed diabetes is between 0.213 and 0.306."
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#mcnemars-test",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#mcnemars-test",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "McNemar’s Test",
    "text": "McNemar’s Test\n\nMcNemar’s test should be used if data is from a matched pairs study\n\n \n\nWhat is a matched-pairs study?\n\nParticipants are paired based on key characteristics\nEach participant within a pair will be assigned to different treatment groups\n\nCategorical test that is parallel to the “paired t-test”\n\n \n\nR packages and functions\n\nNormal approximation: mcnemar.test() in built-in stats package\nExact test: mcnemar.exact() in exact2x2 package\n\n\n \n\nIf you would like more information of McNemar’s test, please see Rosner TB: 10.4 and 10.5: Paired Samples"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#summary-so-far",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#summary-so-far",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Summary so far",
    "text": "Summary so far\n\nIntroduced categorical data as the response in analysis\nReviewed an important distribution (Binomial distribution) for categorical data analysis\nEstimated a single proportion from a sample with its confidence interval\nEstimated a difference in proportions from a sample with its confidence interval\n\n   \n\nCan we expand this to ask a more general question about association between a response and explanatory variable?\n\nWhat if there is more than 2 categories for either variable?"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#contingency-tables-r-x-c",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#contingency-tables-r-x-c",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Contingency Tables (R x C)",
    "text": "Contingency Tables (R x C)\n\nR X C contingency tables\n\nContains information for two discrete variables: one has R categories and the other has C categories.\nRefers to the number of rows (R) and number of columns (C) in the table\n\n\n \n\nFor two proportions: focused on 2 X 2 contingency tables\n\nR = 2, C = 2\n\n\n \n\nExpand our contingency tables to variables with 2 or more categories\n\nCategories can be ordinal or nominal"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#contingency-table-example",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#contingency-table-example",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Contingency Table: Example",
    "text": "Contingency Table: Example\nLet’s say we are interested in learning the association between the development of breast cancer and age at first birth. Our first step is typically to present the observed data:\n\n\nThis is a 2 x 5 contingency table"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#test-associationtrend-of-r-x-c-contingency-table",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#test-associationtrend-of-r-x-c-contingency-table",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Test Association/Trend of R X C Contingency Table",
    "text": "Test Association/Trend of R X C Contingency Table\n \n\n\nIf both variables are nominal, a test of general association will be sufficient\n\nTest of general association is the same regardless of R and C\nTest used for 2x2 contingency table same as 5x3 contingency table\nWe will cover:\n\nChi-squared test\nFisher Exact test\n\n\n\n\n\nIf one or both variables are ordinal, a test of trend may be of interest\n\nTreats ordinal variables as quantitative rather than qualitative (nominal scale)\nTest of trend has greater power than the test of general association\nWe will cover:\n\nCochran-Armitage test\nMantel-Haenszel test"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#poll-everywhere-question-4",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#poll-everywhere-question-4",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#test-of-general-association",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#test-of-general-association",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Test of General Association",
    "text": "Test of General Association\n\nGeneral research question: Are two variables (both categorical, nominal) associated with each other?\n\n \n\nTranslated to a hypothesis test:\n\n\\(H_0\\) : There is no association between the two variables / The variables are independent\n\\(H_1\\) : There is an association between the two variables / The variables are not independent\n\n\n   \n\nWe have two options for testing general association:\n\nChi-squared test\nFisher’s Exact test"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#test-of-general-association-shs-example",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#test-of-general-association-shs-example",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Test of General Association: SHS Example",
    "text": "Test of General Association: SHS Example\n\nMain question: Do American Indians with impaired glucose tolerance have a different incidence of diabetes?\n\nIs glucose tolerance associated with diabetes incidence among American Indians?\n\nWe have two variables, and both variables have two nominal categories\n\nDiabetes outcome: Not diabetic and Diabetic\nGlucose tolerance: Normal or Impaired\n\n\n \n\nAnswer research question with a test of general association\nHypothesis:\n\n\\(H_0\\) : There is no association between glucose tolerance and diabetes / Glucose tolerance and diabetes are independent\n\\(H_1\\) : There is an association between glucose tolerance and diabetes / Glucose tolerance and diabetes are not independent"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Chi-squared test",
    "text": "Chi-squared test\n\nTest to see how likely is it that we observe our data given the null hypothesis (no association)\n\n \n\nWe use the null to calculate the expected cell counts and compare them to the observed cell counts\n\n \n\nRequirements to conduct Chi-squared test (expected cell counts)\n\nFor 2 x 2 contingency table:\n\nNo expected cell counts should be less than 10\n\nFor contingency table with 3x2, 3x3, 4x4, etc.:\n\nNo more than 20% of expected cell counts are less than 5\nNo expected cell counts are less than 1"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test-process",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test-process",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Chi-squared test: Process",
    "text": "Chi-squared test: Process\n\nCheck that the expected cell counts threshold is met\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic and p-value for Chi-squared test in R\n\nWe will not discuss the test statistic’s equation\n\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test-expected-cell-counts",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test-expected-cell-counts",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Chi-squared test: Expected Cell Counts",
    "text": "Chi-squared test: Expected Cell Counts\n\nIs the sample size big enough for the chi-square test to be adequate? What are the expected cell counts?\n\n \n\nIf you want an explanation of how to calculate by hand, please see Vu and Harringtion TB (section 8.3.1, page 405)\n\n \n\nToo time consuming for this class, but R does it quickly using the expected() function in the epitools package"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test-expected-cell-counts-1",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test-expected-cell-counts-1",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Chi-squared test: Expected Cell Counts",
    "text": "Chi-squared test: Expected Cell Counts\n\nIn the Strong Heart Study…\n\n\nSHS_table = table(SHS$Glucose, SHS$Diabetes)\nSHS_table\n\n          \n           Diabetic Not diabetic\n  Impaired      198          334\n  Normal        128         1004\n\nlibrary(epitools)\nexpected(SHS_table)\n\n          \n           Diabetic Not diabetic\n  Impaired  104.226      427.774\n  Normal    221.774      910.226\n\n\n \n\n\n\n\n\nAll expected counts &gt; 5"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test-shs-example",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test-shs-example",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Chi-squared test: SHS Example",
    "text": "Chi-squared test: SHS Example\n\n\n\nCheck expected cell counts threshold\n\n\nexpected(SHS_table)\n\n          \n           Diabetic Not diabetic\n  Impaired  104.226      427.774\n  Normal    221.774      910.226\n\n\nAll expected cells are greater than 5.\n\n\\(\\alpha = 0.05\\)\nHypothesis test:\n\n\\(H_0\\) : There is no association between glucose tolerance and diabetes\n\\(H_1\\) : There is an association between glucose tolerance and diabetes\n\n\n\n\nCalculate the test statistic and p-value for Chi-squared test in R\n\n\nchisq.test(x = SHS_table, correct = T)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  SHS_table\nX-squared = 152.6, df = 1, p-value &lt; 2.2e-16\n\n\n\nConclusion to the hypothesis test\n\nWe reject the null hypothesis that glucose tolerance and diabetes are not associated (\\(p&lt;2.2\\cdot10^{-16}\\)). There is sufficient evidence that glucose tolerance and diabetes incidence are associated among American Indians."
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Fisher’s Exact Test",
    "text": "Fisher’s Exact Test\n\nOnly necessary when expected counts in one or more cells is less than 5\nGiven row and column totals fixed, computes exact probability that we observe our data or more extreme data\nConsider a general 2 x 2 table:\n\n\n\nThe exact probability of observing a table with cells (a, b, c, d) can be computed based on the hypergeometric distribution\n\n\\[P(a, b, c, d) = \\dfrac{(a+b)!\\cdot(c+d)!\\cdot(a+c)!\\cdot(b+d)!}{n!\\cdot a!\\cdot b!\\cdot c!\\cdot d!}\\]\n\nNumerator is fixed and denominator changes"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-process",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-process",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Fisher’s Exact test: Process",
    "text": "Fisher’s Exact test: Process\n\nCheck the expected cell counts\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic and p-value for Fisher Exact test in R\n\nWe will not discuss the test statistic’s equation\n\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-shs-example-not-appropriate-use-of-fishers-exact",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-shs-example-not-appropriate-use-of-fishers-exact",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Fisher’s Exact test: SHS Example (not appropriate use of Fisher’s Exact)",
    "text": "Fisher’s Exact test: SHS Example (not appropriate use of Fisher’s Exact)\n\n\n\nCheck expected cell counts threshold\n\n\nexpected(GAC_table)\n\n     \n             No      Yes\n  No  143.21983 5.780172\n  Yes  79.78017 3.219828\n\n\nWe’re going to pretend they are less than 5.\n\n\\(\\alpha = 0.05\\)\nHypothesis test:\n\n\\(H_0\\) : There is no association between glucose tolerance and diabetes\n\\(H_1\\) : There is an association between glucose tolerance and diabetes\n\n\n\n\nCalculate the test statistic and p-value for Chi-squared test in R\n\n\nfisher.test(x = GAC_table)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  GAC_table\np-value = 0.2877\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  0.4829292 12.0164676\nsample estimates:\nodds ratio \n  2.314727 \n\n\n\nConclusion to the hypothesis test\n\nWe reject the null hypothesis that glucose tolerance and diabetes are not associated (\\(p&lt;2.2\\cdot10^{-16}\\)). There is sufficient evidence that glucose tolerance and diabetes incidence are associated among American Indians."
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#test-of-trend",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#test-of-trend",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Test of Trend",
    "text": "Test of Trend\n\nIf one or both variables are ordinal, a test of trend may be of interest\n\nTreats ordinal variables as quantitative rather than qualitative (nominal scale)\nTest of trend has greater power than the test of general association\nYou can use a test of general association for non-ordinal variables\n\n\n \n\nTwo tests of trend that we we learn:\n\nCochran-Armitage test\n\nTests association between a binary response and an ordinal explanatory variable\n\nMantel-Haenszel test\n\nTest association between an ordinal response and an ordinal explanatory variable"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Cochran-Armitage test",
    "text": "Cochran-Armitage test\n\nCochran-Armitage test for trend will determine if there is association between a binary response variable and an ordinal variable with 3 or more categories\n\n \n\nIt will test the trend of the proportions over the ordinal variable\n\nAnswers the question: Does the proportion of people with a “successful” outcome increase as the ordinal explanatory variable increases?\n\n\n \n\nCochran-Armitage test for trend is only suitable for 2 x C contingency tables"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-hypothesis-test",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-hypothesis-test",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Cochran-Armitage test: Hypothesis Test",
    "text": "Cochran-Armitage test: Hypothesis Test\n\n\n\n\nNull Hypothesis (\\(H_0\\))\n\n\nThe proportions of successes are the same across all C ordinal values of the explanatory variable. \\[p_1 = p_2 = ... = p_C\\]\n\n\n\n\n\nAlternative Hypothesis (\\(H_1\\))\n\n\nThe proportions of successes tend to increase as ordinal value of the explanatory variable increases\n\\[p_1 \\leq p_2 \\leq ... \\leq p_C\\]\nOR\nThe proportions of successes tend to decrease as ordinal value of the explanatory variable increases\n\\[p_1 \\geq p_2 \\geq ... \\geq p_C\\]"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-process",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-process",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Cochran-Armitage test: Process",
    "text": "Cochran-Armitage test: Process\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic and p-value for Cochran-Armitage test in R\n\nWe will not discuss the test statistic’s equation\nJust know it follows a Normal distribution\n\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-example-13",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-example-13",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Cochran-Armitage test: Example (1/3)",
    "text": "Cochran-Armitage test: Example (1/3)\nWe are interested in learning the association between the development of breast cancer and age at first birth among people who have given birth"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-example-23",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-example-23",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Cochran-Armitage test: Example (2/3)",
    "text": "Cochran-Armitage test: Example (2/3)\n\nBefore we go into the hypothesis test procedure, we need to construct the contingency table in R\n\n\nCancer = c(320, 1206, 1011, 463, 220)\nNo_Cancer = c(1422, 4432, 2893, 1092, 406)\nbscancer = matrix (c(Cancer, No_Cancer), nrow = 2, byrow = T)\nrownames(bscancer) = c(\"Cancer\",\"No Cancer\")\ncolnames(bscancer) = c(\"&lt;20\",\"20-24\",\"25-29\",\"30-34\",\"&gt;=35\")\nbscancer\n\n           &lt;20 20-24 25-29 30-34 &gt;=35\nCancer     320  1206  1011   463  220\nNo Cancer 1422  4432  2893  1092  406\n\n\n\nIt does not need to be pretty to use in function"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-example-33",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-example-33",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Cochran-Armitage test: Example (3/3)",
    "text": "Cochran-Armitage test: Example (3/3)\n\n\n\n\\(\\alpha = 0.05\\)\nHypothesis test:\n\n\\(H_0\\): The proportions of breast cancer are the same for all age levels of first birth. \\[p_1 = p_2 = ... = p_5\\]\n\\(H_1\\): The proportions of breast cancer tends to increase as level of age of first birth increases\n\n\n\\[p_1 \\leq p_2 \\leq ... \\leq p_5\\]\n\n\nCalculate the test statistic and p-value for Cochran-Armitage test in R\n\n\nlibrary(DescTools)\nCochranArmitageTest(bscancer)\n\n\n    Cochran-Armitage test for trend\n\ndata:  bscancer\nZ = 11.358, dim = 5, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\nConclusion to the hypothesis test\n\nWe reject the null hypothesis that proportions of breast cancer are the same for all age levels of first birth (\\(p&lt;2.2\\cdot10^{-16}\\)). There is sufficient evidence that the proportion of of breast cancer increase as the the age at first birth increases."
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Mantel-Haenszel test",
    "text": "Mantel-Haenszel test\n\nWhen both variables are ordinal, we can conduct Mantel-Haenszel test of trend for linear association\nMantel-Haenszel test for linear trend is suitable for any R x C contingency tables with two ordinal variables\nHypothesis test:\n\n\n\n\n\nNull Hypothesis (\\(H_0\\))\n\n\nThere is no correlation between the two variables \\[ \\rho = 0\\]\n\n\n\n\n\nAlternative Hypothesis (\\(H_1\\))\n\n\nThere is correlation between the two variables\n\\[ \\rho \\neq 0\\]"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-process",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-process",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Mantel-Haenszel test: Process",
    "text": "Mantel-Haenszel test: Process\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic and p-value for Mantel-Haenszel test in R\n\nWe will not discuss the test statistic’s equation\n\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-example-13",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-example-13",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Mantel-Haenszel test: Example (1/3)",
    "text": "Mantel-Haenszel test: Example (1/3)\nA water treatment company is studying water additives and investigating how they affect clothes washing (through measurements of abrasions, wearing, and color loss).\nThe treatments studies where no treatment (plain water), the standard treatment, and a double dose of the standard treatment, called super. Washability was measured as low, medium and high.\nAre levels of washability associated with treatment?"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-example-23",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-example-23",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Mantel-Haenszel test: Example (2/3)",
    "text": "Mantel-Haenszel test: Example (2/3)\n\nBefore we go into the hypothesis test procedure, we need to construct the contingency table in R\n\n\nwater = matrix (c(27, 14, 5, 10, 17, 26, 5, 12, 50), nrow = 3, byrow = T)\nrownames(water) = c(\"plain\",\"standard\",\"super\")\ncolnames(water) = c(\"low\",\"medium\",\"high\")\nwater\n\n         low medium high\nplain     27     14    5\nstandard  10     17   26\nsuper      5     12   50\n\n\n\nIt does not need to be pretty to use in function"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-example-33",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-example-33",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Mantel-Haenszel test: Example (3/3)",
    "text": "Mantel-Haenszel test: Example (3/3)\n\n\n\n\\(\\alpha = 0.05\\)\nHypothesis test:\n\n\\(H_0\\): Water treatment and washability are not correlated. \\[ \\rho = 0\\]\n\\(H_1\\): Water treatment and washability are correlated.\n\n\n\\[ \\rho \\neq 0\\]\n\n\nCalculate the test statistic and p-value for Mantel-Haenszel test in R\n\n\nlibrary(DescTools)\nMHChisqTest(water)\n\n\n    Mantel-Haenszel Chi-Square\n\ndata:  water\nX-squared = 50.602, df = 1, p-value = 1.132e-12\n\n\n\nConclusion to the hypothesis test\n\nWe reject the null hypothesis that there is no correlation between washability and water treatment (\\(p = 1.13 \\cdot 10^{-12} &lt; 0.05\\)). There is sufficient evidence that level of water treatment is associated with washability."
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#more-resources",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#more-resources",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "More resources",
    "text": "More resources\n\nReview from my EPI 525 class in F25\n\nSchedule from class: Lessons 15, 16, 19\n\nFor a refresher or review of one proportion and differences in proportions\n\nAnd their power calculations\nFrom Meike’s BSTA 511 course (see Day 12!)\n\nFor a refresher or review of Chi-squared test or Fisher’s Exact test\n\nFrom Meike’s BSTA 511 course (see Day 13!)"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-1",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-1",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1\nMake sure to remember your answer!! We’ll use this on Wednesday!"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#review-of-test-of-association-12",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#review-of-test-of-association-12",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Review of Test of Association (1/2)",
    "text": "Review of Test of Association (1/2)\n\nLast class: learned some tests of association for contingency tables\n\n \n\nFor studies with two independent samples\n\nGeneral association\n\nChi-squared test\nFisher’s Exact test\n\nTest of trends\n\nCochran-Armitage test\nMantel-Haenszel test"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#review-of-test-of-association-22",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#review-of-test-of-association-22",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Review of Test of Association (2/2)",
    "text": "Review of Test of Association (2/2)"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#test-of-association-does-not-measure-association",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#test-of-association-does-not-measure-association",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Test of association does not measure association",
    "text": "Test of association does not measure association\n\nTest of association does not provide an effective measure of association.\n\n \n\nThe p-value alone is not enough\n\n\\(\\text{p-value} &lt; 0.05\\) suggests there is a statistically significant association between the group and outcome\n\\(\\text{p-value} = 0.00001\\) vs. \\(\\text{p-value} = 0.01\\) does not mean the magnitude of association is different\n\n\n \n\nBut it does not tell how different the risks are between the two groups\n\n \n\nWe want a measurement to quantify different risks across the groups"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#measures-of-association",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#measures-of-association",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Measures of Association",
    "text": "Measures of Association\n\nWhen we have a 2x2 contingency table (binary outcome and explanatory variable) and independent samples, we have three main options to measure of association:\n \n\nRisk difference (RD)\n\n \n\nRelative risk (RR)\n\n \n\nOdds ratio (OR)\n\n\n \nEach measures association by comparing the proportion of successes/failures from each categorical group of our explanatory variable."
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#before-we-discuss-each-further",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#before-we-discuss-each-further",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Before we discuss each further…",
    "text": "Before we discuss each further…\n \nLet’s define the cells within a 2x2 contingency table:\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nThen we can define risk: the proportion of “successes”\n\nRisk of successful response for explanatory group 1: \\(\\text{Risk}_1 = \\dfrac{n_{11}}{n_1}\\)"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#risk-difference-rd",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#risk-difference-rd",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Risk Difference (RD)",
    "text": "Risk Difference (RD)\n\nRisk difference computes the absolute difference in risk for the two groups (from the explanatory variable)\n\n\n\n\nPoint estimate: \\[\\widehat{RD} = \\widehat{p}_1 - \\widehat{p}_1 = \\dfrac{n_{11}}{n_1} - \\dfrac{n_{21}}{n_2}\\]\n\nWith range of point estimate from \\([-1, 1]\\)\n\n\n\n\n\n\n\n\n\n\nApproximate standard error:\n\n\\[ SE_{\\widehat{RD}} = \\sqrt{\\frac{\\hat{p}_1\\cdot(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2\\cdot(1-\\hat{p}_2)}{n_2}}\\]\n\n95% Wald confidence interval for \\(\\widehat{RD}\\):\n\n\\[\\widehat{RD} \\pm 1.96 \\cdot SE_{\\widehat{RD}}\\]"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#recall-the-strong-heart-study",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#recall-the-strong-heart-study",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Recall the Strong Heart Study",
    "text": "Recall the Strong Heart Study\n\n\nThe Strong Heart Study is an ongoing study of American Indians residing in 13 tribal communities in three geographic areas (AZ, OK, and SD/ND). We will look at data from this study examining the incidence of diabetes at a follow-up visit and impaired glucose tolerance (ITG) at baseline (4 years apart).\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Risk Difference",
    "text": "SHS Example: Risk Difference\n\n\n\n\nRisk difference\n\n\nCompute the point estimate and 95% confidence interval for the diabetes risk difference between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\nNeeded steps:\n\nCompute the risk difference\nCompute 95% confidence interval\nInterpret the estimate"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-14",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-14",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Risk Difference (1/4)",
    "text": "SHS Example: Risk Difference (1/4)\n\n\n\n\nRisk difference\n\n\nCompute the point estimate and 95% confidence interval for the diabetes risk difference between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nCompute the risk difference \\[\\widehat{RD}={\\hat{p}}_1 - {\\hat{p}}_2=\\frac{n_{11}}{n_1}-\\frac{n_{21}}{n_2}=\\ \\frac{198}{532}\\ - \\frac{128}{1132}=0.3722−0.1131=0.2591\\]"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-24",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-24",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Risk Difference (2/4)",
    "text": "SHS Example: Risk Difference (2/4)\n\n\n\n\nRisk difference\n\n\nCompute the point estimate and 95% confidence interval for the diabetes risk difference between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nCompute 95% confidence interval\n\n\\[\\begin{aligned} &\\widehat{RD}\\pm z_{\\left(1-\\frac{\\alpha}{2}\\right)}^\\ast \\times SE_{\\widehat{RD}} \\\\\n= &\\widehat{RD}\\pm z_{\\left(1-\\frac{\\alpha}{2}\\right)}^\\ast \\times \\sqrt{\\frac{{\\hat{p}}_1\\ (1-{\\hat{p}}_1)}{n_1}+\\frac{{\\hat{p}}_2(1-{\\hat{p}}_2)}{n_2}\\ }\\\\\n=  & 0.2591 \\pm 1.96\\times \\sqrt{\\frac{0.3722(1-0.3722)}{532}+\\frac{0.1131(1-0.1131)}{1132}\\ }\\\\\n=  & (0.2141,\\ 0.3041 )\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-34",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-34",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Risk Difference (3/4)",
    "text": "SHS Example: Risk Difference (3/4)\n\n\n\n\nRisk difference\n\n\nCompute the point estimate and 95% confidence interval for the diabetes risk difference between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n1/2. Compute risk difference and 95% confidence interval\n\nfmsb::riskdifference(198, 128, 532, 1132)\n\n                 Cases People at risk         Risk\nExposed    198.0000000    532.0000000    0.3721805\nUnexposed  128.0000000   1132.0000000    0.1130742\nTotal      326.0000000   1664.0000000    0.1959135\n\n\n\n    Risk difference and its significance probability (H0: The difference\n    equals to zero)\n\ndata:  198 128 532 1132\np-value &lt; 2.2e-16\n95 percent confidence interval:\n 0.2140779 0.3041346\nsample estimates:\n[1] 0.2591062"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-44",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-44",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Risk Difference (4/4)",
    "text": "SHS Example: Risk Difference (4/4)\n\n\n\n\nRisk difference\n\n\nCompute the point estimate and 95% confidence interval for the diabetes risk difference between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nInterpret the estimate\n\nThe diabetes diagnosis risk difference between impaired and normal glucose tolerance is 0.2591 (95% CI: 0.2141, 0.3041). Since the 95% confidence interval does not contain 0, we have sufficient evidence that the risk of diabetes diagnosis within 4 year follow-up for people with impaired versus normal glucose tolerance is different."
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#when-is-the-risk-difference-misleading",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#when-is-the-risk-difference-misleading",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "When is the risk difference misleading?",
    "text": "When is the risk difference misleading?\n\nThe same risk differences can have very different clinical meanings depending on the risk for each group\n\n \n \n\nExample: for two treatments A and B, we know the risk difference (RD) is 0.009. Is it a meaningful difference?\n\nIf the risk is 0.01 for Trt A and 0.001 for Trt B?\nIf the risk is 0.41 for Trt A and 0.401 for Trt B?\n\n\n \n\nUsing the RD alone to summarize the difference in risks for comparing the two groups can be misleading\n\nThe ratio of risk can provide an informative descriptive measure of the “relative risk”"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#relative-risk-rr",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#relative-risk-rr",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Relative Risk (RR)",
    "text": "Relative Risk (RR)\n\nRelative risk computes the ratio of each group’s proportions of “success”\n\nAlso called risk ratio    \n\n\n\n\n\nPoint estimate: \\[\\widehat{RR}=\\dfrac{\\hat{p}_1}{\\hat{p}_2} = \\dfrac{n_{11}/n_1}{n_{21}/n_2}\\]\n\nRange: \\([0, \\infty]\\)"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-2",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-2",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#log-transformation-of-rr",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#log-transformation-of-rr",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Log-transformation of RR",
    "text": "Log-transformation of RR\n\nSampling distribution of the relative risk is highly skewed unless sample sizes are quite large\n\nLog transformation results in approximately normal distribution\nThus, compute confidence interval using normally distributed, log-transformed RR\nThen we convert back to the RR\n\nWe take the log (natural log) of RR: \\(\\ln(\\widehat{RR})\\) or \\(log(\\widehat{RR})\\)\n\nWhenever I say “log” I mean natural log (base \\(e\\), very common in statistics)\n\n\n\n\n\nThen we need to find approximate standard error for \\(\\ln(\\widehat{RR})\\) \\[SE_{\\ln(\\widehat{RR})}=\\sqrt{\\frac{1}{n_{11}}\\ -\\frac{1}{n_1}+\\frac{1}{n_{21}}-\\frac{1}{n_2}}\\]\n95% confidence interval for \\(\\ln(\\widehat{RR})\\): \\[\\ln(\\widehat{RR}) \\pm 1.96 \\times SE_{\\ln(\\widehat{RR})}\\]"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#how-do-we-get-back-to-the-rr-scale",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#how-do-we-get-back-to-the-rr-scale",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "How do we get back to the RR scale?",
    "text": "How do we get back to the RR scale?\n\nWe computed confidence interval using normally distributed, log-transformed RR (\\(\\ln(\\widehat{RR})\\)):\n\n\\[ \\bigg(\\ln(\\widehat{RR}) - 1.96 \\times SE_{\\ln(\\widehat{RR})}, \\ \\ln(\\widehat{RR}) + 1.96 \\times SE_{\\ln(\\widehat{RR})}\\bigg)\\]\n\nNow we need to exponentiate the CI to get back to interpretable values\n\nTake exponential of lower and upper bounds\n\n95% confidence interval for RR: two ways to display equation\n\n\\[ \\bigg(e^{\\ln(\\widehat{RR}) - 1.96 \\times SE_{\\ln(\\widehat{RR})}}, \\ e^{\\ln(\\widehat{RR}) + 1.96 \\times SE_{\\ln(\\widehat{RR})}}\\bigg)\\] \\[ \\bigg(\\exp\\big[\\ln(\\widehat{RR}) - 1.96 \\times SE_{\\ln(\\widehat{RR})}\\big], \\ \\exp\\big[\\ln(\\widehat{RR}) + 1.96 \\times SE_{\\ln(\\widehat{RR})}\\big]\\bigg)\\]"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#relative-risk-rr-1",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#relative-risk-rr-1",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Relative Risk (RR)",
    "text": "Relative Risk (RR)\n\nCan you compute the estimated RRs for the previous example?\n\nIf the risk for Trt A is 0.01 and Trt B is 0.001? \\(\\widehat{RR}= 10\\)\nIf the risk for Trt A is 0.41 and Trt B is 0.401? \\(\\widehat{RR}= 1.02\\)\n\n\n \n\nWhen \\(\\widehat{RR}= 1\\) …\n\nRisk is the same for the two groups\nIn other words, the group and the outcome are independent\n\n\n \n\nWhen computing \\(\\widehat{RR}\\) it is important to identify which variable is the response variable and which is explanatory variable\n\nWe may say “risk for Trt A” but this translates to the risk (or probability) of outcome success for those receiving Trt A"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-16",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-16",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Relative Risk (1/6)",
    "text": "SHS Example: Relative Risk (1/6)\n\n\n\n\nRelative risk\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Relative risk between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\nNeeded steps:\n\nCompute the relative risk\nFind confidence interval of log RR\nConvert back to RR\nInterpret the estimate"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-26",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-26",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Relative Risk (2/6)",
    "text": "SHS Example: Relative Risk (2/6)\n\n\n\n\nRelative risk\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Relative risk between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nCompute the relative risk \\[\\widehat{RR}=\\dfrac{{\\hat{p}}_1}{{\\hat{p}}_2}=\\dfrac{n_{11}/{n_1}}{n_{21}/{n_2}}=\\ \\frac{ 198/532}{128/1132}=\\dfrac{0.3722}{0.1131}=3.2915\\]"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-36",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-36",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Relative Risk (3/6)",
    "text": "SHS Example: Relative Risk (3/6)\n\n\n\n\nRelative risk\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Relative risk between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nFind confidence interval of log RR\n\n\\[\\begin{aligned} & \\ln(\\widehat{RR}) \\pm 1.96 \\times SE_{\\ln(\\widehat{RR})} \\\\\n= &\\ln(\\widehat{RR}) \\pm z_{\\left(1-\\frac{\\alpha}{2}\\right)}^\\ast \\times \\sqrt{\\frac{1}{n_{11}}\\ -\\frac{1}{n_1}+\\frac{1}{n_{21}}-\\frac{1}{n_2}}\\\\\n=  & 1.1913 \\pm 1.96\\times \\sqrt{\\frac{1}{198}\\ -\\frac{1}{532}+\\frac{1}{128}-\\frac{1}{1132}}\\\\\n=  & (0.9944,\\ 1.3883 )\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-46",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-46",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Relative Risk (4/6)",
    "text": "SHS Example: Relative Risk (4/6)\n\n\n\n\nRelative risk\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Relative risk between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nConvert back to RR\n\n\\[\\begin{aligned} & (\\exp(0.9944),\\ \\exp(1.3883 )) \\\\\n= & (2.703,\\ 4.0081 )\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-56",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-56",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Relative Risk (5/6)",
    "text": "SHS Example: Relative Risk (5/6)\n\n\n\n\nRelative risk\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Relative risk between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n1/2/3. Compute risk ratio and 95% confidence interval\n\nlibrary(epitools)\nSHS_ct = table(SHS$glucimp, SHS$case)\nriskratio(x = SHS_ct, rev = \"rows\")$measure\n\n          risk ratio with 95% C.I.\n           estimate    lower    upper\n  Normal   1.000000       NA       NA\n  Impaired 3.291471 2.702998 4.008061"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#pause-other-option-in-pubh-package",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#pause-other-option-in-pubh-package",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Pause: other option in pubh package",
    "text": "Pause: other option in pubh package\n\nSHS = SHS %&gt;% mutate(glucimp = as.factor(glucimp) %&gt;% relevel(ref = \"Normal\"))\ncontingency(case ~ glucimp, data = SHS)\n\n          Outcome\nPredictor     1    0\n  Impaired  198  334\n  Normal    128 1004\n\n             Outcome +    Outcome -      Total                 Inc risk *\nExposed +          198          334        532     37.22 (33.10 to 41.48)\nExposed -          128         1004       1132      11.31 (9.52 to 13.30)\nTotal              326         1338       1664     19.59 (17.71 to 21.58)\n\nPoint estimates and 95% CIs:\n-------------------------------------------------------------------\nInc risk ratio                                 3.29 (2.70, 4.01)\nInc odds ratio                                 4.65 (3.61, 6.00)\nAttrib risk in the exposed *                   25.91 (21.41, 30.41)\nAttrib fraction in the exposed (%)            69.62 (63.00, 75.05)\nAttrib risk in the population *                8.28 (5.63, 10.94)\nAttrib fraction in the population (%)         42.28 (34.71, 48.98)\n-------------------------------------------------------------------\nUncorrected chi2 test that OR = 1: chi2(1) = 154.239 Pr&gt;chi2 = &lt;0.001\nFisher exact test that OR = 1: Pr&gt;chi2 = &lt;0.001\n Wald confidence limits\n CI: confidence interval\n * Outcomes per 100 population units \n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  dat\nX-squared = 152.6, df = 1, p-value &lt; 2.2e-16"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-66",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-66",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Relative Risk (6/6)",
    "text": "SHS Example: Relative Risk (6/6)\n\n\n\n\nRelative risk\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Relative risk between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nInterpret the estimate\n\nThe estimated risk of diabetes is 3.29 times greater for American Indians who had impaired glucose tolerance at baseline compared to those who had normal glucose tolerance (95% CI: 2.70, 4.01).\n \nAdditional interpretation of 95% CI (not needed): We are 95% confident that the (population) relative risk is between 2.70 and 4.01.\n \nSince the 95% confidence interval does not include 1, there is sufficient evidence that the risk of diabetes differs significantly between impaired and normal glucose tolerance at baseline."
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#odds-building-up-to-odds-ratio",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#odds-building-up-to-odds-ratio",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Odds (building up to Odds Ratio)",
    "text": "Odds (building up to Odds Ratio)\n\n\n\nFor a probability of success \\(p\\) (or sometimes referred to as \\(\\pi\\)), the odds of success is: \\[\\widehat{\\text{odds}}=\\frac{\\widehat{p}}{1-\\widehat{p}}=\\frac{\\widehat{\\pi}}{1-\\widehat{\\pi}}\\]\n\nExample: if \\(\\widehat{\\pi}=0.75\\), then odds of success \\(= \\dfrac{0.75}{0.25}=3\\)\n\n\n\n\n\n\n\n\n\n\nIf odds &gt; 1, it implies a success is more likely than a failure\n\nExample: for \\(\\widehat{\\text{odds}} = 3\\), we expect to observe three times as many successes as failures\n\nIf odds is known, the probability of success can be computed \\[\\widehat{\\pi} = \\dfrac{\\widehat{\\text{odds}}}{\\widehat{\\text{odds}}+1}\\]"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#odds-ratio-or",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#odds-ratio-or",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Odds Ratio (OR)",
    "text": "Odds Ratio (OR)\n\n\n\nOdds ratio is the ratio of two odds:\\[\\widehat{OR}=\\frac{\\widehat{\\text{odds}}_1}{\\widehat{\\text{odds}}_2}=\\frac{{\\hat{p}}_1/(1-{\\hat{p}}_1)}{{\\hat{p}}_2/(1-{\\hat{p}}_2)}\\]\n\nRange: \\([0, \\infty]\\)\nInterpretation: The odds of success for “group 1” is “\\(\\widehat{OR}\\)” times the odds of success for “group 2”\n\n\n\n\n\n\n\n\n\n \n\nWhat do values of odds ratios mean?\n\n\n\n\n\n\n\nOdds Ratio\nClinical Meaning\n\n\n\n\n\\(\\widehat{OR} &lt; 1\\)\nOdds of success is smaller in group 1 than in group 2\n\n\n\\(\\widehat{OR} = 1\\)\nExplanatory and response variables are independent\n\n\n\\(\\widehat{OR} &gt; 1\\)\nOdds of success is greater in group 1 than in group 2"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-3",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-3",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#odds-ratio-or-1",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#odds-ratio-or-1",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Odds Ratio (OR)",
    "text": "Odds Ratio (OR)\n\nValues of OR farther from 1.0 in a given direction represent stronger association\n\nAn OR = 4 is farther from independence than an OR = 2\nAn OR = 0.25 is farther from independence than an OR = 0.5\nFor OR = 4 and OR = 0.25, they are equally away from independence (because ¼ = 0.25)\n\n\n \n\nWe take the inverse of the OR for success of group 1 compared to group 2 to get…\n\nOR for failure of group 1 compared to group 2\nOR for success of group 2 compared to group 1"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#log-transformation-of-or",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#log-transformation-of-or",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Log-transformation of OR",
    "text": "Log-transformation of OR\n\nLike RR, sampling distribution of the odds ratio is highly skewed\n\nLog transformation results in approximately normal distribution\nThus, compute confidence interval using normally distributed, log-transformed OR\n\n\n\n\n\nApproximate standard error for \\(\\ln (\\widehat{OR})\\): \\[SE_{\\ln(\\widehat{OR})}=\\sqrt{\\frac{1}{n_{11}}\\ +\\frac{1}{n_{12}}+\\frac{1}{n_{21}}+\\frac{1}{n_{22}}}\\]\n95% confidence interval for \\(\\ln(\\widehat{OR})\\): \\[\\ln(\\widehat{OR}) \\pm 1.96 \\times SE_{\\ln(\\widehat{OR})}\\]"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#how-do-we-get-back-to-the-or-scale",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#how-do-we-get-back-to-the-or-scale",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "How do we get back to the OR scale?",
    "text": "How do we get back to the OR scale?\n\nWe computed confidence interval using normally distributed, log-transformed 0R (\\(\\ln(\\widehat{OR})\\)):\n\n\\[ \\bigg(\\ln(\\widehat{OR}) - 1.96 \\times SE_{\\ln(\\widehat{OR})}, \\ \\ln(\\widehat{OR}) + 1.96 \\times SE_{\\ln(\\widehat{OR})}\\bigg)\\]\n\nNow we need to exponentiate the CI to get back to interpretable values\n\nTake exponential of lower and upper bounds\n\n95% confidence interval for RR: two ways to display equation\n\n\\[ \\bigg(e^{\\ln(\\widehat{OR}) - 1.96 \\times SE_{\\ln(\\widehat{OR})}}, \\ e^{\\ln(\\widehat{OR}) + 1.96 \\times SE_{\\ln(\\widehat{OR})}}\\bigg)\\] \\[ \\bigg(\\exp\\big[\\ln(\\widehat{OR}) - 1.96 \\times SE_{\\ln(\\widehat{OR})}\\big], \\ \\exp\\big[\\ln(\\widehat{OR}) + 1.96 \\times SE_{\\ln(\\widehat{OR})}\\big]\\bigg)\\]"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-16",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-16",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Odds Ratio (1/6)",
    "text": "SHS Example: Odds Ratio (1/6)\n\n\n\n\nOdds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes odds ratio between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\nNeeded steps:\n\nCompute the odds ratio\nFind confidence interval of log OR\nConvert back to OR\nInterpret the estimate"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-26",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-26",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Odds Ratio (2/6)",
    "text": "SHS Example: Odds Ratio (2/6)\n\n\n\n\nOdds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Odds ratio between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nCompute the odds ratio\n\n\\(\\widehat{p}_1 = 198/532 = 0.3722\\), \\(\\widehat{p}_2 = 128/1132 = 0.1131\\) \\[\\widehat{OR}=\\frac{\\widehat{p_1}/(1-\\widehat{p_1})}{\\widehat{p_2}/(1-\\widehat{p_2})}= \\dfrac{0.3722/(1-0.3722)}{0.1131/(1-0.1131)}= 4.6499\\]"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-36",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-36",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Odds Ratio (3/6)",
    "text": "SHS Example: Odds Ratio (3/6)\n\n\n\n\nOdds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Odds ratio between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nFind confidence interval of log OR\n\n\\[\\begin{aligned} & \\ln(\\widehat{OR}) \\pm 1.96 \\times SE_{\\ln(\\widehat{OR})} \\\\\n= &\\ln(\\widehat{OR}) \\pm z_{\\left(1-\\frac{\\alpha}{2}\\right)}^\\ast \\times \\sqrt{\\frac{1}{n_{11}}\\ +\\frac{1}{n_{12}}+\\frac{1}{n_{21}}+\\frac{1}{n_{22}}}\\\\\n=  & 1.5368 \\pm 1.96\\times \\sqrt{\\frac{1}{198}\\ +\\frac{1}{334}+\\frac{1}{128}+\\frac{1}{1004}}\\\\\n=  & (1.2824,\\ 1.7913 )\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-46",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-46",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Odds Ratio (4/6)",
    "text": "SHS Example: Odds Ratio (4/6)\n\n\n\n\nOdds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Odds ratio between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nConvert back to OR\n\n\\[\\begin{aligned} & (\\exp(1.2824),\\ \\exp(1.7913 )) \\\\\n= & (3.6053,\\ 5.9971 )\\end{aligned}\\]"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-56",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-56",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Odds Ratio (5/6)",
    "text": "SHS Example: Odds Ratio (5/6)\n\n\n\n\nOdds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Odds ratio between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n1/2/3. Compute OR and 95% confidence interval\n\nlibrary(epitools)\nSHS_ct = table(SHS$glucimp, SHS$case)\n# no `rev` needed below bc we set the reference level in slide 32\noddsratio(x = SHS_ct, method = \"wald\")$measure \n\n          odds ratio with 95% C.I.\n           estimate    lower    upper\n  Normal   1.000000       NA       NA\n  Impaired 4.649888 3.605289 5.997148"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#pause-other-option-in-pubh-package-1",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#pause-other-option-in-pubh-package-1",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Pause: other option in pubh package",
    "text": "Pause: other option in pubh package\n\ncontingency(case ~ glucimp, data = SHS, digits = 3)\n\n          Outcome\nPredictor     1    0\n  Impaired  198  334\n  Normal    128 1004\n\n             Outcome +    Outcome -      Total                 Inc risk *\nExposed +          198          334        532  37.218 (33.097 to 41.482)\nExposed -          128         1004       1132   11.307 (9.521 to 13.298)\nTotal              326         1338       1664  19.591 (17.709 to 21.581)\n\nPoint estimates and 95% CIs:\n-------------------------------------------------------------------\nInc risk ratio                                 3.291 (2.703, 4.008)\nInc odds ratio                                 4.650 (3.605, 5.997)\nAttrib risk in the exposed *                   25.911 (21.408, 30.413)\nAttrib fraction in the exposed (%)            69.618 (63.004, 75.050)\nAttrib risk in the population *                8.284 (5.631, 10.937)\nAttrib fraction in the population (%)         42.284 (34.713, 48.976)\n-------------------------------------------------------------------\nUncorrected chi2 test that OR = 1: chi2(1) = 154.239 Pr&gt;chi2 = &lt;0.001\nFisher exact test that OR = 1: Pr&gt;chi2 = &lt;0.001\n Wald confidence limits\n CI: confidence interval\n * Outcomes per 100 population units \n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  dat\nX-squared = 152.6, df = 1, p-value &lt; 2.2e-16"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-66",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-66",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Odds Ratio (6/6)",
    "text": "SHS Example: Odds Ratio (6/6)\n\n\n\n\nOdds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Odds ratio between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\n\nInterpret the estimate\n\nThe estimated odds of diabetes for American Indians with impaired glucose tolerance at baseline is 4.65 times the odds for American Indians with normal glucose tolerance at baseline.\n \nAdditional interpretation of 95% CI (not needed): We are 95% confident that the odds ratio is between 3.61 and 6.00.\n \nSince the 95% confidence interval does not include 1, there is sufficient evidence that the odds of diabetes differs significantly between impaired and normal glucose tolerance at baseline."
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#inversing-an-odds-ratio",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#inversing-an-odds-ratio",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Inversing an Odds Ratio",
    "text": "Inversing an Odds Ratio\n\nSome people prefer interpretations of OR &gt; 1 instead of an OR &lt; 1\nThe transformation can easily be done by inverse\n\nRemember we discussed that \\(OR = 4\\) is an equivalent a strong association as \\(OR = 0.25\\) (1/4)\n\n\n \n\nOR comparing group 1 to group 2 = inverse of OR comparing group 2 to group 1\n\n\\[ OR_{1v2}=\\frac{{\\hat{p}}_1/(1-{\\hat{p}}_1)}{{\\hat{p}}_2/(1-{\\hat{p}}_2)}=\\frac{1}{\\frac{{\\hat{p}}_2/(1-{\\hat{p}}_2)}{{\\hat{p}}_1/(1-{\\hat{p}}_1)}}=\\frac{1}{OR_{2v1}}\\]"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-4",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-4",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-inversing-odds-ratio",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-inversing-odds-ratio",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Inversing Odds Ratio",
    "text": "SHS Example: Inversing Odds Ratio\n\n\n\n\nInversing odds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes odds ratio between normal and impaired glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\nNeeded steps:\n\nInverse point estimate and confidence interval\n\n\\[\\widehat{OR}=\\frac{1}{4.6499}=0.2151\\] The 95% Confidence interval is then\n\\[ \\left(\\frac{1}{5.9971}, \\frac{1}{3.6053}\\right)\\ =\\ (0.1667, 0.2774)\\]"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-inversing-odds-ratio-1",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-inversing-odds-ratio-1",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Inversing Odds Ratio",
    "text": "SHS Example: Inversing Odds Ratio\n\n\n\n\nInversing odds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes odds ratio between normal and impaired glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\nNeeded steps:\n\nInverse point estimate and confidence interval\n\n\nlibrary(epitools)\noddsratio(x = SHS_ct, method = \"wald\", rev = \"rows\")$measure \n\n          odds ratio with 95% C.I.\n           estimate     lower     upper\n  Impaired 1.000000        NA        NA\n  Normal   0.215059 0.1667459 0.2773702"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-inversing-odds-ratio-2",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-inversing-odds-ratio-2",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Inversing Odds Ratio",
    "text": "SHS Example: Inversing Odds Ratio\n\n\n\n\nInversing odds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes odds ratio between normal and impaired glucose tolerance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlucose tolerance\n\nDiabetes\n\nTotal\n\n\nNo\nYes\n\n\n\n\nImpaired\n334\n198\n532\n\n\nNormal\n1004\n128\n1132\n\n\nTotal\n1338\n326\n1664\n\n\n\n\n\n\n\n\nNeeded steps:\n\nInterpret the estimate\n\nThe estimated odds of diabetes for American Indians with normal glucose tolerance at baseline is 0.22 times the odds for American Indians with impaired glucose tolerance at baseline.\n \nAdditional interpretation of 95% CI (not needed): We are 95% confident that the odds ratio is between 0.17 and 0.28.\n \nSince the 95% confidence interval does not include 1, there is sufficient evidence that the odds of diabetes differs significantly between impaired and normal glucose tolerance at baseline."
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#pubh-vs.-epitools",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#pubh-vs.-epitools",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "pubh vs. epitools",
    "text": "pubh vs. epitools\n\nIn pubh with contingency()\n\nGet all the info at once\nReally nice to double check how the code is interpreting your input\n\nIn epitools with riskratio() or oddsratio()\n\nMuch easier to grab the numbers!\nIn Quarto you can take R code and directly put it in your text\n\ng = oddsratio(x = SHS_ct, method = \"wald\", rev = \"rows\")\ng$measure[2,1]\n\n[1] 0.215059\n\n\n\nI can write {r eval=\"false\" echo=\"true\"} round(g$measure[2,1], 3) to print the number 0.215"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-1",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-1",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Fisher’s Exact test:",
    "text": "Fisher’s Exact test:\n\n\n\nCheck expected cell counts threshold\n\n\nexpected(GAC_table)\n\n     \n             No      Yes\n  No  143.21983 5.780172\n  Yes  79.78017 3.219828\n\n\nOne cell has an expected count less than 5\n\n\\(\\alpha = 0.05\\)\nHypothesis test:\n\n\\(H_0\\) : There is no association between glucose tolerance and diabetes\n\\(H_1\\) : There is an association between glucose tolerance and diabetes\n\n\n\n\nCalculate the test statistic and p-value for Fisher Exact test\n\n\nfisher.test(x = GAC_table)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  GAC_table\np-value = 0.2877\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  0.4829292 12.0164676\nsample estimates:\nodds ratio \n  2.314727 \n\n\n\nConclusion to the hypothesis test\n\nWe fail to reject the null hypothesis that estimated blood loss and postop neuropathy are not associated (\\(p=0.29\\)). There is insufficient evidence that estimated blood loss and postop neuropathy are associated."
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-2",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-2",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Fisher’s Exact test:",
    "text": "Fisher’s Exact test:\n\n\n\nCheck expected cell counts threshold\n\n\nexpected(GAC_table)\n\n     \n             No      Yes\n  No  143.21983 5.780172\n  Yes  79.78017 3.219828\n\n\nOne cell has an expected cell count less than 5\n\n\\(\\alpha = 0.05\\)\nHypothesis test:\n\n\\(H_0\\) : There is no association between glucose tolerance and diabetes\n\\(H_1\\) : There is an association between glucose tolerance and diabetes\n\n\n\n\nCalculate the test statistic and p-value for Chi-squared test in R\n\n\nfisher.test(x = GAC_table)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  GAC_table\np-value = 0.2877\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  0.4829292 12.0164676\nsample estimates:\nodds ratio \n  2.314727 \n\n\n\nConclusion to the hypothesis test\n\nWe reject the null hypothesis that glucose tolerance and diabetes are not associated (\\(p&lt;2.2\\cdot10^{-16}\\)). There is sufficient evidence that glucose tolerance and diabetes incidence are associated among American Indians."
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-postoperative-neuropathy-risk-in-gender-affirming-care",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-postoperative-neuropathy-risk-in-gender-affirming-care",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Fisher’s Exact test: Postoperative neuropathy risk in gender affirming care",
    "text": "Fisher’s Exact test: Postoperative neuropathy risk in gender affirming care\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEBL &gt; 250ml\n\nPostoperative Neuropathy\n\nTotal\n\n\nNo\nYes\n\n\n\n\nNo\n145\n4\n149\n\n\nYes\n78\n5\n83\n\n\nTotal\n223\n9\n232"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-postoperative-neuropathy-in-gender-affirming-care",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-postoperative-neuropathy-in-gender-affirming-care",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Fisher’s Exact test: Postoperative neuropathy in gender affirming care",
    "text": "Fisher’s Exact test: Postoperative neuropathy in gender affirming care\n\nStudy on new postoperative neuropathy after receiving gender affirming care (Study link)\n\nPostop neuropathy = nerve damage following surgery\n\nStudy comprised of 232 trans men and trans women receiving their respective hormone therapies\nThey measured different aspects of the surgery to see if there was an association with development of postop neuropathy\nGoal in this example: see if estimated blood loss, greater than or less than 250 ml, is associated with postop neuropathy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEBL &gt; 250ml\n\nPostoperative Neuropathy\n\nTotal\n\n\nNo\nYes\n\n\n\n\nNo\n145\n4\n149\n\n\nYes\n78\n5\n83\n\n\nTotal\n223\n9\n232"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-postop-neuropathy-in-gender-affirming-care-12",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-postop-neuropathy-in-gender-affirming-care-12",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Fisher’s Exact test: Postop neuropathy in gender affirming care (1/2)",
    "text": "Fisher’s Exact test: Postop neuropathy in gender affirming care (1/2)\n\nStudy on new postoperative neuropathy after receiving gender affirming care (Study link)\n\nPostop neuropathy = nerve damage following surgery\n\nStudy comprised of 232 trans men and trans women receiving their respective hormone therapies\nThey measured different aspects of the surgery to see if there was an association with development of postop neuropathy\nGoal in this example: see if estimated blood loss, greater than or less than 250 ml, is associated with postop neuropathy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEBL &gt; 250ml\n\nPostoperative Neuropathy\n\nTotal\n\n\nNo\nYes\n\n\n\n\nNo\n145\n4\n149\n\n\nYes\n78\n5\n83\n\n\nTotal\n223\n9\n232"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-postop-neuropathy-in-gender-affirming-care-22",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-postop-neuropathy-in-gender-affirming-care-22",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Fisher’s Exact test: Postop neuropathy in gender affirming care (2/2)",
    "text": "Fisher’s Exact test: Postop neuropathy in gender affirming care (2/2)\n\n\n\nCheck expected cell counts threshold\n\n\nexpected(neuro_table)\n\n     \n             No      Yes\n  No  143.21983 5.780172\n  Yes  79.78017 3.219828\n\n\nOne cell has an expected count less than 5\n\n\\(\\alpha = 0.05\\)\nHypothesis test:\n\n\\(H_0\\) : There is no association between glucose tolerance and diabetes\n\\(H_1\\) : There is an association between glucose tolerance and diabetes\n\n\n\n\nCalculate the test statistic and p-value for Fisher Exact test\n\n\nfisher.test(x = neuro_table)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  neuro_table\np-value = 0.2877\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  0.4829292 12.0164676\nsample estimates:\nodds ratio \n  2.314727 \n\n\n\nConclusion to the hypothesis test\n\nWe fail to reject the null hypothesis that estimated blood loss and postop neuropathy are not associated (\\(p=0.29\\)). There is insufficient evidence that estimated blood loss and postop neuropathy are associated."
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA.html#different-tests-of-association",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA.html#different-tests-of-association",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Different tests of association",
    "text": "Different tests of association"
  },
  {
    "objectID": "instructors.html#academic-success-center-tutors",
    "href": "instructors.html#academic-success-center-tutors",
    "title": "Instructors",
    "section": "Academic Success Center Tutors",
    "text": "Academic Success Center Tutors\nThe Academic Success Center has biostatistics/epidemiology tutors available. Unlike TAs, tutors are not closely associated with any particular course sections or professors, nor do they have any grading responsibilities. Consider meeting with a tutor when you feel stuck on a project, need help with R coding, want to review course concepts, or simply complete assignments in a social learning environment. There is no cost to use tutoring.\nYour tutors this year are Mickey McVeety (MPH Epi program) and Sofia Chapela Lara (PhD program). They are looking forward to working with you in a one-to-one or group setting, depending on your needs. Please schedule to meet with them at least 24 hours in advance. If you have questions about tutoring or need assistance with the schedule, contact one of the tutors or the Academic Success Center: mcveety@ohsu.edu, chapelal@ohsu.edu, learningsupport@ohsu.edu"
  },
  {
    "objectID": "homeworks.html#file-naming",
    "href": "homeworks.html#file-naming",
    "title": "Homework",
    "section": "File Naming",
    "text": "File Naming\n\nFor HW Assignments, please use the following file naming: “Lastname_Firstinitial_HW0.qmd” and “Lastname_Firstinitial_HW0.html”"
  },
  {
    "objectID": "homeworks.html#rubrics",
    "href": "homeworks.html#rubrics",
    "title": "Homework",
    "section": "Rubrics",
    "text": "Rubrics\n\n\n\n\n\n\n\n\n\n1 point\n0 points\n\n\n\n\nCompletion\n75% of the question parts are thoroughly worked out and have an answer. “Question parts” means the sub-questions labeled “Part _”\nLess than 75% of question parts are thoroughly worked out. Attempts do not count as thoroughly worked out."
  },
  {
    "objectID": "homework/HW_01.html",
    "href": "homework/HW_01.html",
    "title": "Homework 1",
    "section": "",
    "text": "Caution\n\n\n\nThis homework is ready to go! (Nicky 4/1/25)"
  },
  {
    "objectID": "homework/HW_01.html#purpose",
    "href": "homework/HW_01.html#purpose",
    "title": "Homework 1",
    "section": "Purpose",
    "text": "Purpose\nThis homework is designed to help you practice the following important skills and knowledge that we covered in Lessons 2-5:\n\nPracticing and outlining your decision process to analyze the relationship between two categorical variables\nInterpreting research aims into questions/tests that can be answered with statistics\nUsing R to calculate sample proportions\nUsing R to calculate test statistic values for inference tests\nInterpreting new phrasing of questions that were introduced in class\nCalculate and interpret the estimated risk difference, relative risk, and odds ratios, and their confidence intervals\nExpand work on contingency tables to evaluate the agreement or reproducibility using Cohen’s Kappa\nUnderstand important differences between linear regression and logistic regression\nConstruct a simple logistic regression model"
  },
  {
    "objectID": "homework/HW_01.html#directions",
    "href": "homework/HW_01.html#directions",
    "title": "Homework 1",
    "section": "Directions",
    "text": "Directions\n\nDownload the .qmd file here.\nYou will not need to download datasets for this homework.\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file\n\nPlease rename you homework as Lastname_Firstinitial_HW1.qmd. This will help organize the homeworks when the TAs grade them.\nPlease also add the following line under subtitle: “BSTA 513/613”: author: First-name Last-name with your first and last name so it is attached to the viewable document.\n\nFor each question, make sure to include all code and resulting output in the html file to support your answers\nShow the work of your calculations using R code within a code chunk. Make sure that both your code and output are visible in the rendered html file. This is the default setting.\nWrite all answers in complete sentences as if communicating the results to a collaborator.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your .qmd file and rendering frequently helps you catch your errors more quickly."
  },
  {
    "objectID": "homework/HW_01.html#questons-part-1",
    "href": "homework/HW_01.html#questons-part-1",
    "title": "Homework 1",
    "section": "Questons Part 1",
    "text": "Questons Part 1\nThe following questions are intended to give you practice in understanding concepts and completing calculations.\n\nQuestion 1\nIf the probability that one white blood cell is a lymphocyte is 0.3, compute the probability of 2 lymphocytes out of 10 white blood cells. Also, compute the probability that at least 3 lymphocytes out of 10 white blood cells. You may calculate by hand, using a web app, or using R.\n\n\nQuestion 2\nConsider a 2 x 2 table from a prospective cohort study:\n\n\n\n\n\n\nFavorable\nUnfavorable\n\n\n\n\nTreatment\n30\n20\n\n\nPlacebo\n10\n60\n\n\n\n\n\n\n\n\nPart a\nEstimate the probability of having favorable results for subjects in the treatment group. Include an interpretation and report with the 95% confidence interval.\n\n\nPart b\nRepeat part a for the placebo group.\n\n\nPart c\nConduct a statistical test to evaluate whether there is an association between group and outcome. What is the name of the test? Make sure to follow the entire test process demonstrated in the slides.\n\n\n\nQuestion 3\nConsider a cohort study with results shown as in following table:\n\n\n\n\n\n\nFavorable\nUnfavorable\n\n\n\n\nTreatment\n6\n1\n\n\nPlacebo\n2\n5\n\n\n\n\n\n\n\nConduct a statistical test to evaluate whether there is an association between group and outcome. What is the name of the test? Make sure to follow the entire test process demonstrated in the slides.\n\n\nQuestion 4\nTable 4 shows the information of a selected group of adolescents on whether they use smokeless tobacco and their perception of risk for using smokeless tobacco.\nTable 4:\n\n\n\n\n\n\nYES\nNO\n\n\n\n\nMinimal\n25\n60\n\n\nModerate\n35\n172\n\n\nSubstantial\n10\n200\n\n\n\n\n\n\n\n\nPart a\nConduct a statistical test to examine general association between adolescent smokeless tobacco users and risk perception. What is the name of the test? Make sure to follow the entire test process demonstrated in the slides.\n\n\nPart b\nIs there a trend of increased risk perception for smokeless tobacco users? What test would you use? Make sure to follow the entire test process demonstrated in the slides.\n\n\n\nQuestion 5\nA study looked at the effects of oral contraceptive (OC) use on heart disease in women 40 to 44 years of age. The researchers prospectively tracked whether or not the women developed a myocardial infarction (MI) over a 3-year period. The table below summarizes their results with columns indicating whether or not women developed MI and rows indicating their OC use.\n\n\n\n\n\n\nYes\nNo\n\n\n\n\nOC users\n13\n4987\n\n\nNon-OC users\n7\n9993\n\n\n\n\n\n\n\n\nPart a\nCompute the estimated risk difference comparing OC users to non-OC users. Include a 95% CI for the estimate and interpretation of the estimated value.\n\n\nPart b\nCompute the estimated relative risk comparing OC users to non-OC users. Include a 95% CI for the estimate and interpretation of the estimated value.\n\n\nPart c\nCompute the estimated odds ratio comparing OC users to non-OC users. Include a 95% CI for the estimate and interpretation of the estimated value.\n\n\nPart d\nIs the OR a good approximation of the RR? Explain why or why not.\n\n\n\nQuestion 6\nOne important aspect of medical diagnosis is its reproducibility. Suppose that two different doctors examine 100 patients for dyspnea in a respiratory-disease clinic, and that doctor A diagnosed 15 patients as having dyspnea (while doctor B did not), doctor B diagnosed 10 patients as having dyspnea (while doctor A did not), and both doctor A and doctor B diagnosed 7 patients as having dyspnea.\n\nPart a\nConstruct a two-way contingency table to summarize the dyspnea diagnoses from doctor A and B.\n\n\nPart b\nCompute the Cohen’s kappa, 95% confidence interval, and interpret the results. What level of agreement does your kappa indicate?"
  },
  {
    "objectID": "homework/HW_01.html#questions-part-2",
    "href": "homework/HW_01.html#questions-part-2",
    "title": "Homework 1",
    "section": "Questions Part 2",
    "text": "Questions Part 2\nThe following questions are intended to give you practice in connecting concepts that will help you make decisions in real world applications.\n\nQuestion 7\nStart making a comprehensive table or outline for the inference tests that we have covered. Here is a list of the tests we have covered:\n\nSingle proportion\nChi-squared test for general association\nFisher’s Exact test for general association\nCochran-Armitage test for trend\nMantel-Haenszel test for linear trend\n\nAnd here is a list of attributes to include:\n\nNumber of variables testing\nTypes of variables\nCriteria (if any)\nHypothesis test\nTest statistic (if we went over it)\nR code for test\nSample size / Power calculation (optional, not discussed in class)\nSpecial notes (optional)\n\nFor example, I could make a table with different rows corresponding to different tests and different columns for each attribute.\n\n\nQuestion 8\nI want you to gain experience exploring a package and function. This is an important skill in coding that can help you grow as an applied statistician.\nIn your previous course, the function lm() was introduced to perform linear regression. In this class, we will heavily use the function glm(). By typing “?glm” in the R console, we can open the Help page for glm(). The following questions ask about the glm() function. You can Google or use R documentation to answer the questions.\nFeel free to read more about the differences between lm() and glm().\n\nPart a\nWhat does the input “family” mean? If I wanted to perform regression using a Poisson distribution, what would I input into family?\n\n\nPart b\nWhat is the default action for the “na.action” input?\n\n\nPart c\nHow does the glm() function fit our model? (Hint: see “method”)\n\n\nPart d\nDo you think the output of summary() will be the same for lm() and glm()?\n\n\n\nQuestion 9\nThis question is meant to emphasize the differences between linear regression and logistic regression. Each part will ask about different aspects of the two regression models. If the question has multiple choice answers, then you must write 1-2 sentences justifying your answer.\n\nPart a\nIn linear regression, what type of variable is our response/outcome variable?\n\nbinary\ncontinuous\ncount\nordinal\n\n\n\nPart b\nIn logistic regression, what type of variable is our response/outcome variable?\n\nbinary\ncontinuous\ncount\nnormal\n\n\n\nPart c\nWhat assumptions in linear regression? Please state the assumption name and characteristics of that assumption.\n\n\nPart d\nWhich assumptions of linear regression are violated if we try to fit a binary response using linear regression? You may choose more than one answer.\n\nIndependence\nLinearity\nNormality\nHomoscedasticity\n\n\n\nPart e\nPlease use our notes on generalized linear models (GLMs) to answer this question. What is the random component used in linear regression? What is the random component used in logistic regression? Name the specific variable type and distribution for it.\n\n\nPart f\nPlease use our notes on generalized linear models (GLMs) to answer this question. What link function do we use in linear regression? What link function do we use in logistic regression? Name the link and write out the function.\n\n\nPart g\nPlease use our notes on generalized linear models (GLMs) to answer this question. What is the systematic component used in simple linear regression? What is the systematic component used in simple logistic regression? Please write out the function for the systematic component for a single covariate.\n\n\nPart h\nHow do we determine our coefficient estimates (estimates of the parameter values) in linear regression? You may choose more than one answer.\n\nOrdinary least squares\nMaximum likelihood estimation\n\n\n\nPart i\nHow do we determine our coefficient estimates (estimates of the parameter values) in logistic regression? You may choose more than one answer.\n\nOrdinary least squares\nMaximum likelihood estimation\n\n\n\n\nQuestion 10\nOPTIONAL\nLet’s make a decision tree on the different tests we learned! I would like you to make a flow chart for the different tests we learned in Classes 1 and 2. You’ll need to include characteristics for:\n\nNumber of variables (1, 2, or 3 - we will go over 3 variables in Class 4)\nNumber of categories in each variable\nSample size is small\nOrdinal/nominal independent variable\nOrdinal/nominal response variable(s)\n\nFor example, if I make a decision tree that includes end nodes for different animals (cat, dog, snake, turtle, and hawk) using yes/no characteristics (has a shell, woof/meows, has fur, or flies), then my flow chart would look like: See my example under Sakai Resources. You are welcome to draw this chart. I used SmartArt under the Insert tab in Word to create mine."
  },
  {
    "objectID": "project/Lab_01_instructions.html",
    "href": "project/Lab_01_instructions.html",
    "title": "Lab 1 Instructions",
    "section": "",
    "text": "Caution\n\n\n\nThis project includes analysis on food insecurity. Please let Nicky know if this is a triggering topic for you. I can find a different dataset for you to work on!"
  },
  {
    "objectID": "project/Lab_01_instructions.html#directions",
    "href": "project/Lab_01_instructions.html#directions",
    "title": "Lab 1 Instructions",
    "section": "1 Directions",
    "text": "1 Directions\nYou can download the .qmd file for this lab here.\nThe above link will take you to your editing file. Please do not remove anything from this editing file!! You will only add your code and work to this file."
  },
  {
    "objectID": "project/Lab_01_instructions.html#lab-activities",
    "href": "project/Lab_01_instructions.html#lab-activities",
    "title": "Lab 1 Instructions",
    "section": "2 Lab activities",
    "text": "2 Lab activities\n\n\n\n\n\n\nNote\n\n\n\nI have left it up to you to load the needed packages for this lab.\n\n\n\n2.1 Reading and listening activities\nI will not check that you have read or listened to any of these, but it is a good starting point for understanding the context of our data: food insecurity in the United States. I haven’t fully read them all yet, but I will be reading and sharing throughout the quarter.\nHere are some articles:\n\nNPR: Millions of American families struggle to get food on the table, report finds\n\nWith option to listen to article\n\nIdentification of factors related to food insecurity and the implications for social determinants of health screenings\nNIMHD’s page on Food Accessibility, Insecurity and Health Outcomes\nFood Insecurity among American Indians and Alaska Natives: A National Profile using the Current Population Survey–Food Security Supplement\nA Framework for Evaluating Social Determinants of Health Screening and Referrals for Assistance\n\n\n\n2.2 Familiarize yourself with the Well-Being and Basic Needs Survey\nPlease read the Urban Institute’s page on the Well-Being and Basic Needs Survey (WBNS) and more of their published information of the survey (at least the first 4 pages). You can also read about the overarching From Safety Net to Solid Ground Initiative that started the survey. Answer the following questions:\n\nWhat is the motivation for this study?\nHow could an analysis that looks at associations with food insecurity help facilitate change in policy?\nWhy is it important to study food insecurity?\n\n\n\n\n\n\n\nTask\n\n\n\nAnswer the following questions using information on WBNS:\n\nWhat is the motivation for this study?\nHow could an analysis that looks at associations with food insecurity help facilitate change in policy?\nWhy is it important to study food insecurity?\n\n\n\n\n\n2.3 File organization\nBefore downloading the data, set up your folders for the class and project, including making an .Rproj file. Make sure you are working with the project by using the here() function to display your working directory.\n\nYou can reference Lesson 2 from BSTA 512/612 for help on folder organization and the here package\n\n\n\n\n\n\n\nTask\n\n\n\nDisplay your working directory using the here package and here() function.\n\n\n\n\n2.4 Access and download the data\n\nGo to the Health and Medical Care Archive page for the Well-Being and Basic Needs Survey\nGo to the Data & Documentation tab \nDownload the R version of the Public use data \nRead and agree to the Terms of Use. After this, you will be redirected to a new page.\nLog into ICPSR by clicking “Access through your institution”. You should be taken to a new page where you need to select “Oregon Health & Science University” \nLogin using standard OHSU login. Then the download should begin!\nMake sure to move this into your project folder under the Data folder.\nTake a look at the folders/files you just downloaded. Make sure to locate and understand the difference between the Codebook, Questionnaire, and User Guide. (Note that the website also contains the codebook if you go to the variable tab. I think the online one has an easier user interface than the pdf.)\n\n\n\n\n\n\n\nTask\n\n\n\nNo task to report back on. Just make sure you have the data!\n\n\n\n\n2.5 Decide on list of variables to focus on\nFrom the codebook, I want you to explore the variables and create a list of 10 predictors that you would like to focus on. Our outcome is FOOD_INSEC so we cannot use this as a predictor. Feel free to take a look at the Urban Institute’s list of publications to get ideas of variables and relationships.\nThere are a few requirements for your predictors:\n\n1 variable must be a numeric (i.e. PPAGE)\n1 variable must be binary\n1 variable must be multi-level categorical (categorical with more than 2 groups)\nYou must choose at least 10 predictors (does not include the outcome)\n\nThere is a good online version of the codebook with information about the variables. I have linked you to the ID varaible, but you can take a look at all the other variables using the left hand side navigator:\n\nYou can look under survey questions to get a better sense of how questions were asked, but please stick to variables under Demographic Variables, Family Income, Insurance Status, and Material Hardship. Do not choose variables from the Administrative levels, Survey Questions, nor School Enrollment or Child Care variables. May leave in the ID variable for easier tracking on individuals, but it does not count towards the 10 predictors.\n\n\n\n\n\n\nTask\n\n\n\nList the 10 predictors that you plan to use in your analysis. Note the type of variable (numeric and continuous, counts, binary, or multi-level categorical)\n\n\n\n\n2.6 Get a sense of how you would like to analyze the data\nFor our project, we will examine the association between the food insecurity and one other variable (our main explanatory variable). From the above readings, survey information, and your list of predictors in Section 2.5, which association are you most interested in analyzing?\nPlease write this in the form of a research question statement. Feel free to copy this sentence and insert your chosen predictor: We will investigate the association between food insecurity and ____.\n\n\n\n\n\n\nTask\n\n\n\nComplete the following statement to identify your research question:\nWe will investigate the association between food insecurity and ____.\n\n\n\n\n2.7 Save data for processing with .Rda\nWithin this document, or in a separate document, use R to save a copy of the dataset so that you can process it without changing the raw data. Recall the file organization that we discussed to set up proper folders. Include a screenshot showing the new .Rda file within your Data folder.\n\n\n\n\n\n\nTask\n\n\n\nInclude a screenshot showing the new .Rda file within your Data folder.\n\n\n\n\n2.8 Getting data in working format\nYou can start by selecting only the variables you will use in your analysis. Again, you can keep ID in addition to your outcome and predictors for easy tracking.\nUse the following code (with your dataset’s name) to remove the parentheses with values that are in front of the category names. Make sure to change old_df and new_df.\n\nnew_df = data.frame(lapply(old_df, function(x) {gsub(\".*) \", \"\", x)}))\n\n\n\n\n\n\n\nTask\n\n\n\n\nSelect the variables that will be used in your analysis and make a new dataset. Include the code that you used.\nRemove the parentheses with values that are in front of the category names.\n\n\n\n\n\n2.9 Explore the outcome and predictors\nThe codebook online gives some nice plots of each variable. Please take a look at the codebook online to see the spread of each variable. Make note of any categorical variables that have less than 100 observations in a group. This may cause issues in our analysis later.\n\n\n\n\n\n\nTask\n\n\n\n\nTo check that you have looked that the variables, please report the percent of respondents that were food insecure in the past 12 months.\nList any categorical variables that have less than 100 observations in a group\n\n\n\n\n\n2.10 Compile above work into an introduction\nPlease check out this source for what a research article introduction includes and how to organize it. The only thing I would add is mentioning the Well-Being and Basic Needs Survey.\n\n\n\n\n\n\nTask\n\n\n\nWrite an introduction to the analysis using 3-5 bullets."
  },
  {
    "objectID": "homework/HW_05.html",
    "href": "homework/HW_05.html",
    "title": "Homework 5",
    "section": "",
    "text": "Caution\n\n\n\nNicky needs to edit"
  },
  {
    "objectID": "homework/HW_05.html#directions",
    "href": "homework/HW_05.html#directions",
    "title": "Homework 5",
    "section": "Directions",
    "text": "Directions\n\nDownload the .qmd file here.\nYou will need to download the datasets. Use this link to download the homework datasets needed in this assignment. If you do not want to make changes to the paths set in this document, then make sure the files are stored in a folder named “data” that is housed in the same location as this homework .qmd file.\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file\n\nPlease rename you homework as Lastname_Firstinitial_HW5.qmd. This will help organize the homeworks when the TAs grade them.\nPlease also add the following line under subtitle: \"BSTA 513/613\": author: First-name Last-name with your first and last name so it is attached to the viewable document.\n\nFor each question, make sure to include all code and resulting output in the html file to support your answers.\nShow the work of your calculations using R code within a code chunk. Make sure that both your code and output are visible in the rendered html file. This is the default setting.\nIf you are computing something by hand, you may take a picture of your work and insert the image in this file. You may also use LaTeX to write it inline.\nWrite all answers in complete sentences as if communicating the results to a collaborator. This means including a sentence summarizing results in the context of the research study.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your qmd file and rendering frequently helps you catch your errors more quickly."
  },
  {
    "objectID": "homework/HW_05.html#question-1",
    "href": "homework/HW_05.html#question-1",
    "title": "Homework 5",
    "section": "Question 1",
    "text": "Question 1\nThis question stems from an example from an online textbook by Dr. Ramzi W. Nahhas. The dataset for this problem includes a subset of individuals from the 2019 National Survey on Drug Use and Health (NSDUH). Overall, our study aims included investigating potential risk factors for lifetime heroin use. Lifetime heroin use is a binary outcome, which we regress on age at first use of alcohol (alc_agefirst), age with 6 categories (demog_age_cat6), and sex assigned at birth (demog_sex).\n\nload(here(\"data\", \"nsduh2019_adult_sub_rmph.RData\"))\nnsduh = nsduh_adult_sub %&gt;% \n  dplyr::select(her_lifetime, alc_agefirst, demog_age_cat6, demog_sex) %&gt;% \n  drop_na()\n\n\nPart a\nUsing the nsduh dataset from the above chunk of code, please run a regression model and present the model summary using lifetime heroin use as our outcome, and age at first use of alcohol, categorical age, and sex assigned at birth as covariates in our model. No need to write out your model, you just need to write the R code to run it.\n\n\nPart b\nAre we encountering a numerical problem with our regression? If yes, please name the numerical issue. What first clued you into that issue? Provide conclusive evidence of this numerical issue (with a contingency table), and explain which variable(s) are causing this problem.\n\n\nPart c\nWhat would you do to “fix” this numerical issue? Please apply your “fix” and rerun the regression"
  },
  {
    "objectID": "homework/HW_05.html#question-2",
    "href": "homework/HW_05.html#question-2",
    "title": "Homework 5",
    "section": "Question 2",
    "text": "Question 2\nThis question is taken from the Hosmer and Lemeshow textbook. The ICU study data set consists of a sample of 200 subjects who were part of a much larger study on survival of patients following admission to an adult intensive care unit (ICU). The dataset should be available within Course Materials. The major goal of this study was to develop a logistic regression model to predict the probability of survival to hospital discharge of these patients. In this question, the primary outcome variable is vital status at hospital discharge, STA. Clinicians associated with the study felt that a key determinant of survival was the patient’s age at admission, AGE. We will be building to a multivariable logistic regression model while adjusting for cancer part of the present problem (CAN), CPR prior to ICU admission (CPR), infection probable at ICU admission (INF), and level of consciousness at ICU admission (LOC).\nWe will use the model from Homework 4 Question 1a for this question: \\[\\text{logit}(\\pi(\\textbf{X}))=\\beta_0 + \\beta_1 \\cdot AGE + \\beta_2 \\cdot CAN + \\beta_3 \\cdot CPR + \\\\ \\beta_4 \\cdot INF\\]\n\nicu = read_csv(here(\"data\", \"icu.csv\"))\nicu1 = icu %&gt;% mutate(STA = as.factor(STA) %&gt;% relevel(ref = \"Lived\"))\nicu2 = icu1 %&gt;% mutate(CAN = as.factor(CAN) %&gt;% relevel(ref = \"No\"), \n                     CPR = as.factor(CPR) %&gt;% relevel(ref = \"No\"), \n                     INF = as.factor(INF) %&gt;% relevel(ref = \"No\"), \n                     LOC = as.factor(LOC) %&gt;% \n                       relevel(ref = \"No Coma or Deep Stupor\"))\n\n\nPart a\nAssess the fit of the above model. You may use Hosmer-Lemeshow test, or Pearson Residual as appropriate. Discuss your choice and interpret.\n\n\nPart b\nAssess the your models ability to discriminate vital status (STA) using AUC.\n\n\nPart c\nLet’s say a colleague found a different preliminary final model than yours. Using the below model that your colleague found, compare your model to theirs using AIC and BIC.\n\ncoll_model = glm(STA ~ SYS + AGE + CPR + INF + AGE*CPR, \n                 data = icu2, family = \"binomial\")\ncoll_model\n\n\nCall:  glm(formula = STA ~ SYS + AGE + CPR + INF + AGE * CPR, family = \"binomial\", \n    data = icu2)\n\nCoefficients:\n(Intercept)          SYS          AGE       CPRYes       INFYes   AGE:CPRYes  \n   -1.47960     -0.01343      0.02340     -3.37369      0.53449      0.08370  \n\nDegrees of Freedom: 199 Total (i.e. Null);  194 Residual\nNull Deviance:      200.2 \nResidual Deviance: 172.5    AIC: 184.5"
  },
  {
    "objectID": "homework/HW_03.html",
    "href": "homework/HW_03.html",
    "title": "Homework 3",
    "section": "",
    "text": "Caution\n\n\n\nNicky needs to edit"
  },
  {
    "objectID": "homework/HW_03.html#directions",
    "href": "homework/HW_03.html#directions",
    "title": "Homework 3",
    "section": "Directions",
    "text": "Directions\n\nDownload the .qmd file here.\nYou will need to download the datasets. Use this link to download the homework datasets needed in this assignment. If you do not want to make changes to the paths set in this document, then make sure the files are stored in a folder named “data” that is housed in the same location as this homework .qmd file.\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file\n\nPlease rename you homework as Lastname_Firstinitial_HW0.qmd. This will help organize the homeworks when the TAs grade them.\nPlease also add the following line under subtitle: \"BSTA 512/612\": author: First-name Last-name with your first and last name so it is attached to the viewable document.\n\nFor each question, make sure to include all code and resulting output in the html file to support your answers.\nShow the work of your calculations using R code within a code chunk. Make sure that both your code and output are visible in the rendered html file. This is the default setting.\nIf you are computing something by hand, you may take a picture of your work and insert the image in this file. You may also use LaTeX to write it inline.\nWrite all answers in complete sentences as if communicating the results to a collaborator. This means including a sentence summarizing results in the context of the research study.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your qmd file and rendering frequently helps you catch your errors more quickly."
  },
  {
    "objectID": "homework/HW_03.html#questions",
    "href": "homework/HW_03.html#questions",
    "title": "Homework 3",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1\nThis question is taken from the Hosmer and Lemeshow textbook. The ICU study data set consists of a sample of 200 subjects who were part of a much larger study on survival of patients following admission to an adult intensive care unit (ICU). The dataset should be available within Course Materials. The major goal of this study was to develop a logistic regression model to predict the probability of survival to hospital discharge of these patients. In this question, the primary outcome variable is vital status at hospital discharge, STA. Clinicians associated with the study felt that a key determinant of survival was the patient’s age at admission, AGE. We will be building to a multivariable logistic regression model while adjusting for cancer part of the present problem (CAN), CPR prior to ICU admission (CPR), infection probable at ICU admission (INF), and level of consciousness at ICU admission (LOC).\nA code sheet for the variables to be considered is displayed in Table 1.5 below (from the Hosmer and Lemeshow textbook, pg. 23). We refer to this data set as the ICU data.\n\n\nPart a\nFrom the above list (AGE, CAN, CPR, INF, and LOC) of independent variables, identify if each is a continuous, binary, or multi-level (&gt;2) categorical variable.\n\n\nPart b\nFor the binary and multi-level categorical variables, please identify a reference group for each. Include justification for the reference group.\n\n\nPart c\nRefer back to Part c from Homework 2’s Question 1. Interpret the odds ratio for age in the simple logistic regression model. Please include the 95% confidence interval.\n\n\nPart d\nCompute the predicted probability of hospital discharge for a subject who is 63 years old. Compute the 95% confidence interval for the predicted probability and interpret the predicted probability.\n\n\nPart e\nFor the categorical variables (binary and multi-group), please mutate the variables within the ICU dataset to set your chosen reference groups.\n\n\nPart f\nWrite down the equation for the logistic regression model of STA on CPR.\n\n\nPart g\nUsing the glm() function, obtain the maximum likelihood estimates of the coefficient parameters of the logistic regression model in Part f. Using these estimates, write down fitted logistic regression model.\n\n\nPart h\nWrite a sentence interpreting the odds ratio for the coefficients in Part g’s model. Please include the 95% confidence interval.\n\n\nPart i\nWrite down the equation for the logistic regression model of STA on LOC.\n\n\nPart j\nUsing the glm() function, obtain the maximum likelihood estimates of the coefficient parameters of the logistic regression model in Part i. Present the coefficient estimates. No need to write out the fitted regression equation.\nPlease take note of the warnings that you receive from fitting the glm() model and any large coefficient estimate with large confidence intervals. In this case, we have a category within LOC that has very few observations. (We will discuss this more in Lesson 14: Numerical Problems)\nCheck the number of observations that have a deep stupor and death at discharge and the number of observations that have a deep stupor and live at discharge. You can do this using the table() function to create a contingency table.\n\n\nPart k\nWrite a sentence interpreting the odds ratio of death for the indicator of coma. Please include the 95% confidence interval."
  },
  {
    "objectID": "homework/HW_02.html",
    "href": "homework/HW_02.html",
    "title": "Homework 2",
    "section": "",
    "text": "Caution\n\n\n\nNicky needs to edit"
  },
  {
    "objectID": "homework/HW_02.html#purpose",
    "href": "homework/HW_02.html#purpose",
    "title": "Homework 2",
    "section": "Purpose",
    "text": "Purpose\nThis homework is designed to help you practice the following important skills and knowledge that we covered in Lessons 3-6:\n\nTest a covariate for significance using the Wald test and LRT"
  },
  {
    "objectID": "homework/HW_02.html#directions",
    "href": "homework/HW_02.html#directions",
    "title": "Homework 2",
    "section": "Directions",
    "text": "Directions\n\nDownload the .qmd file here.\nYou will need to download the datasets. Use this link to download the homework datasets needed in this assignment. If you do not want to make changes to the paths set in this document, then make sure the files are stored in a folder named “data” that is housed in the same location as this homework .qmd file.\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file\n\nPlease rename you homework as Lastname_Firstinitial_HW1.qmd. This will help organize the homeworks when the TAs grade them.\nPlease also add the following line under subtitle: “BSTA 513/613”: author: First-name Last-name with your first and last name so it is attached to the viewable document.\n\nFor each question, make sure to include all code and resulting output in the html file to support your answers\nShow the work of your calculations using R code within a code chunk. Make sure that both your code and output are visible in the rendered html file. This is the default setting.\nWrite all answers in complete sentences as if communicating the results to a collaborator.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your qmd file and rendering frequently helps you catch your errors more quickly."
  },
  {
    "objectID": "homework/HW_02.html#questions",
    "href": "homework/HW_02.html#questions",
    "title": "Homework 2",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1\nThis question is taken from the Hosmer and Lemeshow textbook. The ICU study data set consists of a sample of 200 subjects who were part of a much larger study on survival of patients following admission to an adult intensive care unit (ICU). The dataset should be available within Course Materials. The major goal of this study was to develop a logistic regression model to predict the probability of survival to hospital discharge of these patients. In this question, the primary outcome variable is vital status at hospital discharge, STA. Clinicians associated with the study felt that a key determinant of survival was the patient’s age at admission, AGE.\nA code sheet for the variables to be considered is displayed in Table 1.5 below (from the Hosmer and Lemeshow textbook, pg. 23). We refer to this data set as the ICU data.\n\n\nPart a\nWrite down the equation for the logistic regression model of STA on AGE. What characteristic of the outcome variable, STA, leads us to consider the logistic regression model as opposed to the usual linear regression model to describe the relationship between STA and AGE?\n\n\nPart b\nWrite down an expression for the log-likelihood for the logistic regression model in Part a. This will me a mathematical expression. Please do not use generic expressions like \\(\\pi(X)\\), instead replace \\(X\\) with the specific variables in this question.\n\n\nPart c\nUsing the glm() function, obtain the maximum likelihood estimates of the coefficient parameters of the logistic regression model in Part a. Using these estimates, write down fitted logistic regression model.\n\n\nPart d\nUse the Wald test to test whether or not the intercept (\\(\\beta_0\\)) of the logistic regression model is significantly different from 0. Make sure to include your: hypothesis test, code/work leading to the computed test statistic, output including the test statistic and p-value, and conclusion. Please refer to the Additional Tips to guide you on what a complete/correct answer contains.\n\n\nPart e\nUse the Likelihood Ratio test to test whether or not the coefficient for age (\\(\\beta_1\\)) of the logistic regression model is significantly different from 0. Make sure to include your: hypothesis test, code/work leading to the computed test statistic, output including the test statistic and p-value, and conclusion. Please refer to the Additional Tips to guide you on what a complete/correct answer contains. You do not need to include an interpretation of the coefficient since we have not covered this.\n\n\n\nQuestion 2\nSame as above\n\nPart a\nFrom the above list (AGE, CAN, CPR, INF, and LOC) of independent variables, identify if each is a continuous, binary, or multi-level (&gt;2) categorical variable.\n\n\nPart b\nFor the binary and multi-level categorical variables, please identify a reference group for each. Include justification for the reference group.\n\n\nPart c\nRefer back to Part c from Homework 2’s Question 1. Interpret the odds ratio for age in the simple logistic regression model. Please include the 95% confidence interval.\n\n\nPart d\nCompute the predicted probability of hospital discharge for a subject who is 63 years old. Compute the 95% confidence interval for the predicted probability and interpret the predicted probability.\n\n\nPart e\nFor the categorical variables (binary and multi-group), please mutate the variables within the ICU dataset to set your chosen reference groups.\n\n\nPart f\nWrite down the equation for the logistic regression model of STA on CPR.\n\n\nPart g\nUsing the glm() function, obtain the maximum likelihood estimates of the coefficient parameters of the logistic regression model in Part f. Using these estimates, write down fitted logistic regression model.\n\n\nPart h\nWrite a sentence interpreting the odds ratio for the coefficients in Part g’s model. Please include the 95% confidence interval.\n\n\nPart i\nWrite down the equation for the logistic regression model of STA on LOC.\n\n\nPart j\nUsing the glm() function, obtain the maximum likelihood estimates of the coefficient parameters of the logistic regression model in Part i. Present the coefficient estimates. No need to write out the fitted regression equation.\nPlease take note of the warnings that you receive from fitting the glm() model and any large coefficient estimate with large confidence intervals. In this case, we have a category within LOC that has very few observations. (We will discuss this more in Lesson 14: Numerical Problems)\nCheck the number of observations that have a deep stupor and death at discharge and the number of observations that have a deep stupor and live at discharge. You can do this using the table() function to create a contingency table.\n\n\nPart k\nWrite a sentence interpreting the odds ratio of death for the indicator of coma. Please include the 95% confidence interval."
  },
  {
    "objectID": "homework/HW_04.html",
    "href": "homework/HW_04.html",
    "title": "Homework 4",
    "section": "",
    "text": "Caution\n\n\n\nNicky needs to edit"
  },
  {
    "objectID": "homework/HW_04.html#directions",
    "href": "homework/HW_04.html#directions",
    "title": "Homework 4",
    "section": "Directions",
    "text": "Directions\n\nDownload the .qmd file here.\nYou will need to download the datasets. Use this link to download the homework datasets needed in this assignment. If you do not want to make changes to the paths set in this document, then make sure the files are stored in a folder named “data” that is housed in the same location as this homework .qmd file.\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file\n\nPlease rename you homework as Lastname_Firstinitial_HW0.qmd. This will help organize the homeworks when the TAs grade them.\nPlease also add the following line under subtitle: \"BSTA 513/613\": author: First-name Last-name with your first and last name so it is attached to the viewable document.\n\nFor each question, make sure to include all code and resulting output in the html file to support your answers.\nShow the work of your calculations using R code within a code chunk. Make sure that both your code and output are visible in the rendered html file. This is the default setting.\nIf you are computing something by hand, you may take a picture of your work and insert the image in this file. You may also use LaTeX to write it inline.\nWrite all answers in complete sentences as if communicating the results to a collaborator. This means including a sentence summarizing results in the context of the research study.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your qmd file and rendering frequently helps you catch your errors more quickly."
  },
  {
    "objectID": "homework/HW_04.html#questions",
    "href": "homework/HW_04.html#questions",
    "title": "Homework 4",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1\nThis question is taken from the Hosmer and Lemeshow textbook. The ICU study data set consists of a sample of 200 subjects who were part of a much larger study on survival of patients following admission to an adult intensive care unit (ICU). The dataset should be available within Course Materials. The major goal of this study was to develop a logistic regression model to predict the probability of survival to hospital discharge of these patients. In this question, the primary outcome variable is vital status at hospital discharge, STA. Clinicians associated with the study felt that a key determinant of survival was the patient’s age at admission, AGE. We will be building to a multivariable logistic regression model while adjusting for cancer part of the present problem (CAN), CPR prior to ICU admission (CPR), infection probable at ICU admission (INF), and level of consciousness at ICU admission (LOC).\nA code sheet for the variables to be considered is displayed in Table 1.5 below (from the Hosmer and Lemeshow textbook, pg. 23). We refer to this data set as the ICU data.\nYou will need to use some of the transformations of variables from Homework 3, Question 1, Part e.\n\n\nPart a\nWrite down the equation for the logistic regression model of STA on AGE, CAN, CPR, and INF. How many parameters does this model contain?\n\n\nPart b\nUsing glm(), obtain the maximum likelihood estimates of the parameters of the logistic regression model in Part a. Using these estimates, write down the equation with the fitted values.\n\n\nPart c\nAssess the significance of the group of coefficients for all variables in the model using the likelihood ratio test. (Hint: part of the ratio in the LRT will be an intercept only model)\n\n\nPart d\nFit a new model using only CAN and INF as the predictors, including an interaction between CAN and INF. Write a sentence interpreting the odds ratio for the main effects in the model. Please include the 95% confidence interval.\n\n\nPart e\nFrom the above model, fill out the following table for the odds ratios. Note, you will only need to report two odds ratios and you already have one from Part d.\n\n\n\n\n\n\n\n\nCancer\nInfection\nOdds ratio\n\n\n\n\nCancer part of present problem\nInfection probable at ICU intake\n\n\n\n\n      No\n\n\n\n\n      Yes\nFILL HERE\n\n\nCancer not part of present problem\nInfection probable at ICU intake\n\n\n\n\n      No\n\n\n\n\n      Yes\nFILL HERE\n\n\n\nThis is a really good way to report odds ratios for interactions between two categorical predictors! Might want to keep this in mind for your project!!\n\n\nPart f\nCompute the predicted probability for a subject who does not have a present issue with cancer nor an infection upon admittance to the ICU. Compute the 95% confidence interval for the predicted probability. Can you use the Normal approximation? Interpret the predicted probability including the confidence interval."
  },
  {
    "objectID": "project/Lab_03_instructions.html",
    "href": "project/Lab_03_instructions.html",
    "title": "Lab 3 Instructions",
    "section": "",
    "text": "Caution\n\n\n\nNicky needs to edit"
  },
  {
    "objectID": "project/Lab_03_instructions.html#directions",
    "href": "project/Lab_03_instructions.html#directions",
    "title": "Lab 3 Instructions",
    "section": "1 Directions",
    "text": "1 Directions\nYou can download the .qmd file for this lab here.\nThe above link will take you to your editing file. Please do not remove anything from this editing file!! You will only add your code and work to this file.\n\n1.1 Purpose\nThe purpose of this lab is to fit a multiple logistic regression model and practice how we would interpret our results for this study.\n\n\n1.2 Grading\nThis lab is graded out of 12 points. The TAs will go through and grade your lab. They will make sure each section is complete and will follow the rubric below. I have instructed them that completion and clear effort is all that is needed to receive 100%. Nicky will go through the labs to give you feedback.\n\n1.2.1 Rubric\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nFormatting\nLab submitted on Sakai with .html file. Answers are written in complete sentences with no major grammatical nor spelling errors. With little editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with grammatical or spelling errors. With editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with major grammatical or spelling errors. With major editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are bulletted or do not use complete sentences.\nLab not submitted on Sakai with .html file.\n\n\nCode/Work\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMost tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nSome tasks are directly followed or answered.This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMore than a quarter of the tasks are not completed properly.\n\n\nReasoning*\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful and can be easily integrated into the final report.\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful, but lack the clarity needed to easily integrate into the final report.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers are fairly thoughtful, but lack connection to the research.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers seem rushed and with minimal thought.\nAnswers lack understanding of research context and investigation of the data. Answers seem rushed and without thought.\n\n\n\n*Applies to questions with reasoning"
  },
  {
    "objectID": "project/Lab_03_instructions.html#lab-activities",
    "href": "project/Lab_03_instructions.html#lab-activities",
    "title": "Lab 3 Instructions",
    "section": "2 Lab activities",
    "text": "2 Lab activities\n\n\n\n\n\n\nNote\n\n\n\nI have left it up to you to load the needed packages for this lab.\n\n\n\n2.1 Restate research question\n\n\n\n\n\n\nTask\n\n\n\nPlease restate your research question below using the provided format (1 sentence). You can change the wording if you’d like, but please make sure it is still clear. It’s repetitive, but it helps me contextualize my feedback as I look through your lab.\n\n\nIn this study, we will investigate the association between food insecurity and ________.\n\n\n2.2 Fit a multiple logistic regression model\n\n\n\n\n\n\nTask\n\n\n\nFit a main effects model with 5 of your covariates. This will not necessarily be your final model, but we can construct interpretations and code that will be useful in your final model.\n\n\n\n\n2.3 Present the odds ratio in a table\n\n\n\n\n\n\nTask\n\n\n\nUse the tbl_regression() function to make a table of the odds ratios from your fitted model.\n\n\n\n\n2.4 Present the odds ratio in a forest plot\n\n\n\n\n\n\nTask\n\n\n\nUse the code from Lesson 10 to make a forest plot of the odds ratio from your fitted model.\n\n\n\n\n2.5 Interpret the odds ratio for your main covariate\n\n\n\n\n\n\nTask\n\n\n\nInterpret the odds ratio(s) for the covariate from your research question. Make sure to include the 95% confidence intervals and what other variables you adjusted for."
  },
  {
    "objectID": "project/Lab_02_instructions.html",
    "href": "project/Lab_02_instructions.html",
    "title": "Lab 2 Instructions",
    "section": "",
    "text": "Caution\n\n\n\nNicky needs to edit"
  },
  {
    "objectID": "project/Lab_02_instructions.html#directions",
    "href": "project/Lab_02_instructions.html#directions",
    "title": "Lab 2 Instructions",
    "section": "1 Directions",
    "text": "1 Directions\nYou can download the .qmd file for this lab here.\nThe above link will take you to your editing file. Please do not remove anything from this editing file!! You will only add your code and work to this file.\n\n1.1 Purpose\nThe purpose of this lab is to explore our data further, set up the unadjusted odds ratio, and create code to later help us present our final model.\n\n\n1.2 Grading\nThis lab is graded out of 12 points. The TAs will go through and grade your lab. They will make sure each section is complete and will follow the rubric below. I have instructed them that completion and clear effort is all that is needed to receive 100%. Nicky will go through the labs to give you feedback.\n\n1.2.1 Rubric\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nFormatting\nLab submitted on Sakai with .html file. Answers are written in complete sentences with no major grammatical nor spelling errors. With little editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with grammatical or spelling errors. With editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with major grammatical or spelling errors. With major editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are bulletted or do not use complete sentences.\nLab not submitted on Sakai with .html file.\n\n\nCode/Work\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMost tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nSome tasks are directly followed or answered.This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMore than a quarter of the tasks are not completed properly.\n\n\nReasoning*\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful and can be easily integrated into the final report.\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful, but lack the clarity needed to easily integrate into the final report.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers are fairly thoughtful, but lack connection to the research.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers seem rushed and with minimal thought.\nAnswers lack understanding of research context and investigation of the data. Answers seem rushed and without thought.\n\n\n\n*Applies to questions with reasoning"
  },
  {
    "objectID": "project/Lab_02_instructions.html#lab-activities",
    "href": "project/Lab_02_instructions.html#lab-activities",
    "title": "Lab 2 Instructions",
    "section": "2 Lab activities",
    "text": "2 Lab activities\n\n\n\n\n\n\nNote\n\n\n\nI have left it up to you to load the needed packages for this lab.\n\n\n\n2.1 Restate research question\n\n\n\n\n\n\nTask\n\n\n\nPlease restate your research question below using the provided format (1 sentence). You can change the wording if you’d like, but please make sure it is still clear. It’s repetitive, but it helps me contextualize my feedback as I look through your lab.\n\n\nIn this study, we will investigate the association between food insecurity and ________.\n\n\n2.2 Make sure variables are coded correctly\nUse class() to determine the class of each of the 11 variables you selected from Lab 1 (including the outcome). A tidyverse equivalent to the apply() function that we learned last quarter is map(). Please take a look at the description of the map() function.\nMake sure the class that R recognizes is the class that you expect the variable to be. Categorical variables should be factors amd numeric variables should be numeric. It is very important that your outcome, food insecurity, is a factor with the reference level set to “No.” For example, if I am using age, but the class is character, I will need to convert age to a numeric variable. If I have a categorical covariate that is recognized as a character, I should convert it to a factor with a specific reference level.\n\ndf_name %&gt;% map(class)\n\n\n\n\n\n\n\nTask\n\n\n\n\nUse class() to determine the class of each of the 11 variables you selected from Lab 1 (including the outcome).\nChange the variable type to the appropriate type.\n\n\n\n\n\n2.3 Consider potential confounders and effect modifiers\nFor each of the 10 predictor variables, fill out the below table. Determine whether you think each variable will be a confounder, effect modifier, or nothing in relation to your main variable and food insecurity. This does not need to be extensive reasoning. If you would like to present this information in another way, you may.\n\n\n\n\n\n\nTask\n\n\n\nFill in the below table (or any other way you wish to present the same information).\n\n\n\n\n\n\n\n\n\n\nVariable name\nConfounder, Effect modifier, or nothing?\nReasoning (1-2 sentences)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.4 Create contingency tables for categorical predictors\nFor each categorical covariate, create a contingency table between it and food insecurity. You can create a data frame with only categorical covariattes, then use lapply() to make a table for each column. Take note of any cells that have less than 10 observations. No need to make these tables pretty.\n\n# You need to replace df_cat_only with your data frame that \n#     only has categorical predictors\nlapply(df_cat_only, function(x) table(df_cat_only$FOOD_INSEC, x))\n\n\n\n\n\n\n\nTask\n\n\n\n\nCreate contingency tables for all categorical covariates with food insecurity.\nTake note of any cell counts that are less than 10\n\n\n\n\n\n2.5 Bivariate exploratory data analysis\nUse ggpairs() (introduced in BSTA 512 Lesson 13) to quickly look at the relationship between variables. If you have trouble seeing or interpreting the individual plots, try recreating them in ggplot().\n\n\n\n\n\n\nTask\n\n\n\n\nUse ggpairs() (introduced in BSTA 512 Lesson 13) to quickly look at the relationship between variables.\nList predictors with which there is a clear trend with food insecurity.\n\n\n\n\n\n2.6 Fit simple logistic regression\n\n\n\n\n\n\nTask\n\n\n\n\nUsing glm(), run a logistic regression with food insecurity and your main variable of interest.\nDisplay the unadjusted odds ratio of the regression. You can use logistic.display()\nInterpret the unadjusted odds ratio (with 95% confidence interval). If you’re main variable is multi-level, then you will need to interpret multiple odds ratios.\n\n\n\n\n\n2.7 Plot the predicted probability\nI want us to plot the predicted probability across our main independent variable. If your main variable of interest (from your research question) is continuous, then you can follow the code from Lesson 7 to construct a plot of the predicted probability. If your main variable of interest in categorical, then you can try plotting the predicted probability in the same way as Lesson 7. You may prefer to present the predicted probabilities for each group as a table.\nThis plot will serve as a good foundation if we have any interactions in the model!\n\n\n\n\n\n\nTask\n\n\n\nPlot or make a table of your predicted probabilities."
  },
  {
    "objectID": "project/Lab_04_instructions.html",
    "href": "project/Lab_04_instructions.html",
    "title": "Lab 4 Instructions",
    "section": "",
    "text": "Caution\n\n\n\nNicky needs to edit"
  },
  {
    "objectID": "project/Lab_04_instructions.html#directions",
    "href": "project/Lab_04_instructions.html#directions",
    "title": "Lab 4 Instructions",
    "section": "1 Directions",
    "text": "1 Directions\nYou can download the .qmd file for this lab here.\nThe above link will take you to your editing file. Please do not remove anything from this editing file!! You will only add your code and work to this file.\n\n1.1 Purpose\nThe purpose of this lab is to fit a multiple logistic regression model and practice how we would interpret our results for this study.\n\n\n1.2 Grading\nThis lab is graded out of 12 points. The TAs will go through and grade your lab. They will make sure each section is complete and will follow the rubric below. I have instructed them that completion and clear effort is all that is needed to receive 100%. Nicky will go through the labs to give you feedback.\n\n1.2.1 Rubric\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nFormatting\nLab submitted on Sakai with .html file. Answers are written in complete sentences with no major grammatical nor spelling errors. With little editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with grammatical or spelling errors. With editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with major grammatical or spelling errors. With major editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are bulletted or do not use complete sentences.\nLab not submitted on Sakai with .html file.\n\n\nCode/Work\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMost tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nSome tasks are directly followed or answered.This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMore than a quarter of the tasks are not completed properly.\n\n\nReasoning*\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful and can be easily integrated into the final report.\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful, but lack the clarity needed to easily integrate into the final report.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers are fairly thoughtful, but lack connection to the research.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers seem rushed and with minimal thought.\nAnswers lack understanding of research context and investigation of the data. Answers seem rushed and without thought.\n\n\n\n*Applies to questions with reasoning"
  },
  {
    "objectID": "project/Lab_04_instructions.html#lab-activities",
    "href": "project/Lab_04_instructions.html#lab-activities",
    "title": "Lab 4 Instructions",
    "section": "2 Lab activities",
    "text": "2 Lab activities\n\n\n\n\n\n\nNote\n\n\n\nI have left it up to you to load the needed packages for this lab.\n\n\n\n2.1 Restate research question\n\n\n\n\n\n\nTask\n\n\n\nPlease restate your research question below using the provided format (1 sentence). You can change the wording if you’d like, but please make sure it is still clear. It’s repetitive, but it helps me contextualize my feedback as I look through your lab.\n\n\nIn this study, we will investigate the association between food insecurity and ________.\n\n\n2.2 Build your model\nThis lab is very open-ended. You will need to build a model for your outcome. We discussed prediction modeling in Lesson 15. (We covered everything you will need in class even though we did not finish my slides.) We discussed association modeling in BSTA 512/612.\nYou may either use LASSO regression OR Purposeful model selection to build a model. I highly suggest picking the model selection strategy based on your desired learning objective. LASSO will help stretch your R coding and machine learning skills. Purposeful model selection will allow you to cement certain concepts that we learned within 512/612 and 513/613.\nI will not be taking you through step-by-step. Please follow my work from Lesson 15 in 513/613 or from Lab 4 in 512/612.\n\n\n2.3 Assess your model fit\nCheck if your model fits the data well (Hosmer-Lemeshow test). Calculate the ROC-AUC of your model.\n\n\n2.4 Perform model diagnostics\nCheck your diagnostic plots and cutoffs (change in Pearson residuals, change in coefficients, and leverage) to identify and investigate any influential or outlier observations. Are these observations feasible?"
  },
  {
    "objectID": "project.html#labs",
    "href": "project.html#labs",
    "title": "Project Central",
    "section": "Labs",
    "text": "Labs\n\n\n\nLab\nRubric\nDue Date\nDiscussion Date\nTopics\n\n\n\n\nLab 1\n\n4/11\n4/16\nExploring the question and data\n\n\nLab 2\n\n4/25\n4/30\nEDA continued + Simple logistic regression\n\n\nLab 3\n\n5/9\n5/12\nTemporary multiple logistic regression + practice interpreting ORs\n\n\nLab 4\n\n5/23\n5/28\nBuilding and interpreting final model\n\n\n\n\nLab make-up assignment if you missed a discussion day"
  },
  {
    "objectID": "project.html#report",
    "href": "project.html#report",
    "title": "Project Central",
    "section": "Report",
    "text": "Report\nPoster Instructions\nDue 6/9/2025 @11pm"
  },
  {
    "objectID": "project.html#information-and-resources-on-food-insecurity",
    "href": "project.html#information-and-resources-on-food-insecurity",
    "title": "Project Central",
    "section": "Information and Resources on Food Insecurity",
    "text": "Information and Resources on Food Insecurity\nThis project will discuss food insecurity and unmet basic needs. If you have experienced or are experiencing food insecurity, and this project impacts your mental health or ability to work, please let me know. We can work on an alternative analysis with a different dataset.\nIf you are currently experiencing food insecurity or not meeting your basic needs, here are some resources for Oregon residents:\n\nOregon One elegibility page\n\nYou can apply for benefits including medical, food, cash, or child care assistance\nThis is one way to apply to multiple benefits\n\nOregon Health Plan\nThe Oregon Food Bank has a food finder\n\nIn addition to these statewide resources, as students, there are other resources available to you:\n\nFood Assistance and Basic Needs for OHSU students\n\nIncludes information on SNAP program\nIncludes information on free groceries available to students at the Food Resource Center\n\nYou can even order it online and pick it up!\n\n\nYou can email basicneeds@ohsu.edu for help getting assistance or connecting you to the following services:\n\nManage your finances\nAccess emergency funds\nFind legal services\nApply for SNAP\nFind child care\nFind housing"
  },
  {
    "objectID": "project.html#reading-and-listening-sources",
    "href": "project.html#reading-and-listening-sources",
    "title": "Project Central",
    "section": "Reading and listening sources",
    "text": "Reading and listening sources"
  },
  {
    "objectID": "lessons/15_Model_building/15_Model_Building.html#some-important-definitions",
    "href": "lessons/15_Model_building/15_Model_Building.html#some-important-definitions",
    "title": "Lesson 15: Model Building",
    "section": "Some important definitions",
    "text": "Some important definitions\n\nModel selection: picking the “best” model from a set of possible models\n\nModels will have the same outcome, but typically differ by the covariates that are included, their transformations, and their interactions\n“Best” model is defined by the research question and by how you want to answer it!\n\n\n \n\nModel selection strategies: a process or framework that helps us pick our “best” model\n\nThese strategies often differ by the approach and criteria used to the determine the “best” model\n\n\n \n\nOverfitting: result of fitting a model so closely to our particular sample data that it cannot be generalized to other samples (or the population)"
  },
  {
    "objectID": "lessons/15_Model_building/15_Model_Building.html#bias-variance-trade-off",
    "href": "lessons/15_Model_building/15_Model_Building.html#bias-variance-trade-off",
    "title": "Lesson 15: Model Building",
    "section": "Bias-variance trade off",
    "text": "Bias-variance trade off\n\n\n\nRecall from 512/612: MSE can be written as a function of the bias and variance\n\\[\nMSE = \\text{bias}\\big(\\widehat\\beta\\big)^2 + \\text{variance}\\big(\\widehat\\beta\\big)\n\\]\n\nWe no longer use MSE in logistic regression to find the best fit model, BUT the idea between the bias and variance trade off holds!\n\nFor the same data:\n\nMore covariates in model: less bias, more variance\n\nPotential overfitting: with new data does our model still hold?\n\nLess covariates in model: more bias, less variance\n\nMore bias bc more likely that were are not capturing the true underlying relationship with less variables\n\n\n\n\n\n\n\nSource: http://scott.fortmann-roe.com/docs/BiasVariance.html"
  },
  {
    "objectID": "lessons/15_Model_building/15_Model_Building.html#the-goals-of-association-vs.-prediction",
    "href": "lessons/15_Model_building/15_Model_Building.html#the-goals-of-association-vs.-prediction",
    "title": "Lesson 15: Model Building",
    "section": "The goals of association vs. prediction",
    "text": "The goals of association vs. prediction\n\n\n\n\nAssociation / Explanatory / One variable’s effect\n\n\n\nGoal: Understand one variable’s (or a group of variable’s) effect on the response after adjusting for other factors\nMainly interpret odds ratios of the variable that is the focus of the study\n\n\n\n\n\n\nPrediction\n\n\n\nGoal: to calculate the most precise prediction of the response variable\nInterpreting coefficients is not important\nChoose only the variables that are strong predictors of the response variable\n\nExcluding irrelevant variables can help reduce widths of the prediction intervals"
  },
  {
    "objectID": "lessons/15_Model_building/15_Model_Building.html#model-selection-strategies-for-categorical-outcomes",
    "href": "lessons/15_Model_building/15_Model_Building.html#model-selection-strategies-for-categorical-outcomes",
    "title": "Lesson 15: Model Building",
    "section": "Model selection strategies for categorical outcomes",
    "text": "Model selection strategies for categorical outcomes\n\n\n\n\nAssociation / Explanatory / One variable’s effect\n\n\n\nSelection of potential models is tied more with the research context with some incorporation of prediction scores\n\n \n\nPre-specification of multivariable model\nPurposeful model selection\n\n“Risk factor modeling”\n\nChange in Estimate (CIE) approaches\n\nWill learn in Survival Analysis (BSTA 514)\n\n\n\n\n\n\n\nPrediction\n\n\n\nSelection of potential models is fully dependent on prediction scores\n\n \n\nLogistic regression with more refined model selection\n\nRegularization techniques (LASSO, Ridge, Elastic net)\n\nMachine learning realm\n\nDecision trees, random forest, k-nearest neighbors, Neural networks"
  },
  {
    "objectID": "lessons/15_Model_building/15_Model_Building.html#before-i-move-on",
    "href": "lessons/15_Model_building/15_Model_Building.html#before-i-move-on",
    "title": "Lesson 15: Model Building",
    "section": "Before I move on…",
    "text": "Before I move on…\n\nWe CAN use purposeful selection from last quarter in any type of generalized linear model (GLM)\n\nThis includes logistic regression!\n\n\n \n\nThe best documented information on purposeful selection is in the Hosmer-Lemeshow textbook on logistic regression\n\nTextbook in student files is linked here\nPurposeful selection starts on page 89 (or page 101 in the pdf)\n\n\n \n\nI will not discuss purposeful selection in this course\n\nBe aware that this is a tool that you can use in any regression!"
  },
  {
    "objectID": "lessons/15_Model_building/15_Model_Building.html#okay-so-prediction-of-categorical-outcomes",
    "href": "lessons/15_Model_building/15_Model_Building.html#okay-so-prediction-of-categorical-outcomes",
    "title": "Lesson 15: Model Building",
    "section": "Okay, so prediction of categorical outcomes",
    "text": "Okay, so prediction of categorical outcomes\n\nClassification: process of predicting categorical responses/outcomes\n\nAssigning a category outcome based on an observation’s predictors\n\n\n \n\nNote: we’ve already done a lot of work around predicting probabilities within logistic regression\n\nCan we take those predicted probabilities one step further to predict the binary outcome??\n\n\n \n\nCommon classification methods (good site on brief explanation of each)\n\nLogistic regression\nNaive Bayes\nk-Nearest Neighbor (KNN)\nDecision Trees\nSupport Vector Machines (SVMs)\nNeural Networks"
  },
  {
    "objectID": "lessons/15_Model_building/15_Model_Building.html#logistic-regression-is-a-classification-method",
    "href": "lessons/15_Model_building/15_Model_Building.html#logistic-regression-is-a-classification-method",
    "title": "Lesson 15: Model Building",
    "section": "Logistic regression is a classification method",
    "text": "Logistic regression is a classification method\n\nBut to be a good classifier, our logistic regression model needs to built a certain way\n\n \n\nPrediction depends on type of variable/model selection!\n\nThis is when it can become machine learning\n\n\n \n\nSo the big question is: how do we select this model??\n\nRegularized techniques, aka penalized regression"
  },
  {
    "objectID": "lessons/15_Model_building/15_Model_Building.html#poll-everywhere-question-1",
    "href": "lessons/15_Model_building/15_Model_Building.html#poll-everywhere-question-1",
    "title": "Lesson 15: Model Building",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lessons/15_Model_building/15_Model_Building.html#before-i-get-really-into-things",
    "href": "lessons/15_Model_building/15_Model_Building.html#before-i-get-really-into-things",
    "title": "Lesson 15: Model Building",
    "section": "Before I get really into things!!",
    "text": "Before I get really into things!!\n\ntidymodels is a great package when we are performing prediction\nOne problem: it uses very different syntax for model fitting than we are used to…\ntidymodels syntax dictates that we need to define:\n\nA model\nA recipe\nA workflow"
  },
  {
    "objectID": "lessons/15_Model_building/15_Model_Building.html#tidymodels-with-glow",
    "href": "lessons/15_Model_building/15_Model_Building.html#tidymodels-with-glow",
    "title": "Lesson 15: Model Building",
    "section": "tidymodels with GLOW",
    "text": "tidymodels with GLOW\nTo fit our logistic regression model with the interaction between age and prior fracture, we use:\n\n# model\nmodel = logistic_reg()\n# recipe\nrecipe = recipe(fracture ~ priorfrac + age_c, data = glow1) %&gt;%\n            step_dummy(priorfrac) %&gt;%\n            step_interact(terms = ~ age_c:starts_with(\"priorfrac\"))\n# workflow\nworkflow = workflow() %&gt;% add_model(model) %&gt;% add_recipe(recipe)\n\nfit = workflow %&gt;% fit(data = glow1)\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n−1.376\n0.134\n−10.270\n0.000\n−1.646\n−1.120\n\n\nage_c\n0.063\n0.015\n4.043\n0.000\n0.032\n0.093\n\n\npriorfrac_Yes\n1.002\n0.240\n4.184\n0.000\n0.530\n1.471\n\n\nage_c_x_priorfrac_Yes\n−0.057\n0.025\n−2.294\n0.022\n−0.107\n−0.008"
  },
  {
    "objectID": "lessons/15_Model_building/15_Model_Building.html#same-as-results-from-previous-lessons",
    "href": "lessons/15_Model_building/15_Model_Building.html#same-as-results-from-previous-lessons",
    "title": "Lesson 15: Model Building",
    "section": "Same as results from previous lessons",
    "text": "Same as results from previous lessons\n\nglow_m3 = glm(fracture ~ priorfrac + age_c + priorfrac*age_c, \n              data = glow1, family = binomial)\n\n\ntidy(glow_m3, conf.int = T) %&gt;% gt() %&gt;% \n  tab_options(table.font.size = 35) %&gt;%\n  fmt_number(decimals = 3)\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n−1.376\n0.134\n−10.270\n0.000\n−1.646\n−1.120\n\n\npriorfracYes\n1.002\n0.240\n4.184\n0.000\n0.530\n1.471\n\n\nage_c\n0.063\n0.015\n4.043\n0.000\n0.032\n0.093\n\n\npriorfracYes:age_c\n−0.057\n0.025\n−2.294\n0.022\n−0.107\n−0.008\n\n\n\n\n\n\n\nInteraction model:  \\[\\begin{aligned} \\text{logit}\\left(\\widehat\\pi(\\mathbf{X})\\right) & = \\widehat\\beta_0 &+ &\\widehat\\beta_1\\cdot I(\\text{PF}) & + &\\widehat\\beta_2\\cdot Age& + &\\widehat\\beta_3 \\cdot I(\\text{PF}) \\cdot Age \\\\  \n\\text{logit}\\left(\\widehat\\pi(\\mathbf{X})\\right) & = -1.376 &+ &1.002\\cdot I(\\text{PF})& + &0.063\\cdot Age& -&0.057 \\cdot I(\\text{PF}) \\cdot Age\n\\end{aligned}\\]\n\nReminder of main effects and interactions"
  },
  {
    "objectID": "lessons/15_Model_building/15_Model_Building.html#penalized-regression",
    "href": "lessons/15_Model_building/15_Model_Building.html#penalized-regression",
    "title": "Lesson 15: Model Building",
    "section": "Penalized regression",
    "text": "Penalized regression\n\nBasic idea: We are running regression, but now we want to incentivize our model fit to have less predictors\n\nInclude a penalty to discourage too many predictors in the model\n\n\n \n\nAlso known as shrinkage or regularization methods\n\n \n\nPenalty will reduce coefficient values to zero (or close to zero) if the predictor does not contribute much information to predicting our outcome\n\n \n\nWe need a tuning parameter that determines the amount of shrinkage called lambda/\\(\\lambda\\)\n\nHow much do we want to penalize additional predictors?"
  },
  {
    "objectID": "lessons/15_Model_building/15_Model_Building.html#poll-everywhere-question-2",
    "href": "lessons/15_Model_building/15_Model_Building.html#poll-everywhere-question-2",
    "title": "Lesson 15: Model Building",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lessons/15_Model_building/15_Model_Building.html#three-types-of-penalized-regression",
    "href": "lessons/15_Model_building/15_Model_Building.html#three-types-of-penalized-regression",
    "title": "Lesson 15: Model Building",
    "section": "Three types of penalized regression",
    "text": "Three types of penalized regression\nMain difference is the type of penalty used\n\n\n\n\nRidge regression\n\n\n\nPenalty called L2 norm, uses sqaured values\nPros\n\nReduces overfitting\nHandles \\(p&gt;n\\)\nHandles collinearity\n\nCons\n\nDoes not shrink coefficients to 0\nDifficult to interpret\n\n\n\n\n\n\n\nLasso regression\n\n\n\nPenalty called L1 norm, uses absolute values\n\n \n\nPros\n\nReduces overfitting\nShrinks coefficients to 0\n\nCons\n\nCannot handle \\(p&gt;n\\)\nDoes not handle multicollinearity well\n\n\n\n\n\n\n\nElastic net regression\n\n\n\nL1 and L2 used, best of both worlds\nPros\n\nReduces overfitting\nHandles \\(p&gt;n\\)\nHandles collinearity\nShrinks coefficients to 0\n\nCons\n\nMore difficult to do than other two"
  },
  {
    "objectID": "lessons/15_Model_building/15_Model_Building.html#overview-of-the-process",
    "href": "lessons/15_Model_building/15_Model_Building.html#overview-of-the-process",
    "title": "Lesson 15: Model Building",
    "section": "Overview of the process",
    "text": "Overview of the process\n\nSplit data into training and testing datasets\n\n \n\nPerform our classification method on training set\n\nThis is where we will use penalized regression!\n\n\n \n\nMeasure predictive accuracy on testing set"
  },
  {
    "objectID": "lessons/15_Model_building/15_Model_Building.html#example-to-be-used-glow-study",
    "href": "lessons/15_Model_building/15_Model_Building.html#example-to-be-used-glow-study",
    "title": "Lesson 15: Model Building",
    "section": "Example to be used: GLOW Study",
    "text": "Example to be used: GLOW Study\n\nFrom GLOW (Global Longitudinal Study of Osteoporosis in Women) study\n\n \n\nOutcome variable: any fracture in the first year of follow up (FRACTURE: 0 or 1)\n\n \n\nRisk factor/variable of interest: history of prior fracture (PRIORFRAC: 0 or 1)\nPotential confounder or effect modifier: age (AGE, a continuous variable)\n\nCenter age will be used! We will center around the rounded mean age of 69 years old\n\n\n \n\nCrossed out because we are no longer attached to specific predictors and their association with fracture\n\nFocused on predicting fracture with whatever variables we can!"
  },
  {
    "objectID": "lessons/15_Model_building/15_Model_Building.html#step-1-splitting-data",
    "href": "lessons/15_Model_building/15_Model_Building.html#step-1-splitting-data",
    "title": "Lesson 15: Model Building",
    "section": "Step 1: Splitting data",
    "text": "Step 1: Splitting data\n\nTraining: act of creating our prediction model based on our observed data\n\nSupervised: Means we keep information on our outcome while training\n\n\n \n\nTesting: act of measuring the predictive accuracy of our model by trying it out on new data\n\n \n\nWhen we use data to create a prediction model, we want to test our prediction model on new data\n\nHelps make sure prediction model can be applied to other data outside of the data that was used to create it!\n\n\n \n\nSo an important first step in prediction modeling is to split our data into a training set and a testing set!"
  },
  {
    "objectID": "lessons/15_Model_building/15_Model_Building.html#step-1-splitting-data-1",
    "href": "lessons/15_Model_building/15_Model_Building.html#step-1-splitting-data-1",
    "title": "Lesson 15: Model Building",
    "section": "Step 1: Splitting data",
    "text": "Step 1: Splitting data\n\n\n\n\nTraining set\n\n\n\nSandbox for model building\nSpend most of your time using the training set to develop the model\nMajority of the data (usually 80%)\n\n\n\n\n\n\nTesting set\n\n\n\nHeld in reserve to determine efficacy of one or two chosen models\nCritical to look at it once at the end, otherwise it becomes part of the modeling process\nRemainder of the data (usually 20%)\n\n\n\n\n     \n \n\nSlide content from Data Science in a Box"
  },
  {
    "objectID": "lessons/15_Model_building/15_Model_Building.html#poll-everywhere-question-3",
    "href": "lessons/15_Model_building/15_Model_Building.html#poll-everywhere-question-3",
    "title": "Lesson 15: Model Building",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lessons/15_Model_building/15_Model_Building.html#step-1-splitting-data-2",
    "href": "lessons/15_Model_building/15_Model_Building.html#step-1-splitting-data-2",
    "title": "Lesson 15: Model Building",
    "section": "Step 1: Splitting data",
    "text": "Step 1: Splitting data\n\nWhen splitting data, we need to be conscious of the proportions of our outcomes\n\nIs there imbalance within our outcome?\nWe want to randomly select observations but make sure the proportions of No and Yes stay the same\nWe stratify by the outcome, meaning we pick Yes’s and No’s separately for the training set\n\n\n\nggplot(glow1, aes(x = fracture)) + geom_bar()\n\n\n\nSide note: took out bmi and weight bc we have multicollinearity issues\n\nCombo of I hate these variables and my previous work in the LASSO identified these as not important\n\n\n\nglow = glow1 %&gt;%\n    dplyr::select(-sub_id, -site_id, -phy_id, -age, -bmi, -weight)"
  },
  {
    "objectID": "lessons/15_Model_building/15_Model_Building.html#step-1-splitting-data-3",
    "href": "lessons/15_Model_building/15_Model_Building.html#step-1-splitting-data-3",
    "title": "Lesson 15: Model Building",
    "section": "Step 1: Splitting data",
    "text": "Step 1: Splitting data\n\nFrom package rsample within tidyverse, we can use initial_split() to create training and testing data\n\nUse strata to stratify by fracture\n\n\n\nglow_split = initial_split(glow, strata = fracture, prop = 0.8)\nglow_split\n\n&lt;Training/Testing/Total&gt;\n&lt;400/100/500&gt;\n\n\n\nThen we can pull the training and testing data into their own datasets\n\n\nglow_train = training(glow_split)\nglow_test = testing(glow_split)"
  },
  {
    "objectID": "lessons/15_Model_building/15_Model_Building.html#step-1-splitting-data-peek-at-the-split",
    "href": "lessons/15_Model_building/15_Model_Building.html#step-1-splitting-data-peek-at-the-split",
    "title": "Lesson 15: Model Building",
    "section": "Step 1: Splitting data: peek at the split",
    "text": "Step 1: Splitting data: peek at the split\n\n\n\nglimpse(glow_train)\n\nRows: 400\nColumns: 10\n$ priorfrac &lt;fct&gt; No, No, Yes, No, No, Yes, No, Yes, Yes, No, No, No, No, No, …\n$ height    &lt;int&gt; 158, 160, 157, 160, 152, 161, 150, 153, 156, 166, 153, 160, …\n$ premeno   &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, Yes, No, No, No,…\n$ momfrac   &lt;fct&gt; No, No, Yes, No, No, No, No, No, No, No, Yes, No, No, No, No…\n$ armassist &lt;fct&gt; No, No, Yes, No, No, No, No, No, No, No, No, No, Yes, No, No…\n$ smoke     &lt;fct&gt; No, No, No, No, No, Yes, No, No, No, No, Yes, No, No, No, No…\n$ raterisk  &lt;fct&gt; Same, Same, Less, Less, Same, Same, Less, Same, Same, Less, …\n$ fracscore &lt;int&gt; 1, 2, 11, 5, 1, 4, 6, 7, 7, 0, 4, 1, 4, 2, 2, 7, 2, 1, 4, 5,…\n$ fracture  &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, No, No, No, …\n$ age_c     &lt;dbl&gt; -7, -4, 19, 13, -8, -2, 15, 13, 17, -11, -2, -5, -1, -2, 0, …\n\n\n\n\nglimpse(glow_test)\n\nRows: 100\nColumns: 10\n$ priorfrac &lt;fct&gt; No, No, No, No, No, No, No, No, Yes, Yes, No, No, No, No, No…\n$ height    &lt;int&gt; 167, 162, 165, 158, 153, 170, 154, 171, 142, 152, 166, 154, …\n$ premeno   &lt;fct&gt; No, No, No, Yes, No, Yes, Yes, Yes, Yes, No, No, No, No, No,…\n$ momfrac   &lt;fct&gt; No, No, No, No, No, Yes, No, No, Yes, No, No, No, No, No, No…\n$ armassist &lt;fct&gt; Yes, No, Yes, No, Yes, No, Yes, No, No, No, No, No, No, No, …\n$ smoke     &lt;fct&gt; Yes, Yes, No, No, No, No, No, No, No, No, No, No, No, No, No…\n$ raterisk  &lt;fct&gt; Same, Less, Less, Greater, Same, Same, Same, Same, Same, Sam…\n$ fracscore &lt;int&gt; 3, 1, 5, 1, 8, 3, 7, 1, 6, 7, 0, 2, 0, 0, 1, 2, 2, 8, 4, 3, …\n$ fracture  &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, No, No, No, …\n$ age_c     &lt;dbl&gt; -13, -10, 3, -8, 17, 0, 6, -5, 1, 17, -11, -6, -10, -12, -6,…"
  },
  {
    "objectID": "lessons/15_Model_building/15_Model_Building.html#step-2-fit-lasso-penalized-logistic-regression-model",
    "href": "lessons/15_Model_building/15_Model_Building.html#step-2-fit-lasso-penalized-logistic-regression-model",
    "title": "Lesson 15: Model Building",
    "section": "Step 2: Fit LASSO penalized logistic regression model",
    "text": "Step 2: Fit LASSO penalized logistic regression model\n\nUsing Lasso penalized regression!\nWe can simply set up a penalized regression model\n\n \n\nlasso_mod = logistic_reg(penalty = 0.001, mixture = 1) %&gt;%\n\n            set_engine(\"glmnet\")\n\n\nglmnet takes the basic fitting of glm and adds penalties!\n\nIn tidymodels we set an engine that will fit the model\n\nmixture option let’s us pick the penalty\n\nmixture = 0 for Ridge regression\nmixture = 1 for Lasso regression\n0 &lt; mixture &lt; 1 for Elastic net regression"
  },
  {
    "objectID": "lessons/15_Model_building/15_Model_Building.html#step-2-fit-lasso-main-effects",
    "href": "lessons/15_Model_building/15_Model_Building.html#step-2-fit-lasso-main-effects",
    "title": "Lesson 15: Model Building",
    "section": "Step 2: Fit LASSO: Main effects",
    "text": "Step 2: Fit LASSO: Main effects\n\nglow_rec_main = recipe(fracture ~ ., data = glow_train) %&gt;%\n\n  step_dummy(priorfrac, premeno, momfrac, armassist, smoke, raterisk)\n\nglow_workflow_main = workflow() %&gt;%\n\n      add_model(lasso_mod) %&gt;% add_recipe(glow_rec_main)\n  \nglow_fit_main = glow_workflow_main %&gt;% fit(glow_train)"
  },
  {
    "objectID": "lessons/15_Model_building/15_Model_Building.html#step-2-fit-lasso-main-effects-identify-variables",
    "href": "lessons/15_Model_building/15_Model_Building.html#step-2-fit-lasso-main-effects-identify-variables",
    "title": "Lesson 15: Model Building",
    "section": "Step 2: Fit LASSO: Main effects: Identify variables",
    "text": "Step 2: Fit LASSO: Main effects: Identify variables\n\nlibrary(vip)  \n\n\nAttaching package: 'vip'\n\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nvi_data_main = glow_fit_main %&gt;% \n    pull_workflow_fit() %&gt;%\n    vi(lambda = 0.001) %&gt;%\n    filter(Importance != 0)\n\nWarning: `pull_workflow_fit()` was deprecated in workflows 0.2.3.\nℹ Please use `extract_fit_parsnip()` instead.\n\nvi_data_main\n\n# A tibble: 9 × 3\n  Variable         Importance Sign \n  &lt;chr&gt;                 &lt;dbl&gt; &lt;chr&gt;\n1 raterisk_Greater     0.559  POS  \n2 momfrac_Yes          0.542  POS  \n3 priorfrac_Yes        0.493  POS  \n4 raterisk_Same        0.438  POS  \n5 smoke_Yes            0.376  NEG  \n6 premeno_Yes          0.285  POS  \n7 fracscore            0.197  POS  \n8 armassist_Yes        0.146  POS  \n9 height               0.0382 NEG  \n\n\n\nLooks like age is removed!"
  },
  {
    "objectID": "lessons/15_Model_building/15_Model_Building.html#step-2-fit-lasso-main-effects-interactions",
    "href": "lessons/15_Model_building/15_Model_Building.html#step-2-fit-lasso-main-effects-interactions",
    "title": "Lesson 15: Model Building",
    "section": "Step 2: Fit LASSO: Main effects + interactions",
    "text": "Step 2: Fit LASSO: Main effects + interactions\n\nWe want to include interactions in our regression\nThe main effect model will be our starting point\n\nOtherwise, we may drop main effects but not their interactions\nCannot do that: see hierarchy principle\n\nI remove age_c from this section because main effects did not include it\n\n\nglow_rec_int = recipe(fracture ~ ., data = glow_train) %&gt;%\n  update_role(age_c, new_role = \"dont_use\") %&gt;%\n\n  step_dummy(priorfrac, premeno, momfrac, armassist, smoke, raterisk) %&gt;%\n\n  step_interact(terms = ~ all_predictors():all_predictors())\n\nglow_workflow_int = workflow() %&gt;%\n      add_model(lasso_mod) %&gt;% add_recipe(glow_rec_int)\n  \nglow_fit_int = glow_workflow_int %&gt;% fit(glow_train)"
  },
  {
    "objectID": "lessons/15_Model_building/15_Model_Building.html#step-2-fit-lasso-identify-interactions",
    "href": "lessons/15_Model_building/15_Model_Building.html#step-2-fit-lasso-identify-interactions",
    "title": "Lesson 15: Model Building",
    "section": "Step 2: Fit LASSO: Identify interactions",
    "text": "Step 2: Fit LASSO: Identify interactions\n\nvi_data_int = glow_fit_int %&gt;%\n    pull_workflow_fit() %&gt;%\n    vi(lambda = 0.001) %&gt;%\n    filter(Importance != 0)\nvi_data_int\n\n# A tibble: 34 × 3\n   Variable                       Importance Sign \n   &lt;chr&gt;                               &lt;dbl&gt; &lt;chr&gt;\n 1 smoke_Yes                            4.29 NEG  \n 2 smoke_Yes_x_raterisk_Greater         3.89 POS  \n 3 smoke_Yes_x_raterisk_Same            3.14 POS  \n 4 premeno_Yes_x_smoke_Yes              3.00 NEG  \n 5 momfrac_Yes_x_armassist_Yes          2.82 NEG  \n 6 priorfrac_Yes_x_premeno_Yes          2.50 NEG  \n 7 priorfrac_Yes                        1.82 POS  \n 8 armassist_Yes_x_smoke_Yes            1.44 POS  \n 9 premeno_Yes_x_raterisk_Greater       1.31 POS  \n10 momfrac_Yes_x_smoke_Yes              1.17 POS  \n# ℹ 24 more rows\n\n\n\nThis is where things got a little annoying for me…"
  },
  {
    "objectID": "lessons/15_Model_building/15_Model_Building.html#step-2-fit-lasso-identify-interactions-1",
    "href": "lessons/15_Model_building/15_Model_Building.html#step-2-fit-lasso-identify-interactions-1",
    "title": "Lesson 15: Model Building",
    "section": "Step 2: Fit LASSO: Identify interactions",
    "text": "Step 2: Fit LASSO: Identify interactions\n\nI combed through the column names of the results to find the interactions\n\n\nvi_data_int$Variable\n\n [1] \"smoke_Yes\"                        \"smoke_Yes_x_raterisk_Greater\"    \n [3] \"smoke_Yes_x_raterisk_Same\"        \"premeno_Yes_x_smoke_Yes\"         \n [5] \"momfrac_Yes_x_armassist_Yes\"      \"priorfrac_Yes_x_premeno_Yes\"     \n [7] \"priorfrac_Yes\"                    \"armassist_Yes_x_smoke_Yes\"       \n [9] \"premeno_Yes_x_raterisk_Greater\"   \"momfrac_Yes_x_smoke_Yes\"         \n[11] \"priorfrac_Yes_x_momfrac_Yes\"      \"priorfrac_Yes_x_armassist_Yes\"   \n[13] \"premeno_Yes_x_armassist_Yes\"      \"momfrac_Yes_x_raterisk_Same\"     \n[15] \"priorfrac_Yes_x_raterisk_Greater\" \"armassist_Yes_x_raterisk_Greater\"\n[17] \"fracscore_x_momfrac_Yes\"          \"priorfrac_Yes_x_smoke_Yes\"       \n[19] \"premeno_Yes_x_raterisk_Same\"      \"fracscore_x_priorfrac_Yes\"       \n[21] \"fracscore_x_premeno_Yes\"          \"raterisk_Same\"                   \n[23] \"fracscore\"                        \"fracscore_x_raterisk_Greater\"    \n[25] \"armassist_Yes_x_raterisk_Same\"    \"fracscore_x_smoke_Yes\"           \n[27] \"height\"                           \"momfrac_Yes_x_raterisk_Greater\"  \n[29] \"priorfrac_Yes_x_raterisk_Same\"    \"fracscore_x_raterisk_Same\"       \n[31] \"height_x_raterisk_Greater\"        \"height_x_premeno_Yes\"            \n[33] \"height_x_fracscore\"               \"height_x_armassist_Yes\""
  },
  {
    "objectID": "lessons/15_Model_building/15_Model_Building.html#step-2-fit-lasso-identify-interactions-2",
    "href": "lessons/15_Model_building/15_Model_Building.html#step-2-fit-lasso-identify-interactions-2",
    "title": "Lesson 15: Model Building",
    "section": "Step 2: Fit LASSO: Identify interactions",
    "text": "Step 2: Fit LASSO: Identify interactions\n\nI combed through the column names of the results to find the interactions\n\nI used ChatGPT to help me because I’m pretty new to tidymodels: let’s view what I asked\n\n\n\ninteractions = vi_data_int %&gt;% filter(grepl(\"_x_\", Variable)) %&gt;%\n                select(Variable) %&gt;% separate(Variable, \"_x_\")\n\nWarning: Expected 1 pieces. Additional pieces discarded in 29 rows [1, 2, 3, 4, 5, 6, 7,\n8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, ...].\n\ninteraction_terms = ~ (all_predictors()^2) - #Makes interactions b/w all predictors\n                      fracscore:starts_with(\"premeno\") - # Removes this interaction\n                      height:starts_with(\"premeno\") - \n                      height:starts_with(\"smoke\") - \n                      height:starts_with(\"momfrac\")"
  },
  {
    "objectID": "lessons/15_Model_building/15_Model_Building.html#step-2-fit-lasso-create-recipe-and-fit-model-from-lasso",
    "href": "lessons/15_Model_building/15_Model_Building.html#step-2-fit-lasso-create-recipe-and-fit-model-from-lasso",
    "title": "Lesson 15: Model Building",
    "section": "Step 2: Fit LASSO: Create recipe and fit model (from LASSO)",
    "text": "Step 2: Fit LASSO: Create recipe and fit model (from LASSO)\n\nThis is not the typical procedure for LASSO, but the tidymodels framework for interactions did not let me keep all main effects when looking at my interactions\n\n\nglow_rec_int2 = recipe(fracture ~ ., data = glow_train) %&gt;%\n  update_role(age_c, new_role = \"dont_use\") %&gt;%\n\n  step_dummy(priorfrac, premeno, momfrac, armassist, smoke, raterisk) %&gt;%\n\n  step_interact(terms = interaction_terms)\n  \nlog_model = logistic_reg()\n\nglow_workflow_int2 = workflow() %&gt;%\n      add_model(log_model) %&gt;% add_recipe(glow_rec_int2)\n  \nglow_fit_int2 = glow_workflow_int2 %&gt;% fit(glow_train)"
  },
  {
    "objectID": "lessons/15_Model_building/15_Model_Building.html#step-2-fit-lasso-look-at-model-fit",
    "href": "lessons/15_Model_building/15_Model_Building.html#step-2-fit-lasso-look-at-model-fit",
    "title": "Lesson 15: Model Building",
    "section": "Step 2: Fit LASSO: Look at model fit",
    "text": "Step 2: Fit LASSO: Look at model fit\n\nprint(tidy(glow_fit_int2), n=60)\n\n# A tibble: 42 × 5\n   term                              estimate std.error statistic p.value\n   &lt;chr&gt;                                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)                        3.09      10.3       0.300   0.764 \n 2 height                            -0.0415     0.0637   -0.652   0.515 \n 3 fracscore                         -2.92       2.15     -1.36    0.175 \n 4 priorfrac_Yes                     15.1        8.61      1.75    0.0793\n 5 premeno_Yes                       -0.805      1.14     -0.709   0.478 \n 6 momfrac_Yes                       -1.71       1.74     -0.984   0.325 \n 7 armassist_Yes                     18.5       10.7       1.73    0.0838\n 8 smoke_Yes                        -22.8      838.       -0.0272  0.978 \n 9 raterisk_Same                     16.0       10.1       1.59    0.112 \n10 raterisk_Greater                   1.13       9.16      0.123   0.902 \n11 height_x_fracscore                 0.0215     0.0136    1.58    0.113 \n12 height_x_priorfrac_Yes            -0.0825     0.0531   -1.55    0.120 \n13 height_x_armassist_Yes            -0.114      0.0645   -1.77    0.0762\n14 height_x_raterisk_Same            -0.0940     0.0623   -1.51    0.131 \n15 height_x_raterisk_Greater          0.00238    0.0563    0.0423  0.966 \n16 fracscore_x_priorfrac_Yes         -0.373      0.177    -2.10    0.0353\n17 fracscore_x_momfrac_Yes            0.608      0.313     1.94    0.0520\n18 fracscore_x_armassist_Yes         -0.111      0.178    -0.626   0.531 \n19 fracscore_x_smoke_Yes              0.604      0.564     1.07    0.284 \n20 fracscore_x_raterisk_Same         -0.257      0.209    -1.23    0.217 \n21 fracscore_x_raterisk_Greater      -0.318      0.212    -1.50    0.133 \n22 priorfrac_Yes_x_premeno_Yes       -2.72       1.06     -2.56    0.0104\n23 priorfrac_Yes_x_momfrac_Yes       -1.35       1.35     -1.00    0.317 \n24 priorfrac_Yes_x_armassist_Yes      1.45       0.820     1.76    0.0779\n25 priorfrac_Yes_x_smoke_Yes         -0.329      1.68     -0.196   0.845 \n26 priorfrac_Yes_x_raterisk_Same      0.122      0.837     0.146   0.884 \n27 priorfrac_Yes_x_raterisk_Greater   0.838      0.916     0.915   0.360 \n28 premeno_Yes_x_momfrac_Yes          0.304      1.58      0.192   0.848 \n29 premeno_Yes_x_armassist_Yes        1.73       0.923     1.87    0.0615\n30 premeno_Yes_x_smoke_Yes           -3.98       1.84     -2.17    0.0300\n31 premeno_Yes_x_raterisk_Same        0.716      1.16      0.620   0.535 \n32 premeno_Yes_x_raterisk_Greater     1.71       1.19      1.44    0.150 \n33 momfrac_Yes_x_armassist_Yes       -3.60       1.43     -2.52    0.0118\n34 momfrac_Yes_x_smoke_Yes            2.73       2.67      1.02    0.307 \n35 momfrac_Yes_x_raterisk_Same        1.87       1.33      1.41    0.160 \n36 momfrac_Yes_x_raterisk_Greater     0.730      1.33      0.548   0.583 \n37 armassist_Yes_x_smoke_Yes          1.58       1.67      0.948   0.343 \n38 armassist_Yes_x_raterisk_Same      0.690      0.893     0.774   0.439 \n39 armassist_Yes_x_raterisk_Greater  -0.247      0.975    -0.253   0.800 \n40 smoke_Yes_x_raterisk_Same         19.5      838.        0.0232  0.981 \n41 smoke_Yes_x_raterisk_Greater      20.0      838.        0.0239  0.981 \n42 raterisk_Same_x_raterisk_Greater  NA         NA        NA      NA"
  },
  {
    "objectID": "lessons/15_Model_building/15_Model_Building.html#poll-everywhere-question-4",
    "href": "lessons/15_Model_building/15_Model_Building.html#poll-everywhere-question-4",
    "title": "Lesson 15: Model Building",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lessons/15_Model_building/15_Model_Building.html#step-3-prediction-on-testing-set",
    "href": "lessons/15_Model_building/15_Model_Building.html#step-3-prediction-on-testing-set",
    "title": "Lesson 15: Model Building",
    "section": "Step 3: Prediction on testing set",
    "text": "Step 3: Prediction on testing set\n\nglow_test_pred = predict(glow_fit_int2, new_data = glow_test, type = \"prob\") %&gt;%\n    bind_cols(glow_test)\n\n\n\n\nglow_test_pred %&gt;% \n  roc_auc(truth = fracture, \n                  .pred_No)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.644\n\n\n\n\nglow_test_pred %&gt;% \n  roc_curve(truth = fracture, .pred_No) %&gt;%\n  autoplot()"
  },
  {
    "objectID": "lessons/15_Model_building/15_Model_Building.html#step-3-prediction-on-testing-set-1",
    "href": "lessons/15_Model_building/15_Model_Building.html#step-3-prediction-on-testing-set-1",
    "title": "Lesson 15: Model Building",
    "section": "Step 3: Prediction on testing set",
    "text": "Step 3: Prediction on testing set\n\nglow_test_pred = predict(glow_fit_int2, new_data = glow_test, type = \"prob\") %&gt;%\n    bind_cols(glow_test)\n\n\n\n\nglow_test_pred %&gt;% \n  roc_auc(truth = fracture, \n                  .pred_No)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.644\n\n\n \n\nWhy is this AUC worse than the one we saw with prior fracture, age, and their interaction?\n\nOnly 1 training and testing set: can overfit training and perform poorly on testing\nWe did not tune our penalty\nOur testing set only has 100 observations!\n\n\n\n\nglow_test_pred %&gt;% \n  roc_curve(truth = fracture, .pred_No) %&gt;%\n  autoplot()"
  },
  {
    "objectID": "lessons/15_Model_building/15_Model_Building.html#cross-validation-specifically-k-fold",
    "href": "lessons/15_Model_building/15_Model_Building.html#cross-validation-specifically-k-fold",
    "title": "Lesson 15: Model Building",
    "section": "Cross-validation (specifically k-fold)",
    "text": "Cross-validation (specifically k-fold)\n\nPrevents overfitting to one set of training data\nSplit data into folds that train and validate model selection\nBasically subsection of training and testing (called validating) before truly testing on our original testing set"
  },
  {
    "objectID": "lessons/15_Model_building/15_Model_Building.html#solutions-resources-beyond-our-class-right-now",
    "href": "lessons/15_Model_building/15_Model_Building.html#solutions-resources-beyond-our-class-right-now",
    "title": "Lesson 15: Model Building",
    "section": "Solutions / Resources (beyond our class right now)",
    "text": "Solutions / Resources (beyond our class right now)\n\nUse a tuning parameter for our penalty\n\nBasically, we need to figure out what the best penalty is for our model\nWe use the training set to determine the best penality\nVideos that includes tuning parameter with LASSO\n\nTidyTuesday video on LASSO with interactions\n\n\nCross-validation\n\nUnder Cross validation within Data Science in a Box\n\nFor complete video of machine learning with LASSO, cross-validation, and tuning parameters\n\nSee “Unit 5 - Deck 4: Machine learning” on this Data Science in a Box page\n\nVideo goes through an example with more complicated data, but can be followed with our work!"
  },
  {
    "objectID": "lessons/15_Model_building/15_Model_Building.html#summary",
    "href": "lessons/15_Model_building/15_Model_Building.html#summary",
    "title": "Lesson 15: Model Building",
    "section": "Summary",
    "text": "Summary\n\nRevisited model selection techniques and discussed how a binary outcome can be treated differently than a continuous outcome\nDiscussed association vs prediction modeling\nDiscussed classification: a type of machine learning!\nIntroduced penalized regression as a classification method\nPerformed penalized regression (specifically LASSO) to select a prediction model\nProcess presented today has major flaws\n\nWe did not tune our parameter\nWe did not perform cross validation"
  },
  {
    "objectID": "lessons/15_Model_building/15_Model_Building.html#for-your-lab-4",
    "href": "lessons/15_Model_building/15_Model_Building.html#for-your-lab-4",
    "title": "Lesson 15: Model Building",
    "section": "For your Lab 4",
    "text": "For your Lab 4\n\nYou can use purposeful selection, like we did last quarter\n\nIf you want to focus on association modeling!\nA good way to practice this again if you struggled with it previously\n\n\n \n\nYou can try out LASSO regression\n\nIf you want to focus on prediction modeling!\nAnd if you want to stretch your R coding skills"
  },
  {
    "objectID": "project/Lab_discussion_make-up.html",
    "href": "project/Lab_discussion_make-up.html",
    "title": "Lab Discussion Make-up Assignment",
    "section": "",
    "text": "Caution\n\n\n\nThis is not set yet. I am still working on the details of this make-up assignment."
  },
  {
    "objectID": "project/Lab_discussion_make-up.html#logistics",
    "href": "project/Lab_discussion_make-up.html#logistics",
    "title": "Lab Discussion Make-up Assignment",
    "section": "Logistics",
    "text": "Logistics\nThe make-up assignment will be due one week after the in-class discussion day. To turn in the assignment, you can email your document to Nicky."
  },
  {
    "objectID": "project/Lab_discussion_make-up.html#directions",
    "href": "project/Lab_discussion_make-up.html#directions",
    "title": "Lab Discussion Make-up Assignment",
    "section": "Directions",
    "text": "Directions\nIn a half page to one page, please answer the following questions about your peer’s (or peers’) lab assignment:\n\nWhat was their research question?\nWhat were three differences in your approach to the lab?\nWhat did you learn from their work?\nWhat would you improve from your own lab?"
  },
  {
    "objectID": "project/Project_poster_instructions.html",
    "href": "project/Project_poster_instructions.html",
    "title": "Project Poster Instructions",
    "section": "",
    "text": "Important\n\n\n\nPoster instructions will be edited for clarity, specifics about our dataset, and an in-depth rubric. - Nicky (3/31)\nThe purpose section was partially developed using ChatGPT by feeding in my previous project report instructions and asking ChatGPT to edit for a poster."
  },
  {
    "objectID": "project/Project_poster_instructions.html#directions",
    "href": "project/Project_poster_instructions.html#directions",
    "title": "Project Poster Instructions",
    "section": "1 Directions",
    "text": "1 Directions\n\n1.1 Purpose\nA scientific poster serves as a visual and concise way to communicate research findings. For this project, your poster should highlight your linear regression analysis and results while ensuring the context and methods are clearly explained. Posters should balance visuals (e.g., tables, figures) with text to engage an audience effectively.\n\n\n1.2 Formatting guide\n\n1.2.1 Poster specifications\n\nThis poster can be done in any program you would like\n\nPowerpoint is a common way to make a poster\n\nThis option is easier to start but more annoying when you have to edit visuals and fix the poster\nSome help creating it\n\nThere are poster templates in Quarto\n\nThis will be a little harder to start, but easier to edit visuals\nQuarto template\n\n\nPlease submit a PDF of your poster\nPoster dimensions: 36” by 24”\n\nMostly important to keep the ratio as 6:4\nUse a landscape layout\n\nFont size should be no less than 36pt\nSectioning of the report\n\nMain sections that were required: Introduction, Methods, Results, Conclusion, and References\nOther sections that might help group specific methods or results\n\nTitle information at the top of the poster\n\nThis includes the title itself, your name, and the date\n\nPoster printing for class on 3/17\n\nPrint in color!!\nUsing your PDF, and opening in Adobe Reader, then use the print function\nYou should see something like this: \nChoose the poster print option then choose the appropriate Tile Scale to make the poster span 4 pages\nNote: not all printers accommodate this\nYou should be able to print in Vanport\n\nI do not want you to pay for printing - please let me know if you are unable to print for free\nHere are two documents in the Student files to help with printing\n\nPrinter info\nPrinter guidance\n\n\nPosters presented on 3/17 in class do NOT need to be what you turn in at 11pm in Sakai\n\n\n\n\n1.2.2 Tables and figures\n\nTables and figures should NOT have variable names as they appear in the data frame\n\nVariable names should be understood by a reader\nVariable names should be written in full words\nInclude a title or caption for all figures\nFigure and tables appear on same page or close to same page where they are first referenced\nTables and figures are an appropriate size in the html\nNicky is able to read all words in figures and tables\n\nFigures and tables should be clear and crisp\n\nMake sure they are not blurred\nScreenshots are okay, but will likely make them blurry if you’re not careful\nBest option is to use the save() to save a jpg or png\n\n\n\n\n1.2.3 Writing\n\nWriting, spelling, and grammar should be admissible\n\nThis means I can generally follow your thought/what you are trying to communicate\nSome spelling and grammar mistakes are allowed\n\nI will not take off points if there are a few sprinkled in\nIf every or close to every sentence has mistakes, then I will take off\n\n\n\n\n\n\n\n\n\nThe project report is a separate file from the labs\n\n\n\nYou can save tables and figures from labs or separate files, then load them in the report\n\nSave R objects in analyses file:\n\nSuppose you named the Table 1 as table1\nsave(table1, file = \"table1.Rdata\")\n\nLoad R objects in report file: load(file = \"table1.Rdata\")\n\n\n\n\n\n\n1.3 Poster template\n\nPowerpoint Template\n\nFeel free to adjust this poster visually\n\nQuarto Template\n\nOnly works if you followed the steps in the Quarto template in the Formatting Guide\nFeel free to adjust this poster visually\n\n\n\n\n1.4 Poster examples\n\nPoster 1\n\nGood example of layouts and well-executed visuals\n\nPoster 2\n\nGood example of a forest plot to display coefficient estimates of many covariates\nGood example of patient information displayed\nGood highlight of the study goals in the background\n\nPoster 3\n\nDone by Nicky (a long time ago) so please excuse a lot of the poor language around race/ethnicity\n\nI have learned a lot since then! My statistics education did not do a great job of incorporating responsible practice around participant’s identities\n\nShowing this poster because it is a good example of the level of detail expected in our poster’s background, methods, and conclusions\n\n\n\n\n1.5 Grading\n\nGradingRubric\n\n\nThe project report is out of 36 points. Note that the Statistical Methods and Results sections are graded on an 8-point scale, while all other components are graded on a 4-point scale.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nFormatting\nPoster submitted on Sakai with PDF file. Report is written in well-constructed bullets with very few grammatical or spelling errors. With little editing, the poster can be presented at a conference.\nPoster submitted on Sakai with PDF file. Report is written in well-constructed bullets with some (around 2 per section) grammatical or spelling errors. With some editing, the report can be distributed.\nPoster submitted on Sakai with PDF file. Report is written in well-constructed bullets, but have many grammatical or spelling errors. With major editing, the report can be distributed.\nPoster submitted on Sakai with PDF file. Report is written in well-constructed bullets, but are very hard to follow due to grammar mistakes.\nPoster not submitted on Sakai. Poster not in PDF file type. Report language cannot be followed. With major editing, the report can be distributed.\n\n\nFigures and work\nAll requested output is displayed, including 2 required figures and tables, and at least one additional figure. Figures and tables look professional, are easily interpreted by the reader, and easily convey the intended message.\nAll requested output is displayed, including 2 required figures and tables, and at least one additional figure. For the most part, figures and tables look professional, are easily interpreted by the reader, and easily convey the intended message. A few mistakes in the figures are made.\nAll requested output is displayed, including 2 required figures and tables, and at least one additional figure. Figures and tables look semi-professional, are not so easily interpreted by the reader, and convey the intended message but after some work by the reader. Some mistakes in the figures are made.\nAll requested output is displayed, including 2 required figures and tables, and at least one additional figure. Figures and tables do not look professional, are not easily interpreted by the reader, and/or do not convey the intended message. Many mistakes in the figures are made.\nRequested output is not displayed, Missing one or more figures.\n\n\nIntroduction\nProvides a good background for the research question, includes motivation for the question, and references previous research that justifies this analysis.\nProvides a decent background for the research question and includes motivation for the question. Previous research is mentioned, but feels disconnected to the current analysis.\nProvides a decent background for the research question and includes motivation for the question. Previous research is mentioned, but feels disconnected to the current analysis.\nDoes not provide a background that connects to the research question. Motivation and previous research are not mentioned.\nNo introduction included.\n\n\nMethods (8 points)\nDescribes statistical methods concisely and highlights pertinent information to the reader (listed Sections below). Demonstrates proper analyses were performed.\nDescribes statistical methods and highlights pertinent information to the reader (listed Sections below). Details were omitted or added that were not needed to explain the overarching methods. Demonstrates proper analyses were performed.\nDescribes statistical methods and highlights pertinent information to the reader (listed Sections below). Details were omitted or added that were not needed to explain the overarching methods. Some incorrect analyses included in the description.\nDescribes statistical methods, but lacks clarity. Demonstrates a lack of understanding about the overall process of regression analysis. Incorrect analyses included in the description.\nNo methods included.\n\n\nResults (8 points)\nCorrectly interprets coefficients for the explanatory variable and identifies any other interesting trends. Highlights pertinent results to the reader (listed Sections below).\nCorrectly interprets coefficients, but does correctly incorporate the interaction (if in the model). Highlights pertinent results to the reader (listed Sections below).\nIncorrectly interprets coefficients. Highlights pertinent results to the reader (listed Sections below).\nIncorrectly interprets coefficients.Omits pertinent results to the reader (listed Sections below).\nNo results included.\n\n\nConclusion/ Discussion\nMain research question is answered and statistical caveats described to non-technical person. Thoroughly and concisely discusses limitations and considerations of the results, and their consequences.\nMain research question is answered and statistical caveats described to non-technical person. Discusses limitations and considerations of the results and their consequences, but misses some big considerations.\nMain research question is somewhat answered (but focus is not on the research question) and statistical caveats described to non-technical person. Discusses limitations and considerations of the results, but does not discuss the consequences.\nMain research question is somewhat answered (but not the focus at all) and statistical caveats are not described.Discusses limitations and considerations of the results, but misses many considerations and does not discuss consequences.\nNo conclusion included.\n\n\nReferences\nReferences are mostly cited consistently within the report, and in the Reference section. This includes the data source!\nReferences are sometimes cited consistently within the report, and in the Reference section. This includes the data source!\nReferences are sometimes cited consistently within the report, and in the Reference section. This includes the data source!\nReferences are not cited consistently within the report, and in the Reference section. This includes the data source!\nReferences are not included at all.\n\n\n\n\nIn formatting, an example of a report with little editing needed is one that has zero to some grammar or spelling mistakes, no code chunks showing, and no output warnings nor messages showing.\nProfessional figures mean\n\nI can read the words and numbers in the html\n\nVariable names are converted from the data frame version to readable text\nFor example: iam_001 does not show up on axes, instead something like: Response to \"Currently, I am...\"\n\nColors are only used if conveying information\nIntended message of the figure is easily understood\n\nIf you are trying to show a trend of mean IAT vs. an ordered categorical variable, then the variable is ordered on the x-axis\n\n\nFor the references\n\nI will not be overly critical about the formatting\nBy consistency, I mean that you if you are citing things like (Last Name, Year) it doesn’t suddenly change to number citations.\nIf you would like to use Quarto’s citation tool, you can! I actually pair it with Zotero and it works beautifully! (But I would not embark on this if you haven’t used Zotero before)"
  },
  {
    "objectID": "project/Project_poster_instructions.html#sections",
    "href": "project/Project_poster_instructions.html#sections",
    "title": "Project Poster Instructions",
    "section": "2 Sections",
    "text": "2 Sections\n\n2.1 Title\n\nPurpose: Create an identifiable name for your research project that includes the main research question’s variables and gives some context to the analysis or results\n\n\n\n2.2 Introduction\n\nLength: 5-8 bullets\nPurpose: Introduce the research question and why it is important to study\nThis section is non-technical.\n\nBy reading just the introduction and conclusion, someone without a technical background should have an idea of what they study was about, why it is important, and what the main results are\n\nYou may start with your bullets from Lab 1, but you should edit it and make sure it flows into your report well!\nShould contain some references\n\n\n\n2.3 Methods\n\nLength: 8-10 bullets\nPurpose: Describe the analyses that were conducted and methods used to select variables and check diagnostics\nSome important methods to discuss (You may divide these into your sections, not necessarily with these names)\n\nGeneral approach to the dataset\n\n2-3 bullets\nWhere did the data come from?\nDid you need to do any quality control?\nMissing data: we performed complete case analysis\n\n1 bullet\nCan be included in the Exploratory data analysis section\n\nWhat program did you use to analyze the data?\n\nVariables and variable creation\n\nThis includes a description of analyses for Table 1 and what statistics were used to summarize the variables\n\nMore on creation of Table 1, not discussing the results of Table 1\n\nIncludes (only include if you did one of the following)\n\nIndicators for gender identity or race\nCreating BMI\nCategorizing a continuous variable (even if performed in model selection)\nUsing scoring for an ordered categorical variable (that is not your explanatory variable)\n\n1 bullet for all variables\n\nModel building: we performed purposeful selection\n\n1-3 bullets\nIncludes\n\nDescribe purposeful selection: combining existing literature, clinical significance, and analysis\nHow did you build the model? Describe the process\nDid you consider confounders and effect modifiers?\n\nExample: We considered the following potential confounders: list fo them. Based on our research question, existing literature, and clinical significance, we used purposeful selection to identify confounders and effect modifiers.\n\nFinal model\n\n1 bullet\nWrite out your final model equation\nI would include the main exploratory variable then a placeholder for all the other covariates\n\nModel diagnostics\n\n1-3 sentences\nIncludes\n\nProcess of investigating model diagnostics\nBy the time you build the model, LINE assumptions should be met\nIf assumptions were not met, what process did you use to fix it?\n\n\n\n\n\n\n\n\n\n\nImportant to keep in mind\n\n\n\nMethods typically describe your approach and process, not the results of that process\n\nFor example: I might say “We investigated the linearity of each continuous covariate visually. If continuous variables were not linear, then we divided the variable into categories using existing guidelines from &lt;insert reference here&gt; or creating quartiles.”\nIn the methods section, I would NOT say: “We investigated the linearity of each continuous covariate visually. We found that age was not linearly related to IAT scores. Thus, we categorized age into the following groups: ___, ____, ____, ____, and ____.”\nThe last two sentences about age would be more appropriate in the Results section\n\n\n\n\n\n2.4 Results\n\n\n\n\n\n\nCaution\n\n\n\nNote: I took out “and stratified by your primary independent variable” for Table 1.\n\n\n\nLength: mostly figures with 2-3 bullet points\nPurpose: Relay the results from our sample’s analysis typically focusing on the numbers and interpretations\nTables & figures (2-3 tables or figures)\n\nThe following are required tables or figures\n\nTable 1 summarizing participant characteristics both overall\nTable or figure with regression results\n\nCan be a forest plot\nIf you have A LOT of coefficient estimates, the forest plot may not work well!\n\n\n1-2 figures that you think are helpful in understanding the results, for example\n\nDAG explaining connection between variables (if you did this)\nTable or figure to compare model fit statistics (if you did this)\nTable or figure for unadjusted relationship between outcome and explanatory variables\n\n\nInterpret the important model coefficients in the context of the research question\n\n2-3 bullets\nInterpreting the explanatory variable’s relationship with IAT score is the most important thing to report!!\n\nWhen doing this, make sure you account for ALL interactions: If your explanatory variable has multiple interactions and you are trying to interpret one, then what does that mean about the other variables involved in the other interactions? If this is confusing, please make an appointment with me!!\n\n\n\n\n\n2.5 Conclusion\n\nLength: 3-6 bullets\nPurpose: Describe the main conclusions to a non-technical audience\nWhat was the answer to your research question?\n\nMention the direction of the association if there was one\n\nAny other interesting results?\n\n\n\n2.6 Discussion\n\nLength: 3-5 bullets\nPurpose: Discuss the results and give them context outside of the sample and its analysis\nSome important things to include\n\nInclude limitations of the results\n\nYou don’t need to hit all the limitations, but think about the big ones (generalizability? independence of samples? large sample size vs. clinical significance? the way we handled variables?)\n\nAfter limitations, discuss the positive parts of the results\n\nWhat can we do with these results? What impact can it have?\n\nAny overarching trends that are worth noting? [@Giebel2024]\n\nShould contain some references\n\n\n\n2.7 References\n\nInclude your references here!\nYou introduction should have references, especially when discussing the social science behind the analysis\nYou must reference the IAT data source!!"
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA_muddy_points.html",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA_muddy_points.html",
    "title": "Muddy Points",
    "section": "",
    "text": "Be careful with how you use “one-sided.” It usually refers to the hypothesis test statements (\\(H_0\\) vs. \\(H_A\\)). For example, \\(H_0: \\beta_1 = 0\\) vs. \\(H_0: \\beta_1 \\neq 0\\) is two-sided test and \\(H_0: \\beta_1 = 0\\) vs. \\(H_0: \\beta_1 &gt; 0\\) is a one-sided test.\nHowever, I think you are asking if the probabilities can we calculate are always \\(P(F &gt; F_{stat})\\). For the F-distribution and Chi-squared distribution, we only ever calculate the probability from the area under the curve to the right of our F-statistic."
  },
  {
    "objectID": "lessons/02_Intro_CDA/02_Intro_CDA_muddy_points.html#muddy-points-from-spring-2025",
    "href": "lessons/02_Intro_CDA/02_Intro_CDA_muddy_points.html#muddy-points-from-spring-2025",
    "title": "Muddy Points",
    "section": "",
    "text": "Be careful with how you use “one-sided.” It usually refers to the hypothesis test statements (\\(H_0\\) vs. \\(H_A\\)). For example, \\(H_0: \\beta_1 = 0\\) vs. \\(H_0: \\beta_1 \\neq 0\\) is two-sided test and \\(H_0: \\beta_1 = 0\\) vs. \\(H_0: \\beta_1 &gt; 0\\) is a one-sided test.\nHowever, I think you are asking if the probabilities can we calculate are always \\(P(F &gt; F_{stat})\\). For the F-distribution and Chi-squared distribution, we only ever calculate the probability from the area under the curve to the right of our F-statistic."
  },
  {
    "objectID": "project/Lab_01_rubric.html",
    "href": "project/Lab_01_rubric.html",
    "title": "Lab 1 Rubric",
    "section": "",
    "text": "Lab activity\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nFamiliarize yourself with the Well-Being and Basic Needs Survey\nClearly answers all three questions with well-supported reasoning\nAnswers all questions but lacks depth in explanation\nAnswers some questions with minimal explanation\nAnswers are unclear or missing\n\n\nFile organization\nCorrectly sets up folders, .Rproj, and uses here()\nMinor issues with folder setup or here() usage\nSome structure but missing key components\nNo clear organization or incorrect setup\n\n\nDecide on list of variables to focus on\nCorrectly selects 10 predictors with all requirements met\nSelects 10 predictors but some do not meet criteria\nSelects fewer than 10 predictors or incorrect types\nSelects incorrect variables or missing response\n\n\nGet a sense of how you would like to analyze the data\nClearly states a well-formed research question\nResearch question is clear but could be refined\nResearch question is vague or missing key elements\nResearch question is unclear or missing\n\n\nSave data for processing with .Rda\nCreates and saves .Rda file with screenshot provided\nSaves .Rda file but no screenshot\nAttempts but incorrect .Rda file format\nNo .Rda file saved\n\n\nGetting data in working format\nSelects and processes variables correctly, cleans names\nSelects variables but has minor processing issues\nAttempts processing but with major errors\nDoes not attempt processing\n\n\nExplore the outcome and predictors\nReports food insecurity percentage and identifies low-count categories\nReports some details but incomplete\nMissing either percentage or variable count details\nDoes not report data insights\n\n\nCompile above work into an introduction\nWell-structured introduction with background, importance, and study purpose\nCovers key points but lacks clarity or depth\nCovers some points but lacks organization\nMissing key introduction elements"
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT_key_info.html",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT_key_info.html",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "Public Health Week starting next week on 4/8\n\nSchedule: https://ohsu-psu-sph.org/this-is-public-health/\n\nAnnual Public Health Week Conference on 4/11\n\nPlease submit your BSTA 512 poster!!\nIf you email the Student Leadership Council, I’m sure they would love to include your poster!\n\n\n\n\nFor National Public Health Week, join us for an exclusive tour of the School of Public Health’s only clinical laboratory, the Center for Infectious Disease (CIDS) on Tuesday, April 8 from 3:00 - 3:45pm.\nGetting a peek into the work of CIDS, you’ll witness first hand how applied public health research shapes real-world solutions and directly supports community well-being. Their expert-led research delves into the depths of pharmacoepidemiology and rheumatologic diseases, enhancing drug safety and efficacy for the broader population. Join us for a tour to expand your academic horizons, ask questions of the research team on what their day-to-day work and career path looks like, and to see what options you have as a pivotal researcher who is advancing health equity and enhancing community health outcomes."
  },
  {
    "objectID": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT_key_info.html#announcements",
    "href": "lessons/03_Meas_Assoc_CT/03_Meas_Assoc_CT_key_info.html#announcements",
    "title": "Key Info and Announcements",
    "section": "",
    "text": "Public Health Week starting next week on 4/8\n\nSchedule: https://ohsu-psu-sph.org/this-is-public-health/\n\nAnnual Public Health Week Conference on 4/11\n\nPlease submit your BSTA 512 poster!!\nIf you email the Student Leadership Council, I’m sure they would love to include your poster!\n\n\n\n\nFor National Public Health Week, join us for an exclusive tour of the School of Public Health’s only clinical laboratory, the Center for Infectious Disease (CIDS) on Tuesday, April 8 from 3:00 - 3:45pm.\nGetting a peek into the work of CIDS, you’ll witness first hand how applied public health research shapes real-world solutions and directly supports community well-being. Their expert-led research delves into the depths of pharmacoepidemiology and rheumatologic diseases, enhancing drug safety and efficacy for the broader population. Join us for a tour to expand your academic horizons, ask questions of the research team on what their day-to-day work and career path looks like, and to see what options you have as a pivotal researcher who is advancing health equity and enhancing community health outcomes."
  }
]